"""
StructureReportGenerator - è§†é¢‘ç»“æ„æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆäººç±»å¯è¯»çš„ Markdown æ ¼å¼è§†é¢‘åˆ†ææŠ¥å‘Š
"""

import json
from typing import List, Dict, Any
from pathlib import Path
from datetime import timedelta
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from models import Atom, NarrativeSegment
from utils import setup_logger

logger = setup_logger(__name__)


class StructureReportGenerator:
    """è§†é¢‘ç»“æ„æŠ¥å‘Šç”Ÿæˆå™¨"""

    def generate(
        self,
        atoms: List[Atom],
        segments: List[NarrativeSegment],
        entities: Dict[str, Any],
        topics: Dict[str, Any],
        validation: Dict[str, Any]
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„è§†é¢‘ç»“æ„æŠ¥å‘Š

        Args:
            atoms: åŸå­åˆ—è¡¨
            segments: å™äº‹ç‰‡æ®µåˆ—è¡¨
            entities: å®ä½“èšåˆæ•°æ®
            topics: ä¸»é¢˜ç½‘ç»œæ•°æ®
            validation: éªŒè¯æŠ¥å‘Š

        Returns:
            Markdown æ ¼å¼çš„æŠ¥å‘Šæ–‡æœ¬
        """
        logger.info("å¼€å§‹ç”Ÿæˆè§†é¢‘ç»“æ„æŠ¥å‘Š")

        sections = []

        # 1. æ ‡é¢˜å’Œæ¦‚è§ˆ
        sections.append(self._generate_header(atoms, segments))

        # 2. æ‰§è¡Œæ‘˜è¦
        sections.append(self._generate_executive_summary(segments, validation))

        # 3. å†…å®¹ç»“æ„æ—¶é—´è½´
        sections.append(self._generate_timeline(segments, atoms))

        # 4. å™äº‹ç‰‡æ®µè¯¦æƒ…
        sections.append(self._generate_segments_detail(segments))

        # 5. ä¸»é¢˜åˆ†æ
        sections.append(self._generate_topics_analysis(topics))

        # 6. å®ä½“ç½‘ç»œ
        sections.append(self._generate_entities_network(entities))

        # 7. å†…å®¹ç»Ÿè®¡
        sections.append(self._generate_statistics(atoms, segments, entities))

        # 8. AI æ´å¯Ÿæ±‡æ€»
        sections.append(self._generate_insights_summary(segments))

        # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
        report = "\n\n".join(sections)

        logger.info("è§†é¢‘ç»“æ„æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

        return report

    def _generate_header(self, atoms: List[Atom], segments: List[NarrativeSegment]) -> str:
        """ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨"""
        total_duration = atoms[-1].end_ms if atoms else 0
        duration_str = self._format_duration(total_duration)

        return f"""# ğŸ“¹ è§†é¢‘å†…å®¹ç»“æ„åˆ†ææŠ¥å‘Š

---

**è§†é¢‘æ—¶é•¿**: {duration_str}
**åˆ†ææ—¥æœŸ**: {self._get_current_date()}
**åŸå­å•å…ƒæ•°**: {len(atoms)}
**å™äº‹ç‰‡æ®µæ•°**: {len(segments)}

---
"""

    def _generate_executive_summary(
        self,
        segments: List[NarrativeSegment],
        validation: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        if not segments:
            return "## ğŸ“Š æ‰§è¡Œæ‘˜è¦\n\næš‚æ— å†…å®¹åˆ†æã€‚"

        # è®¡ç®—å¹³å‡è´¨é‡å’Œé‡è¦æ€§
        avg_importance = sum(s.importance_score for s in segments) / len(segments)
        avg_quality = sum(s.quality_score for s in segments) / len(segments)

        quality_emoji = "ğŸŸ¢" if avg_quality >= 0.7 else "ğŸŸ¡" if avg_quality >= 0.5 else "ğŸ”´"
        importance_emoji = "â­" * int(avg_importance * 5)

        coverage = validation.get('coverage_rate', 0) * 100

        return f"""## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### å†…å®¹è´¨é‡è¯„ä¼°

- **æ•´ä½“è´¨é‡**: {quality_emoji} {avg_quality:.2f}/1.0
- **å†…å®¹é‡è¦æ€§**: {importance_emoji} {avg_importance:.2f}/1.0
- **æ—¶é—´è¦†ç›–ç‡**: {coverage:.1f}%

### æ ¸å¿ƒä¸»é¢˜

{self._format_segment_topics(segments)}

### å…³é”®å‘ç°

{self._format_key_findings(segments)}
"""

    def _format_segment_topics(self, segments: List[NarrativeSegment]) -> str:
        """æ ¼å¼åŒ–ç‰‡æ®µä¸»é¢˜"""
        topics = []
        for seg in segments:
            if seg.topics.primary_topic:
                topics.append(f"- **{seg.title}**: {seg.topics.primary_topic}")

        return "\n".join(topics) if topics else "- æš‚æ— æ˜ç¡®ä¸»é¢˜"

    def _format_key_findings(self, segments: List[NarrativeSegment]) -> str:
        """æ ¼å¼åŒ–å…³é”®å‘ç°"""
        findings = []
        for seg in segments:
            if seg.ai_analysis.key_insights:
                for insight in seg.ai_analysis.key_insights[:2]:  # æ¯ä¸ªç‰‡æ®µå–å‰2æ¡
                    findings.append(f"- {insight}")

        return "\n".join(findings[:5]) if findings else "- æš‚æ— å…³é”®æ´å¯Ÿ"

    def _generate_timeline(
        self,
        segments: List[NarrativeSegment],
        atoms: List[Atom]
    ) -> str:
        """ç”Ÿæˆå†…å®¹æ—¶é—´è½´"""
        lines = ["## â±ï¸ å†…å®¹æ—¶é—´è½´", ""]

        if not segments:
            return "\n".join(lines + ["æš‚æ— ç‰‡æ®µæ•°æ®"])

        for seg in segments:
            start_time = self._ms_to_time(seg.start_ms)
            end_time = self._ms_to_time(seg.end_ms)
            duration_min = seg.duration_ms / 60000

            # é‡è¦æ€§æŒ‡ç¤ºå™¨
            importance_bar = "â–ˆ" * int(seg.importance_score * 10)

            lines.append(f"### {start_time} - {end_time} ({duration_min:.1f}åˆ†é’Ÿ)")
            lines.append(f"**{seg.title}**")
            lines.append(f"")
            lines.append(f"- **é‡è¦æ€§**: {importance_bar} {seg.importance_score:.2f}")
            lines.append(f"- **è´¨é‡**: {'â­' * int(seg.quality_score * 5)} {seg.quality_score:.2f}")
            lines.append(f"- **ç±»å‹**: {seg.narrative_structure.type}")

            if seg.topics.primary_topic:
                lines.append(f"- **ä¸»é¢˜**: {seg.topics.primary_topic}")

            lines.append(f"- **æ‘˜è¦**: {seg.summary[:150]}...")
            lines.append("")

        return "\n".join(lines)

    def _generate_segments_detail(self, segments: List[NarrativeSegment]) -> str:
        """ç”Ÿæˆç‰‡æ®µè¯¦ç»†åˆ†æ"""
        lines = ["## ğŸ“ å™äº‹ç‰‡æ®µè¯¦ç»†åˆ†æ", ""]

        if not segments:
            return "\n".join(lines + ["æš‚æ— ç‰‡æ®µæ•°æ®"])

        for i, seg in enumerate(segments, 1):
            lines.append(f"### {i}. {seg.title}")
            lines.append("")
            lines.append(f"**æ—¶é—´**: {self._ms_to_time(seg.start_ms)} - {self._ms_to_time(seg.end_ms)}")
            lines.append(f"**åŸå­æ•°**: {len(seg.atoms)}ä¸ª")
            lines.append("")

            # å™äº‹ç»“æ„
            lines.append("#### ğŸ¬ å™äº‹ç»“æ„")
            lines.append(f"- **ç±»å‹**: {seg.narrative_structure.type}")
            lines.append(f"- **ç»“æ„**: {seg.narrative_structure.structure}")

            if seg.narrative_structure.acts:
                lines.append("- **å¹•æ¬¡**:")
                for act in seg.narrative_structure.acts:
                    lines.append(f"  - **{act.get('role', 'æœªçŸ¥')}**: {act.get('description', '')}")
            lines.append("")

            # å†…å®¹æ‘˜è¦
            lines.append("#### ğŸ“„ å†…å®¹æ‘˜è¦")
            lines.append(seg.summary)
            lines.append("")

            # æ ¸å¿ƒè®ºç‚¹
            if seg.ai_analysis.core_argument:
                lines.append("#### ğŸ’¡ æ ¸å¿ƒè®ºç‚¹")
                lines.append(f"> {seg.ai_analysis.core_argument}")
                lines.append("")

            # å…³é”®æ´å¯Ÿ
            if seg.ai_analysis.key_insights:
                lines.append("#### ğŸ” å…³é”®æ´å¯Ÿ")
                for insight in seg.ai_analysis.key_insights:
                    lines.append(f"- {insight}")
                lines.append("")

            # å®ä½“æåŠ
            if any([seg.entities.persons, seg.entities.events, seg.entities.concepts]):
                lines.append("#### ğŸ·ï¸ æåŠçš„å®ä½“")
                if seg.entities.persons:
                    lines.append(f"- **äººç‰©**: {', '.join(seg.entities.persons)}")
                if seg.entities.events:
                    lines.append(f"- **äº‹ä»¶**: {', '.join(seg.entities.events)}")
                if seg.entities.concepts:
                    lines.append(f"- **æ¦‚å¿µ**: {', '.join(seg.entities.concepts)}")
                lines.append("")

            # äºŒåˆ›å»ºè®®
            if seg.ai_analysis.reuse_suggestions:
                lines.append("#### âœ‚ï¸ å†…å®¹å¤ç”¨å»ºè®®")
                for suggestion in seg.ai_analysis.reuse_suggestions:
                    lines.append(f"- {suggestion}")
                lines.append("")

            lines.append("---")
            lines.append("")

        return "\n".join(lines)

    def _generate_topics_analysis(self, topics: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸»é¢˜åˆ†æ"""
        lines = ["## ğŸ¯ ä¸»é¢˜åˆ†æ", ""]

        primary_topics = topics.get('primary_topics', [])
        secondary_topics = topics.get('secondary_topics', {})
        tags = topics.get('tags', {})

        if not primary_topics:
            return "\n".join(lines + ["æš‚æ— ä¸»é¢˜æ•°æ®"])

        # ä¸»è¦ä¸»é¢˜
        lines.append("### æ ¸å¿ƒä¸»é¢˜")
        lines.append("")
        for topic in primary_topics:
            weight = topic.get('weight', 0)
            topic_name = topic.get('topic', 'æœªçŸ¥')
            segments = topic.get('segments', [])

            lines.append(f"#### {topic_name}")
            lines.append(f"- **æƒé‡**: {'â–ˆ' * int(weight * 10)} {weight:.2f}")
            lines.append(f"- **å‡ºç°ç‰‡æ®µ**: {len(segments)}ä¸ª")

            subtopics = topic.get('subtopics', [])
            if subtopics:
                lines.append(f"- **å…³è”æ¬¡è¦ä¸»é¢˜**: {', '.join(list(subtopics)[:5])}")
            lines.append("")

        # æ¬¡è¦ä¸»é¢˜
        if secondary_topics:
            lines.append("### æ¬¡è¦ä¸»é¢˜")
            lines.append("")
            # secondary_topics æ˜¯åˆ—è¡¨
            if isinstance(secondary_topics, list):
                for topic_data in secondary_topics[:10]:
                    topic_name = topic_data.get('topic', 'æœªçŸ¥')
                    weight = topic_data.get('weight', 0)
                    lines.append(f"- **{topic_name}** (æƒé‡: {weight:.2f})")
            else:
                # å¦‚æœæ˜¯å­—å…¸ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                for topic_name, data in list(secondary_topics.items())[:10]:
                    count = data.get('count', 0)
                    lines.append(f"- **{topic_name}** (å‡ºç°{count}æ¬¡)")
            lines.append("")

        # æ ‡ç­¾äº‘
        if tags:
            lines.append("### ğŸ“Œ å†…å®¹æ ‡ç­¾")
            lines.append("")
            # tags å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸
            if isinstance(tags, list):
                # åˆ—è¡¨æ ¼å¼ï¼š[{tag: '', count: n}, ...]
                sorted_tags = sorted(tags, key=lambda x: x.get('count', 0), reverse=True)
                tag_list = [f"`{item.get('tag', '')}`" for item in sorted_tags[:20]]
            else:
                # å­—å…¸æ ¼å¼ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
                sorted_tags = sorted(tags.items(), key=lambda x: x[1].get('count', 0), reverse=True)
                tag_list = [f"`{tag}`" for tag, _ in sorted_tags[:20]]
            lines.append(" ".join(tag_list))
            lines.append("")

        return "\n".join(lines)

    def _generate_entities_network(self, entities: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®ä½“ç½‘ç»œ"""
        lines = ["## ğŸ‘¥ å®ä½“å…³ç³»ç½‘ç»œ", ""]

        stats = entities.get('statistics', {})
        if stats.get('total_entities', 0) == 0:
            return "\n".join(lines + ["æš‚æ— å®ä½“æ•°æ®"])

        # ç»Ÿè®¡æ¦‚è§ˆ
        lines.append("### å®ä½“ç»Ÿè®¡")
        lines.append("")
        by_type = stats.get('by_type', {})
        for entity_type, count in by_type.items():
            emoji = self._get_entity_emoji(entity_type)
            lines.append(f"- {emoji} **{entity_type}**: {count}ä¸ª")
        lines.append("")

        # é‡è¦äººç‰©
        persons = entities.get('persons', [])
        if persons:
            lines.append("### ğŸ§‘ é‡è¦äººç‰©")
            lines.append("")
            for person in persons[:10]:
                name = person.get('name', 'æœªçŸ¥')
                mentions = person.get('mentions', 0)
                atoms = person.get('atoms', [])
                context = person.get('context', [])

                lines.append(f"#### {name}")
                lines.append(f"- **æåŠæ¬¡æ•°**: {mentions}")
                lines.append(f"- **å‡ºç°åŸå­**: {len(atoms)}ä¸ª ({', '.join(atoms[:3])}...)")
                if context:
                    lines.append(f"- **ä¸Šä¸‹æ–‡**: {', '.join(context[:2])}")
                lines.append("")

        # å…³é”®äº‹ä»¶
        events = entities.get('events', [])
        if events:
            lines.append("### ğŸ“… å…³é”®äº‹ä»¶")
            lines.append("")
            for event in events[:10]:
                name = event.get('name', 'æœªçŸ¥')
                mentions = event.get('mentions', 0)
                context = event.get('context', [])

                lines.append(f"- **{name}** (æåŠ{mentions}æ¬¡)")
                if context:
                    lines.append(f"  - ç›¸å…³: {', '.join(context[:2])}")
            lines.append("")

        # æ ¸å¿ƒæ¦‚å¿µ
        concepts = entities.get('concepts', [])
        if concepts:
            lines.append("### ğŸ’­ æ ¸å¿ƒæ¦‚å¿µ")
            lines.append("")
            for concept in concepts[:10]:
                name = concept.get('name', 'æœªçŸ¥')
                mentions = concept.get('mentions', 0)
                atoms = concept.get('atoms', [])

                lines.append(f"- **{name}** (å‡ºç°åœ¨{len(atoms)}ä¸ªåŸå­ä¸­)")
            lines.append("")

        return "\n".join(lines)

    def _generate_statistics(
        self,
        atoms: List[Atom],
        segments: List[NarrativeSegment],
        entities: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆå†…å®¹ç»Ÿè®¡"""
        lines = ["## ğŸ“ˆ å†…å®¹ç»Ÿè®¡", ""]

        # åŸå­ç±»å‹åˆ†å¸ƒ
        atom_types = {}
        for atom in atoms:
            atom_types[atom.type] = atom_types.get(atom.type, 0) + 1

        lines.append("### åŸå­ç±»å‹åˆ†å¸ƒ")
        lines.append("")
        for atom_type, count in sorted(atom_types.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(atoms) * 100
            bar = "â–ˆ" * int(percentage / 5)
            lines.append(f"- **{atom_type}**: {count}ä¸ª ({percentage:.1f}%) {bar}")
        lines.append("")

        # ç‰‡æ®µè´¨é‡åˆ†å¸ƒ
        if segments:
            lines.append("### ç‰‡æ®µè´¨é‡åˆ†å¸ƒ")
            lines.append("")

            quality_bins = {"ä¼˜ç§€(â‰¥0.8)": 0, "è‰¯å¥½(0.6-0.8)": 0, "ä¸­ç­‰(<0.6)": 0}
            for seg in segments:
                if seg.quality_score >= 0.8:
                    quality_bins["ä¼˜ç§€(â‰¥0.8)"] += 1
                elif seg.quality_score >= 0.6:
                    quality_bins["è‰¯å¥½(0.6-0.8)"] += 1
                else:
                    quality_bins["ä¸­ç­‰(<0.6)"] += 1

            for quality, count in quality_bins.items():
                lines.append(f"- {quality}: {count}ä¸ªç‰‡æ®µ")
            lines.append("")

        return "\n".join(lines)

    def _generate_insights_summary(self, segments: List[NarrativeSegment]) -> str:
        """ç”ŸæˆAIæ´å¯Ÿæ±‡æ€»"""
        lines = ["## ğŸ¤– AI æ·±åº¦æ´å¯Ÿæ±‡æ€»", ""]

        if not segments:
            return "\n".join(lines + ["æš‚æ— æ´å¯Ÿæ•°æ®"])

        all_insights = []
        for seg in segments:
            if seg.ai_analysis.key_insights:
                all_insights.extend(seg.ai_analysis.key_insights)

        if not all_insights:
            return "\n".join(lines + ["æš‚æ— æ´å¯Ÿæ•°æ®"])

        lines.append("### ğŸ’¡ å…³é”®æ´å¯Ÿé›†åˆ")
        lines.append("")
        for i, insight in enumerate(all_insights[:15], 1):
            lines.append(f"{i}. {insight}")
        lines.append("")

        # å†…å®¹å¤ç”¨å»ºè®®æ±‡æ€»
        reuse_suggestions = []
        for seg in segments:
            if seg.ai_analysis.reuse_suggestions:
                reuse_suggestions.extend(seg.ai_analysis.reuse_suggestions)

        if reuse_suggestions:
            lines.append("### âœ‚ï¸ å†…å®¹å¤ç”¨å»ºè®®æ±‡æ€»")
            lines.append("")
            unique_suggestions = list(set(reuse_suggestions))
            for i, suggestion in enumerate(unique_suggestions[:10], 1):
                lines.append(f"{i}. {suggestion}")
            lines.append("")

        return "\n".join(lines)

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _format_duration(self, ms: int) -> str:
        """æ ¼å¼åŒ–æ—¶é•¿"""
        seconds = ms / 1000
        if seconds < 60:
            return f"{seconds:.0f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}å°æ—¶"

    def _ms_to_time(self, ms: int) -> str:
        """æ¯«ç§’è½¬æ—¶é—´å­—ç¬¦ä¸²"""
        td = timedelta(milliseconds=ms)
        hours, remainder = divmod(td.seconds, 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def _get_current_date(self) -> str:
        """è·å–å½“å‰æ—¥æœŸ"""
        from datetime import datetime
        return datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")

    def _get_entity_emoji(self, entity_type: str) -> str:
        """è·å–å®ä½“ç±»å‹çš„emoji"""
        emoji_map = {
            "persons": "ğŸ‘¤",
            "countries": "ğŸŒ",
            "organizations": "ğŸ¢",
            "time_points": "ğŸ“…",
            "events": "ğŸ“Œ",
            "concepts": "ğŸ’­"
        }
        return emoji_map.get(entity_type, "ğŸ“")

    def save(self, report: str, output_path: Path):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

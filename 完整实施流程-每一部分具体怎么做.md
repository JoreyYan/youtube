# è§†é¢‘å†…å®¹æ·±åº¦ç†è§£ç³»ç»Ÿ - å®Œæ•´å®æ–½æµç¨‹

æ¯ä¸€éƒ¨åˆ†çš„å…·ä½“åšæ³•æ‹†è§£

---

## ğŸ“‹ å®Œæ•´å®æ–½æµç¨‹

### é˜¶æ®µ0ï¼šå‡†å¤‡å·¥ä½œ

#### 0.1 ç¡®å®šæŠ€æœ¯æ ˆ

```bash
# æ¨èçš„æŠ€æœ¯ç»„åˆ

1. å­—å¹•è§£æï¼šPython srtåº“ æˆ– JavaScript
2. AIåˆ†æï¼šClaude API (Sonnet 3.5/3.7)
3. Embeddingï¼šOpenAI text-embedding-3-large
4. å‘é‡æ•°æ®åº“ï¼šChromadb (ç®€å•) æˆ– Qdrant (å¼ºå¤§)
5. å­˜å‚¨ï¼šJSONæ–‡ä»¶ + å‘é‡DB
```

#### 0.2 é¡¹ç›®ç»“æ„

```
subtitle-analyzer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ 1-parse/          # é˜¶æ®µ1ï¼šè§£æå­—å¹•
â”‚   â”œâ”€â”€ 2-atomize/        # é˜¶æ®µ2ï¼šåŸå­åŒ–
â”‚   â”œâ”€â”€ 3-analyze/        # é˜¶æ®µ3ï¼šè¯­ä¹‰åˆ†æ
â”‚   â”œâ”€â”€ 4-structure/      # é˜¶æ®µ4ï¼šç»“æ„é‡ç»„
â”‚   â”œâ”€â”€ 5-index/          # é˜¶æ®µ5ï¼šå»ºç«‹ç´¢å¼•
â”‚   â””â”€â”€ 6-generate/       # é˜¶æ®µ6ï¼šç”Ÿæˆæ–¹æ¡ˆ
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # åŸå§‹å­—å¹•æ–‡ä»¶
â”‚   â”œâ”€â”€ processed/        # å¤„ç†åçš„æ•°æ®
â”‚   â””â”€â”€ output/           # æœ€ç»ˆçŸ¥è¯†åº“
â”œâ”€â”€ prompts/              # AIæç¤ºè¯æ¨¡æ¿
â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
```

---

## ğŸ”§ é˜¶æ®µ1ï¼šè§£æå­—å¹• (Parse)

### ç›®æ ‡
æŠŠSRTå­—å¹•æ–‡ä»¶è§£ææˆç»“æ„åŒ–æ•°æ®

### è¾“å…¥
```
é‡‘ä¸‰è§’å¤§ä½¬4ï¼šç¼…åŒ—åŒé›„æ—¶ä»£1962-1998.srt
```

### å¤„ç†æ­¥éª¤

#### 1.1 è¯»å–SRTæ–‡ä»¶

```python
import srt
from datetime import timedelta

def parse_srt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    subtitles = list(srt.parse(content))

    parsed = []
    for sub in subtitles:
        parsed.append({
            "id": sub.index,
            "start": str(sub.start),
            "end": str(sub.end),
            "text": sub.content.strip(),
            "duration": (sub.end - sub.start).total_seconds()
        })

    return parsed

# ç»“æœç¤ºä¾‹
[
  {"id": 1, "start": "0:00:07", "end": "0:00:07.900", "text": "what", "duration": 0.9},
  {"id": 2, "start": "0:00:08.933", "end": "0:00:10.400", "text": "å¼€å§‹äº†æ²¡æœ‰å•Š", "duration": 1.467},
  ...
]
```

#### 1.2 æ¸…æ´—å’Œé¢„å¤„ç†

```python
def clean_subtitles(parsed):
    cleaned = []

    for item in parsed:
        text = item['text']

        # å»é™¤æ— æ„ä¹‰å†…å®¹
        if text in ['å‘ƒ', 'uh', 'um', '...']:
            continue

        # å»é™¤è¿‡çŸ­çš„ç‰‡æ®µï¼ˆ<0.5ç§’ï¼‰
        if item['duration'] < 0.5:
            continue

        # æ ‡å‡†åŒ–æ–‡æœ¬
        text = text.replace('\n', ' ').strip()

        cleaned.append({
            **item,
            'text': text
        })

    return cleaned
```

### è¾“å‡º
```json
{
  "video_id": "jinSanJiao_04",
  "total_subtitles": 3580,
  "total_duration": "02:30:45",
  "subtitles": [
    {"id": 1, "start": "0:00:08.933", "end": "0:00:10.400", "text": "å¼€å§‹äº†æ²¡æœ‰å•Š"},
    ...
  ]
}
```

**ä¿å­˜ä¸ºï¼š`data/processed/parsed_subtitles.json`**

---

## ğŸ§© é˜¶æ®µ2ï¼šåŸå­åŒ– (Atomize)

### ç›®æ ‡
æŠŠç¢ç‰‡åŒ–çš„å­—å¹•åˆå¹¶æˆ"è¯­ä¹‰å®Œæ•´çš„ä¿¡æ¯åŸå­"

### æ ¸å¿ƒæŒ‘æˆ˜
å¦‚ä½•åˆ¤æ–­å“ªäº›å¥å­åº”è¯¥åˆå¹¶ï¼Ÿ

### æ–¹æ³•ï¼šAIé©±åŠ¨çš„æ™ºèƒ½åˆå¹¶

#### 2.1 è®¾è®¡AIæç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/atomize.txt`

```
ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€æ®µç›´æ’­çš„å­—å¹•ç‰‡æ®µï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠå®ƒä»¬åˆå¹¶æˆ"è¯­ä¹‰å®Œæ•´çš„ä¿¡æ¯å•å…ƒ"ã€‚

ã€è§„åˆ™ã€‘
1. ä¸€ä¸ªä¿¡æ¯å•å…ƒåº”è¯¥è¡¨è¾¾ä¸€ä¸ªå®Œæ•´çš„æ„æ€ï¼ˆä¸€ä¸ªè§‚ç‚¹ã€ä¸€ä¸ªæ•…äº‹ç‰‡æ®µã€ä¸€æ¬¡äº’åŠ¨ï¼‰
2. è¯†åˆ«è¾¹ç•Œï¼š
   - ä¸»é¢˜è½¬æ¢ â†’ æ–°å•å…ƒ
   - é•¿æ—¶é—´åœé¡¿ï¼ˆ>5ç§’ï¼‰â†’ æ–°å•å…ƒ
   - ä»å™äº‹è½¬åˆ°äº’åŠ¨ â†’ æ–°å•å…ƒ
3. ä¿æŒæ—¶é—´æˆ³çš„è¿ç»­æ€§

ã€è¾“å…¥æ ¼å¼ã€‘
[æ—¶é—´æ®µ] æ–‡æœ¬å†…å®¹

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›JSONæ•°ç»„ï¼š
[
  {
    "atom_id": "A001",
    "start": "èµ·å§‹æ—¶é—´",
    "end": "ç»“æŸæ—¶é—´",
    "merged_text": "åˆå¹¶åçš„å®Œæ•´æ–‡æœ¬",
    "type": "å™è¿°å†å²/å›åº”å¼¹å¹•/å‘è¡¨è§‚ç‚¹/é—²èŠ",
    "completeness": "å®Œæ•´/éœ€è¦ä¸Šä¸‹æ–‡"
  }
]

ã€ç¤ºä¾‹è¾“å…¥ã€‘
[00:08:20] 1962å¹´
[00:08:25] å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’
[00:08:30] è¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº
[00:08:38] hello æµ·ç»µå®å®
[00:08:40] ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„

ã€ç¤ºä¾‹è¾“å‡ºã€‘
[
  {
    "atom_id": "A001",
    "start": "00:08:20",
    "end": "00:08:30",
    "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’ï¼Œè¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº",
    "type": "å™è¿°å†å²",
    "completeness": "å®Œæ•´"
  },
  {
    "atom_id": "A002",
    "start": "00:08:38",
    "end": "00:08:40",
    "merged_text": "hello æµ·ç»µå®å®",
    "type": "å›åº”å¼¹å¹•",
    "completeness": "å®Œæ•´"
  },
  {
    "atom_id": "A003",
    "start": "00:08:40",
    "end": "00:08:45",
    "merged_text": "ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„",
    "type": "å™è¿°å†å²",
    "completeness": "éœ€è¦ä¸Šä¸‹æ–‡"
  }
]
```

#### 2.2 æ‰¹é‡å¤„ç†ä»£ç 

```python
import anthropic
import json

def atomize_subtitles(subtitles, batch_size=50):
    """
    æŠŠå­—å¹•åˆ†æ‰¹é€ç»™AIè¿›è¡ŒåŸå­åŒ–
    """
    client = anthropic.Anthropic(api_key="your_api_key")
    atoms = []

    # è¯»å–æç¤ºè¯
    with open('prompts/atomize.txt', 'r', encoding='utf-8') as f:
        ATOMIZE_PROMPT = f.read()

    # æ¯æ¬¡å¤„ç†50æ¡å­—å¹•ï¼ˆçº¦2-3åˆ†é’Ÿå†…å®¹ï¼‰
    for i in range(0, len(subtitles), batch_size):
        batch = subtitles[i:i+batch_size]

        # æ„å»ºè¾“å…¥
        input_text = "\n".join([
            f"[{sub['start']}] {sub['text']}"
            for sub in batch
        ])

        # è°ƒç”¨Claude
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": f"{ATOMIZE_PROMPT}\n\nã€è¾“å…¥ã€‘\n{input_text}"
            }]
        )

        # è§£æJSON
        batch_atoms = json.loads(response.content[0].text)
        atoms.extend(batch_atoms)

        print(f"å¤„ç†è¿›åº¦: {i}/{len(subtitles)}")

    return atoms
```

#### 2.3 åå¤„ç†å’ŒéªŒè¯

```python
from datetime import datetime

def validate_atoms(atoms):
    """
    æ£€æŸ¥åŸå­çš„è´¨é‡
    """
    issues = []

    for i, atom in enumerate(atoms):
        # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        if i > 0:
            prev_end = parse_time(atoms[i-1]['end'])
            curr_start = parse_time(atom['start'])
            gap = (curr_start - prev_end).total_seconds()

            if gap > 30:  # è¶…è¿‡30ç§’çš„ç©ºéš™
                issues.append(f"Atom {atom['atom_id']}: å¤§é—´éš” {gap}ç§’")

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦
        if len(atom['merged_text']) < 10:
            issues.append(f"Atom {atom['atom_id']}: æ–‡æœ¬è¿‡çŸ­")

        if len(atom['merged_text']) > 500:
            issues.append(f"Atom {atom['atom_id']}: æ–‡æœ¬è¿‡é•¿ï¼Œå¯èƒ½éœ€è¦æ‹†åˆ†")

    return issues

def parse_time(time_str):
    """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
    # å®ç°æ—¶é—´è§£æé€»è¾‘
    pass
```

### è¾“å‡º
```json
{
  "total_atoms": 856,
  "atoms": [
    {
      "atom_id": "A001",
      "start": "00:00:08",
      "end": "00:00:25",
      "merged_text": "å¼€å§‹äº†æ²¡æœ‰å•Šï¼Œæˆ‘çœ‹çœ‹å¼€å§‹äº†æ²¡æœ‰...",
      "type": "å¼€åœºç¡®è®¤",
      "completeness": "å®Œæ•´",
      "source_subtitle_ids": [2, 3, 4, 5]
    },
    {
      "atom_id": "A012",
      "start": "00:08:20",
      "end": "00:09:15",
      "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’ï¼Œè¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº...",
      "type": "å™è¿°å†å²",
      "completeness": "å®Œæ•´",
      "source_subtitle_ids": [85, 86, 87, 88, 89]
    }
  ]
}
```

**ä¿å­˜ä¸ºï¼š`data/processed/atoms.json`**

---

## ğŸ·ï¸ é˜¶æ®µ3ï¼šè¯­ä¹‰åˆ†æ (Analyze)

### ç›®æ ‡
ç»™æ¯ä¸ªåŸå­æ‰“ä¸Šå¤šç»´åº¦çš„æ ‡ç­¾

### 3.1 ä¸»é¢˜æ ‡æ³¨

#### æç¤ºè¯è®¾è®¡

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/tag_topics.txt`

```
åˆ†æè¿™æ®µæ–‡æœ¬å±äºå“ªäº›ä¸»é¢˜ã€‚

ã€æ–‡æœ¬ã€‘
{atom_text}

ã€å¯é€‰ä¸»é¢˜ã€‘
ä¸€çº§ä¸»é¢˜ï¼š
- å†å²å™äº‹ï¼šè®²è¿°è¿‡å»çš„äº‹ä»¶
- äººç‰©åˆ†æï¼šåˆ†æäººç‰©æ€§æ ¼ã€åŠ¨æœºã€è¡Œä¸º
- è§‚ä¼—äº’åŠ¨ï¼šè¯»æ¥ä¿¡ã€å›åº”å¼¹å¹•ã€ç­”ç–‘
- å½“ä»£å¯¹æ¯”ï¼šå°†å†å²ä¸å½“ä»£è¿›è¡Œå¯¹æ¯”
- ä¸ªäººè§‚ç‚¹ï¼šåšä¸»çš„è¯„ä»·å’Œçœ‹æ³•
- é—²èŠè¿‡åœºï¼šå¼€åœºã€æŠ€æœ¯æ€§å¯¹è¯ã€æ— å…³å†…å®¹

äºŒçº§ä¸»é¢˜ï¼ˆå¦‚æœæ˜¯å†å²å™äº‹ï¼‰ï¼š
- 1960sèƒŒæ™¯/å¤æ²™å´›èµ·/ç½—æ˜Ÿæ±‰å¯¹æŠ—/æƒåŠ›æ–—äº‰/å†›äº‹å†²çª/ç»æµåšå¼ˆ/æœ€ç»ˆç»“å±€

ã€è¦æ±‚ã€‘
1. ä¸»é¢˜å¯å¤šé€‰
2. ç»™æ¯ä¸ªä¸»é¢˜æ‰“åˆ†ï¼ˆ1-10ï¼‰ï¼Œè¡¨ç¤ºç›¸å…³åº¦
3. æå–å…³é”®å®ä½“ï¼šäººç‰©ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶

ã€è¾“å‡ºJSONã€‘
{
  "primary_topic": "å†å²å™äº‹",
  "secondary_topics": ["å¤æ²™å´›èµ·", "å†›äº‹å†²çª"],
  "topic_scores": {
    "å†å²å™äº‹": 10,
    "å¤æ²™å´›èµ·": 9,
    "å†›äº‹å†²çª": 8
  },
  "entities": {
    "persons": ["å¤æ²™"],
    "locations": ["é‡‘ä¸‰è§’", "ç¼…ç”¸"],
    "time_points": ["1962å¹´"],
    "events": ["å›½æ°‘å…šæ’¤é€€"],
    "concepts": ["å†›äº‹å‰²æ®", "æƒåŠ›çœŸç©º"]
  }
}
```

#### æ‰¹é‡å¤„ç†

```python
def analyze_topics(atoms):
    """
    åˆ†ææ‰€æœ‰åŸå­çš„ä¸»é¢˜
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
        TOPIC_PROMPT = f.read()

    analyzed = []

    for atom in atoms:
        prompt = TOPIC_PROMPT.format(atom_text=atom['merged_text'])

        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )

        analysis = json.loads(response.content[0].text)

        analyzed.append({
            **atom,
            "topics": analysis
        })

        print(f"åˆ†æè¿›åº¦: {len(analyzed)}/{len(atoms)}")

    return analyzed
```

### 3.2 æƒ…æ„Ÿä¸èƒ½é‡æ ‡æ³¨

#### æç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/tag_emotion.txt`

```
åˆ†æè¿™æ®µæ–‡æœ¬çš„æƒ…æ„Ÿå’Œèƒ½é‡ã€‚

ã€æ–‡æœ¬ã€‘
{atom_text}

ã€åˆ†æç»´åº¦ã€‘
1. æƒ…æ„Ÿç±»å‹ï¼š
   - å®¢è§‚å™è¿°ï¼šå¹³é“ºç›´å™ï¼Œæ— æ˜æ˜¾æƒ…æ„Ÿ
   - æ¿€åŠ¨/æ„¤æ€’ï¼šè¯­æ°”å¼ºçƒˆï¼Œç”¨è¯æ¿€çƒˆ
   - å¹½é»˜/è°ƒä¾ƒï¼šå¼€ç©ç¬‘ã€è®½åˆº
   - åŒæƒ…/æ„Ÿæ…¨ï¼šè¡¨è¾¾åŒæƒ…æˆ–æ„Ÿæ…¨
   - æ‰¹åˆ¤/è´¨ç–‘ï¼šæ‰¹è¯„ã€è´¨ç–‘

2. èƒ½é‡å€¼ï¼ˆ1-10ï¼‰ï¼š
   - 1-3ï¼šå¹³é™ã€ä½æ²‰
   - 4-6ï¼šæ­£å¸¸å™è¿°
   - 7-8ï¼šæœ‰æ¿€æƒ…ã€æŠ•å…¥
   - 9-10ï¼šé«˜åº¦æ¿€åŠ¨ã€é«˜æ½®

3. è¶‹åŠ¿ï¼šé€’å¢/é€’å‡/å¹³ç¨³

ã€è¾“å‡ºJSONã€‘
{
  "emotion_type": "æ¿€åŠ¨",
  "energy_level": 9,
  "energy_trend": "é€’å¢",
  "indicators": ["ç”¨è¯æ¿€çƒˆ", "è¯­æ°”å¼ºè°ƒ", "é‡å¤å¼ºè°ƒ"]
}
```

### 3.3 å†…å®¹ä»·å€¼æ ‡æ³¨

#### æç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/tag_value.txt`

```
è¯„ä¼°è¿™æ®µå†…å®¹çš„ä»·å€¼å’Œå¯å‰ªè¾‘æ€§ã€‚

ã€æ–‡æœ¬ã€‘
{atom_text}

ã€è¯„ä¼°æ ‡å‡†ã€‘

1. ä¿¡æ¯å¯†åº¦ï¼ˆ1-10ï¼‰ï¼š
   - 1-3ï¼šé‡å¤ã€é—²èŠã€æŠ€æœ¯æ€§å¯¹è¯
   - 4-6ï¼šä¸€èˆ¬æ€§å™è¿°
   - 7-8ï¼šæœ‰ä»·å€¼çš„ä¿¡æ¯
   - 9-10ï¼šæ ¸å¿ƒä¿¡æ¯ã€å…³é”®è§‚ç‚¹ã€é‡‘å¥

2. å¯å‰ªè¾‘æ€§ï¼š
   - å¿…å‰ªï¼šæ ¸å¿ƒå†…å®¹ï¼Œä¸èƒ½åˆ 
   - å¯å‰ªï¼šæœ‰ä»·å€¼ä½†éå¿…éœ€
   - å¯åˆ ï¼šå†—ä½™ã€é‡å¤
   - å¿…åˆ ï¼šæ— æ„ä¹‰å¡«å……

3. ç‹¬ç«‹æ€§ï¼š
   - ç‹¬ç«‹ï¼šå¯å•ç‹¬æˆç‰‡ï¼Œä¸éœ€ä¸Šä¸‹æ–‡
   - éœ€é“ºå«ï¼šéœ€è¦å‰ç½®èƒŒæ™¯
   - éœ€è¡¥å……ï¼šéœ€è¦åç»­è§£é‡Š
   - ä¾èµ–ä¸Šä¸‹æ–‡ï¼šå¿…é¡»åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­

4. ç‰¹æ®Šä»·å€¼ï¼š
   - é‡‘å¥ï¼šå€¼å¾—å•ç‹¬æ‘˜å½•
   - é«˜æ½®ç‚¹ï¼šæƒ…èŠ‚è½¬æŠ˜æˆ–å†²çªé¡¶ç‚¹
   - äº‰è®®ç‚¹ï¼šå¯èƒ½å¼•å‘è®¨è®º
   - æ— 

ã€è¾“å‡ºJSONã€‘
{
  "information_density": 9,
  "editability": "å¿…å‰ª",
  "independence": "éœ€é“ºå«",
  "special_value": "é«˜æ½®ç‚¹",
  "reason": "æè¿°äº†å¤æ²™çš„å…³é”®å†›äº‹è¡ŒåŠ¨ï¼Œæ˜¯æƒåŠ›è½¬æŠ˜ç‚¹"
}
```

#### æ•´åˆåˆ†æä»£ç 

```python
def analyze_all_dimensions(atoms):
    """
    å¯¹æ¯ä¸ªåŸå­è¿›è¡Œå®Œæ•´çš„å¤šç»´åº¦åˆ†æ
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    # è¯»å–æ‰€æœ‰æç¤ºè¯
    with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
        topic_prompt = f.read()
    with open('prompts/tag_emotion.txt', 'r', encoding='utf-8') as f:
        emotion_prompt = f.read()
    with open('prompts/tag_value.txt', 'r', encoding='utf-8') as f:
        value_prompt = f.read()

    analyzed = []

    for i, atom in enumerate(atoms):
        text = atom['merged_text']

        # ä¸»é¢˜åˆ†æ
        topics = call_claude(topic_prompt.format(atom_text=text))

        # æƒ…æ„Ÿåˆ†æ
        emotion = call_claude(emotion_prompt.format(atom_text=text))

        # ä»·å€¼åˆ†æ
        value = call_claude(value_prompt.format(atom_text=text))

        analyzed.append({
            **atom,
            "topics": json.loads(topics),
            "emotion": json.loads(emotion),
            "value": json.loads(value)
        })

        print(f"åˆ†æè¿›åº¦: {i+1}/{len(atoms)}")

    return analyzed

def call_claude(prompt):
    """è°ƒç”¨Claude APIçš„è¾…åŠ©å‡½æ•°"""
    client = anthropic.Anthropic(api_key="your_api_key")
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text
```

### 3.4 ç”ŸæˆEmbeddingå‘é‡

```python
from openai import OpenAI

def generate_embeddings(atoms):
    """
    ä¸ºæ¯ä¸ªåŸå­ç”Ÿæˆè¯­ä¹‰å‘é‡
    """
    client = OpenAI(api_key="your_openai_key")

    texts = [atom['merged_text'] for atom in atoms]

    # æ‰¹é‡ç”Ÿæˆï¼ˆOpenAIæ”¯æŒæ‰¹é‡ï¼Œæ¯æ¬¡æœ€å¤š2048æ¡ï¼‰
    batch_size = 2048

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]

        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=batch_texts
        )

        for j, embedding_data in enumerate(response.data):
            atoms[i+j]['embedding'] = embedding_data.embedding

        print(f"Embeddingè¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")

    return atoms
```

### è¾“å‡º
```json
{
  "atoms": [
    {
      "atom_id": "A012",
      "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›...",
      "topics": {
        "primary_topic": "å†å²å™äº‹",
        "secondary_topics": ["1960sèƒŒæ™¯"],
        "topic_scores": {"å†å²å™äº‹": 10, "1960sèƒŒæ™¯": 9},
        "entities": {
          "persons": ["å›½æ°‘å…šæ®‹å†›"],
          "locations": ["é‡‘ä¸‰è§’"],
          "time_points": ["1962å¹´"],
          "events": ["å›½æ°‘å…šæ’¤é€€"],
          "concepts": ["å†›äº‹å‰²æ®"]
        }
      },
      "emotion": {
        "emotion_type": "å®¢è§‚å™è¿°",
        "energy_level": 6,
        "energy_trend": "å¹³ç¨³"
      },
      "value": {
        "information_density": 9,
        "editability": "å¿…å‰ª",
        "independence": "éœ€é“ºå«",
        "special_value": "å…³é”®èƒŒæ™¯"
      },
      "embedding": [0.023, 0.451, -0.234, ...]
    }
  ]
}
```

**ä¿å­˜ä¸ºï¼š`data/processed/analyzed_atoms.json`**

---

## ğŸ—ï¸ é˜¶æ®µ4ï¼šç»“æ„é‡ç»„ (Structure)

### ç›®æ ‡
æŠŠåˆ†æ•£çš„åŸå­é‡æ–°ç»„ç»‡æˆç»“æ„åŒ–çŸ¥è¯†

### 4.1 æ„å»ºå®ä½“å¡ç‰‡

#### æç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/build_entity.txt`

```
åŸºäºä»¥ä¸‹åŸå­ï¼Œæ„å»º"{entity_name}"çš„å®ä½“å¡ç‰‡ã€‚

ã€ç›¸å…³åŸå­ã€‘
{related_atoms_text}

ã€è¦æ±‚ã€‘
1. ç”Ÿæˆäººç‰©/åœ°ç‚¹/äº‹ä»¶çš„å®Œæ•´ç”»åƒ
2. æŒ‰ä¸åŒfacetï¼ˆé¢å‘ï¼‰ç»„ç»‡ä¿¡æ¯
3. æå–æ—¶é—´çº¿
4. è¯†åˆ«å…³ç³»ç½‘ç»œ
5. ç”ŸæˆAIæ€»ç»“

ã€è¾“å‡ºJSONã€‘
{
  "entity_id": "person_kunsha",
  "type": "person",
  "name": "å¤æ²™",
  "profile": {
    "basic_info": "1934-2007ï¼Œé‡‘ä¸‰è§’å†›é˜€...",
    "key_characteristics": ["æƒè°‹é«˜æ‰‹", "å–„ç”¨åœ°ç¼˜æ”¿æ²»"],
    "historical_significance": "..."
  },
  "facets": {
    "æ€§æ ¼ç‰¹è´¨": {
      "summary": "...",
      "supporting_atoms": ["A023", "A045"]
    },
    "æƒåŠ›æ‰‹æ®µ": {
      "summary": "...",
      "supporting_atoms": ["A034", "A056"]
    }
  },
  "timeline": [
    {"year": "1962", "event": "è¿›å…¥é‡‘ä¸‰è§’", "atoms": ["A012"]},
    {"year": "1985", "event": "å…³é”®å†›äº‹è¡ŒåŠ¨", "atoms": ["A045"]}
  ],
  "relationships": [
    {
      "target": "ç½—æ˜Ÿæ±‰",
      "relation": "å¯¹æ‰‹",
      "description": "é•¿æœŸæƒåŠ›ç«äº‰",
      "atoms": ["A023", "A067"]
    }
  ]
}
```

#### å®ç°ä»£ç 

```python
def build_entities(analyzed_atoms):
    """
    è‡ªåŠ¨è¯†åˆ«å’Œæ„å»ºæ‰€æœ‰å®ä½“
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/build_entity.txt', 'r', encoding='utf-8') as f:
        entity_prompt = f.read()

    # 1. æ”¶é›†æ‰€æœ‰æåˆ°çš„å®ä½“
    entity_mentions = {}

    for atom in analyzed_atoms:
        persons = atom['topics']['entities']['persons']
        for person in persons:
            if person not in entity_mentions:
                entity_mentions[person] = []
            entity_mentions[person].append(atom['atom_id'])

    # 2. ä¸ºæ¯ä¸ªå®ä½“æ„å»ºå¡ç‰‡
    entities = []

    for entity_name, atom_ids in entity_mentions.items():
        if len(atom_ids) < 3:  # è‡³å°‘å‡ºç°3æ¬¡æ‰å»ºå¡ç‰‡
            continue

        # è·å–ç›¸å…³åŸå­çš„æ–‡æœ¬
        related_atoms = [a for a in analyzed_atoms if a['atom_id'] in atom_ids]
        atoms_text = "\n\n".join([
            f"[{a['atom_id']}] {a['merged_text']}"
            for a in related_atoms
        ])

        # è°ƒç”¨AIæ„å»ºå®ä½“å¡ç‰‡
        prompt = entity_prompt.format(
            entity_name=entity_name,
            related_atoms_text=atoms_text
        )

        response = call_claude(prompt)
        entity_card = json.loads(response)

        # ç”Ÿæˆå®ä½“çš„embedding
        entity_summary = entity_card['profile']['basic_info']
        entity_card['embedding'] = generate_single_embedding(entity_summary)

        entities.append(entity_card)

        print(f"æ„å»ºå®ä½“: {entity_name}")

    return entities

def generate_single_embedding(text):
    """ä¸ºå•ä¸ªæ–‡æœ¬ç”Ÿæˆembedding"""
    client = OpenAI(api_key="your_openai_key")
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=[text]
    )
    return response.data[0].embedding
```

### 4.2 æ„å»ºä¸»é¢˜ç½‘ç»œ

#### æç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/build_topic_network.txt`

```
åŸºäºæ‰€æœ‰åŸå­ï¼Œæ„å»ºä¸»é¢˜ç½‘ç»œã€‚

ã€ä¸»é¢˜åç§°ã€‘
{topic_name}

ã€ä¸»é¢˜ç»Ÿè®¡ã€‘
å‡ºç°æ¬¡æ•°: {count}

ã€ç›¸å…³åŸå­ç¤ºä¾‹ã€‘
{sample_atoms}

ã€è¦æ±‚ã€‘
1. è¯†åˆ«è¿™ä¸ªä¸»é¢˜çš„å­ä¸»é¢˜
2. ç”Ÿæˆä¸»é¢˜å®šä¹‰å’Œæè¿°
3. è®¾è®¡å™äº‹æ¨¡æ¿

ã€è¾“å‡ºJSONã€‘
{
  "topic_id": "theme_power_struggle",
  "name": "æƒåŠ›æ–—äº‰",
  "definition": "é‡‘ä¸‰è§’åœ°åŒºä¸åŒåŠ¿åŠ›ä¹‹é—´çš„æ§åˆ¶æƒäº‰å¤º",
  "sub_topics": [
    {
      "name": "å†›äº‹å¯¹æŠ—",
      "atoms": ["A045", "A067"],
      "description": "é€šè¿‡æ­¦è£…å†²çªäº‰å¤ºåœ°ç›˜"
    }
  ],
  "narrative_templates": [
    {
      "template_name": "åŒé›„äº‰éœ¸",
      "structure": "èƒŒæ™¯ â†’ å´›èµ· â†’ å¯¹æŠ— â†’ ç»“å±€",
      "estimated_duration": "15:00"
    }
  ]
}
```

#### å®ç°ä»£ç 

```python
def build_topic_network(analyzed_atoms):
    """
    æ„å»ºä¸»é¢˜ç½‘ç»œ
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/build_topic_network.txt', 'r', encoding='utf-8') as f:
        topic_prompt = f.read()

    # 1. ç»Ÿè®¡æ‰€æœ‰ä¸»é¢˜å‡ºç°é¢‘ç‡
    topic_stats = {}
    topic_atoms = {}

    for atom in analyzed_atoms:
        topics = atom['topics']['secondary_topics']
        for topic in topics:
            if topic not in topic_stats:
                topic_stats[topic] = 0
                topic_atoms[topic] = []
            topic_stats[topic] += 1
            topic_atoms[topic].append(atom['atom_id'])

    # 2. ä¸ºä¸»è¦ä¸»é¢˜ï¼ˆå‡ºç°10æ¬¡ä»¥ä¸Šï¼‰å»ºç«‹è¯¦ç»†ç½‘ç»œ
    topic_network = []

    for topic_name, count in topic_stats.items():
        if count < 10:
            continue

        related_atoms = [a for a in analyzed_atoms
                        if a['atom_id'] in topic_atoms[topic_name]]

        # å–å‰10ä¸ªåŸå­ä½œä¸ºç¤ºä¾‹
        sample_text = "\n\n".join([
            f"[{a['atom_id']}] {a['merged_text']}"
            for a in related_atoms[:10]
        ])

        # è°ƒç”¨AIæ„å»ºä¸»é¢˜ç½‘ç»œ
        prompt = topic_prompt.format(
            topic_name=topic_name,
            count=count,
            sample_atoms=sample_text
        )

        response = call_claude(prompt)
        topic_card = json.loads(response)

        # è¡¥å……å®Œæ•´çš„atomåˆ—è¡¨
        topic_card['all_atoms'] = topic_atoms[topic_name]

        # ç”Ÿæˆembedding
        topic_card['embedding'] = generate_single_embedding(
            topic_card['definition']
        )

        topic_network.append(topic_card)

        print(f"æ„å»ºä¸»é¢˜: {topic_name} ({count}æ¬¡)")

    return topic_network
```

### 4.3 è¯†åˆ«å™äº‹ç‰‡æ®µ

#### æç¤ºè¯

åˆ›å»ºæ–‡ä»¶ï¼š`prompts/identify_narratives.txt`

```
è¯†åˆ«è§†é¢‘ä¸­å®Œæ•´çš„å™äº‹ç‰‡æ®µã€‚

ã€åŸå­åºåˆ—ã€‘
{sequential_atoms}

ã€è¦æ±‚ã€‘
1. æ‰¾å‡º"å®Œæ•´æ•…äº‹"ï¼šæœ‰å¼€å¤´ã€å‘å±•ã€ç»“å±€
2. è¯†åˆ«å™äº‹ç»“æ„
3. è¯„ä¼°å®Œæ•´æ€§å’Œç‹¬ç«‹æ€§

ã€è¾“å‡ºJSONã€‘
{
  "segment_id": "narrative_001",
  "title": "å¤æ²™çš„å´›èµ·ï¼ˆ1962-1985ï¼‰",
  "atoms": ["A012", "A023", "A045", "A056"],
  "structure": {
    "setup": {"atoms": ["A012"], "duration": "02:30"},
    "conflict": {"atoms": ["A023", "A034"], "duration": "08:15"},
    "climax": {"atoms": ["A045"], "duration": "03:10"}
  },
  "completeness": "å®Œæ•´",
  "independence": "éœ€è¦å‰ç½®èƒŒæ™¯"
}
```

#### å®ç°ä»£ç 

```python
def identify_narratives(analyzed_atoms):
    """
    è¯†åˆ«å™äº‹ç‰‡æ®µ
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/identify_narratives.txt', 'r', encoding='utf-8') as f:
        narrative_prompt = f.read()

    # æŒ‰æ—¶é—´é¡ºåºå‡†å¤‡åŸå­åºåˆ—
    atoms_text = "\n\n".join([
        f"[{a['atom_id']}] {a['start']}-{a['end']}: {a['merged_text'][:100]}..."
        for a in analyzed_atoms
    ])

    prompt = narrative_prompt.format(sequential_atoms=atoms_text)

    response = call_claude(prompt)
    narratives = json.loads(response)

    return narratives
```

### è¾“å‡º

ç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ï¼š

**1. entities.json**
```json
{
  "entities": [
    {
      "entity_id": "person_kunsha",
      "type": "person",
      "name": "å¤æ²™",
      "profile": {...},
      "facets": {...},
      "timeline": [...],
      "relationships": [...],
      "embedding": [...]
    }
  ]
}
```

**2. topics.json**
```json
{
  "topics": [
    {
      "topic_id": "theme_power_struggle",
      "name": "æƒåŠ›æ–—äº‰",
      "definition": "...",
      "sub_topics": [...],
      "narrative_templates": [...],
      "all_atoms": ["A023", "A045", ...],
      "embedding": [...]
    }
  ]
}
```

**3. narratives.json**
```json
{
  "narratives": [
    {
      "segment_id": "narrative_001",
      "title": "å¤æ²™çš„å´›èµ·",
      "atoms": ["A012", "A023", "A045"],
      "structure": {...}
    }
  ]
}
```

---

## ğŸ—‚ï¸ é˜¶æ®µ5ï¼šå»ºç«‹ç´¢å¼• (Index)

### ç›®æ ‡
å»ºç«‹å¤šç»´åº¦æ£€ç´¢ç³»ç»Ÿ

### 5.1 è¯­ä¹‰å‘é‡ç´¢å¼•

```python
import chromadb

def build_semantic_index(analyzed_atoms, entities, topics):
    """
    æŠŠæ‰€æœ‰embeddingå­˜å…¥å‘é‡æ•°æ®åº“
    """
    client = chromadb.PersistentClient(path="data/output/vector_db")

    # åˆ›å»ºcollection
    collection = client.create_collection(
        name="video_knowledge",
        metadata={"description": "é‡‘ä¸‰è§’å¤§ä½¬4çš„çŸ¥è¯†åº“"}
    )

    # æ·»åŠ åŸå­
    for atom in analyzed_atoms:
        collection.add(
            ids=[atom['atom_id']],
            embeddings=[atom['embedding']],
            metadatas=[{
                "type": "atom",
                "start_time": atom['start'],
                "end_time": atom['end'],
                "primary_topic": atom['topics']['primary_topic'],
                "value_score": atom['value']['information_density']
            }],
            documents=[atom['merged_text']]
        )

    # æ·»åŠ å®ä½“
    for entity in entities:
        collection.add(
            ids=[entity['entity_id']],
            embeddings=[entity['embedding']],
            metadatas=[{
                "type": "entity",
                "entity_type": entity['type'],
                "name": entity['name']
            }],
            documents=[entity['profile']['basic_info']]
        )

    # æ·»åŠ ä¸»é¢˜
    for topic in topics:
        collection.add(
            ids=[topic['topic_id']],
            embeddings=[topic['embedding']],
            metadatas=[{
                "type": "topic",
                "name": topic['name']
            }],
            documents=[topic['definition']]
        )

    print(f"âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±{collection.count()}æ¡è®°å½•")
```

### 5.2 ç»“æ„åŒ–ç´¢å¼•

```python
def build_structured_index(analyzed_atoms):
    """
    æ„å»ºå¤šç»´åº¦çš„ç»“æ„åŒ–ç´¢å¼•
    """
    indexes = {
        "by_time": {},
        "by_person": {},
        "by_topic": {},
        "by_value": {},
        "by_emotion": {}
    }

    # æŒ‰æ—¶é—´ç´¢å¼•ï¼ˆæŒ‰10åˆ†é’Ÿåˆ†ç»„ï¼‰
    for atom in analyzed_atoms:
        time_bucket = get_time_bucket(atom['start'], bucket_size=600)
        if time_bucket not in indexes['by_time']:
            indexes['by_time'][time_bucket] = []
        indexes['by_time'][time_bucket].append(atom['atom_id'])

    # æŒ‰äººç‰©ç´¢å¼•
    for atom in analyzed_atoms:
        persons = atom['topics']['entities']['persons']
        for person in persons:
            if person not in indexes['by_person']:
                indexes['by_person'][person] = []
            indexes['by_person'][person].append(atom['atom_id'])

    # æŒ‰ä¸»é¢˜ç´¢å¼•
    for atom in analyzed_atoms:
        topics = atom['topics']['secondary_topics']
        for topic in topics:
            if topic not in indexes['by_topic']:
                indexes['by_topic'][topic] = []
            indexes['by_topic'][topic].append(atom['atom_id'])

    # æŒ‰ä»·å€¼ç´¢å¼•
    for atom in analyzed_atoms:
        value = atom['value']['information_density']
        value_tier = f"{(value-1)//2*2+1}-{(value-1)//2*2+2}åˆ†"
        if value_tier not in indexes['by_value']:
            indexes['by_value'][value_tier] = []
        indexes['by_value'][value_tier].append(atom['atom_id'])

    # æŒ‰æƒ…æ„Ÿç´¢å¼•
    for atom in analyzed_atoms:
        emotion = atom['emotion']['emotion_type']
        if emotion not in indexes['by_emotion']:
            indexes['by_emotion'][emotion] = []
        indexes['by_emotion'][emotion].append(atom['atom_id'])

    return indexes

def get_time_bucket(time_str, bucket_size=600):
    """å°†æ—¶é—´è½¬æ¢ä¸ºæ—¶é—´æ¡¶ï¼ˆç§’ï¼‰"""
    # è§£ææ—¶é—´å­—ç¬¦ä¸²ï¼Œè¿”å›æ‰€å±çš„æ¡¶
    # ä¾‹å¦‚ 00:05:30 â†’ "00:00-00:10"
    pass
```

### 5.3 çŸ¥è¯†å›¾è°±

```python
import networkx as nx

def build_knowledge_graph(entities):
    """
    æ„å»ºçŸ¥è¯†å›¾è°±
    """
    G = nx.DiGraph()

    # æ·»åŠ å®ä½“èŠ‚ç‚¹
    for entity in entities:
        G.add_node(
            entity['entity_id'],
            type=entity['type'],
            name=entity['name'],
            data=entity
        )

    # æ·»åŠ å…³ç³»è¾¹
    for entity in entities:
        if 'relationships' in entity:
            for rel in entity['relationships']:
                # æ‰¾åˆ°ç›®æ ‡å®ä½“çš„ID
                target_id = None
                for e in entities:
                    if e['name'] == rel['target']:
                        target_id = e['entity_id']
                        break

                if target_id:
                    G.add_edge(
                        entity['entity_id'],
                        target_id,
                        relation=rel['relation'],
                        atoms=rel['atoms'],
                        description=rel.get('description', '')
                    )

    # ä¿å­˜ä¸ºJSON
    graph_data = nx.node_link_data(G)

    return graph_data
```

### è¾“å‡º

**1. vector_db/** (å‘é‡æ•°æ®åº“ç›®å½•)
```
vector_db/
â”œâ”€â”€ chroma.sqlite3
â””â”€â”€ [å…¶ä»–chromadbæ–‡ä»¶]
```

**2. indexes/structured.json**
```json
{
  "by_time": {
    "00:00-00:10": ["A001", "A002", "A003"],
    "00:10-00:20": ["A015", "A016", "A020"]
  },
  "by_person": {
    "å¤æ²™": ["A012", "A023", "A045"],
    "ç½—æ˜Ÿæ±‰": ["A023", "A034"]
  },
  "by_topic": {
    "å†›äº‹å†²çª": ["A045", "A067"],
    "æƒåŠ›æ–—äº‰": ["A023", "A045", "A067"]
  },
  "by_value": {
    "9-10åˆ†": ["A045", "A067"],
    "7-8åˆ†": ["A023", "A034"]
  },
  "by_emotion": {
    "æ¿€åŠ¨": ["A045", "A089"],
    "å®¢è§‚": ["A012", "A023"]
  }
}
```

**3. indexes/graph.json**
```json
{
  "directed": true,
  "nodes": [
    {
      "id": "person_kunsha",
      "type": "person",
      "name": "å¤æ²™"
    },
    {
      "id": "person_luoxinghan",
      "type": "person",
      "name": "ç½—æ˜Ÿæ±‰"
    }
  ],
  "links": [
    {
      "source": "person_kunsha",
      "target": "person_luoxinghan",
      "relation": "å¯¹æ‰‹",
      "atoms": ["A023", "A067"]
    }
  ]
}
```

---

## ğŸ¬ é˜¶æ®µ6ï¼šç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ (Generate)

### ç›®æ ‡
åŸºäºçŸ¥è¯†åº“ï¼Œè‡ªåŠ¨ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ

### 6.1 æŸ¥è¯¢æ¥å£

```python
import chromadb
import json

class VideoKnowledgeBase:
    def __init__(self, data_path):
        # åŠ è½½JSONæ•°æ®
        with open(f"{data_path}/analyzed_atoms.json", 'r', encoding='utf-8') as f:
            self.atoms = json.load(f)
        with open(f"{data_path}/entities.json", 'r', encoding='utf-8') as f:
            self.entities = json.load(f)
        with open(f"{data_path}/topics.json", 'r', encoding='utf-8') as f:
            self.topics = json.load(f)
        with open(f"{data_path}/narratives.json", 'r', encoding='utf-8') as f:
            self.narratives = json.load(f)
        with open(f"{data_path}/indexes/structured.json", 'r', encoding='utf-8') as f:
            self.indexes = json.load(f)

        # è¿æ¥å‘é‡æ•°æ®åº“
        self.vector_client = chromadb.PersistentClient(f"{data_path}/vector_db")
        self.collection = self.vector_client.get_collection("video_knowledge")

    def semantic_search(self, query, top_k=10, filter_type=None):
        """è¯­ä¹‰æœç´¢"""
        # ç”ŸæˆæŸ¥è¯¢çš„embedding
        query_embedding = generate_single_embedding(query)

        # æœç´¢
        where_filter = {"type": filter_type} if filter_type else None
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where_filter
        )

        return results

    def get_entity(self, entity_name):
        """è·å–å®ä½“å¡ç‰‡"""
        for entity in self.entities['entities']:
            if entity['name'] == entity_name:
                return entity
        return None

    def get_atoms_by_topic(self, topic_name):
        """æŒ‰ä¸»é¢˜è·å–åŸå­"""
        atom_ids = self.indexes['by_topic'].get(topic_name, [])
        return [a for a in self.atoms['atoms'] if a['atom_id'] in atom_ids]

    def get_atoms_by_person(self, person_name):
        """æŒ‰äººç‰©è·å–åŸå­"""
        atom_ids = self.indexes['by_person'].get(person_name, [])
        return [a for a in self.atoms['atoms'] if a['atom_id'] in atom_ids]

    def get_high_value_atoms(self, min_score=8):
        """è·å–é«˜ä»·å€¼åŸå­"""
        return [a for a in self.atoms['atoms']
                if a['value']['information_density'] >= min_score]

    def get_atom_by_id(self, atom_id):
        """æ ¹æ®IDè·å–åŸå­"""
        for atom in self.atoms['atoms']:
            if atom['atom_id'] == atom_id:
                return atom
        return None
```

### 6.2 ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ

```python
def generate_edit_plan(kb, user_request):
    """
    æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    # æç¤ºè¯
    prompt = f"""
åŸºäºè§†é¢‘çŸ¥è¯†åº“ï¼Œä¸ºä»¥ä¸‹éœ€æ±‚ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆï¼š

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_request}

ã€å¯ç”¨èµ„æºã€‘
- æ€»è®¡{len(kb.atoms['atoms'])}ä¸ªä¿¡æ¯åŸå­
- {len(kb.entities['entities'])}ä¸ªå®ä½“
- {len(kb.topics['topics'])}ä¸ªä¸»é¢˜

ã€ä»»åŠ¡ã€‘
1. ç†è§£ç”¨æˆ·éœ€æ±‚
2. ä»çŸ¥è¯†åº“ä¸­é€‰æ‹©åˆé€‚çš„ç´ æ
3. è®¾è®¡å™äº‹ç»“æ„
4. ç”Ÿæˆå®Œæ•´çš„å‰ªè¾‘æ–¹æ¡ˆ

ã€è¾“å‡ºJSONã€‘
{{
  "plan_name": "æ–¹æ¡ˆåç§°",
  "target_duration": "ç›®æ ‡æ—¶é•¿",
  "narrative_structure": "å™äº‹ç»“æ„æè¿°",
  "required_atoms": ["A001", "A012", ...],
  "timeline": [
    {{"order": 1, "type": "clip", "atom_id": "A012"}},
    {{"order": 2, "type": "transition", "content": "è½¬åœºæ–‡æ¡ˆ"}},
    ...
  ],
  "editing_suggestions": ["å»ºè®®1", "å»ºè®®2"]
}}
"""

    # Step 1: è¯­ä¹‰æœç´¢ç›¸å…³å†…å®¹
    search_results = kb.semantic_search(user_request, top_k=20, filter_type="atom")

    # Step 2: å‡†å¤‡ä¸Šä¸‹æ–‡
    relevant_atoms = []
    for atom_id in search_results['ids'][0]:
        atom = kb.get_atom_by_id(atom_id)
        if atom:
            relevant_atoms.append({
                "id": atom['atom_id'],
                "text": atom['merged_text'][:200],
                "time": f"{atom['start']}-{atom['end']}",
                "value": atom['value']['information_density']
            })

    context = {
        "relevant_atoms": relevant_atoms,
        "available_topics": [t['name'] for t in kb.topics['topics']]
    }

    # Step 3: è°ƒç”¨AIç”Ÿæˆæ–¹æ¡ˆ
    full_prompt = prompt + "\n\nã€ç›¸å…³ç´ æã€‘\n" + json.dumps(context, ensure_ascii=False, indent=2)

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=4000,
        messages=[{"role": "user", "content": full_prompt}]
    )

    plan = json.loads(response.content[0].text)

    # Step 4: ä¸°å¯Œæ–¹æ¡ˆç»†èŠ‚
    plan = enrich_plan(kb, plan)

    return plan

def enrich_plan(kb, plan):
    """
    ä¸°å¯Œå‰ªè¾‘æ–¹æ¡ˆçš„ç»†èŠ‚
    """
    # è·å–æ‰€æœ‰æ¶‰åŠçš„åŸå­çš„å®Œæ•´ä¿¡æ¯
    atom_ids = plan['required_atoms']
    atoms_detail = []
    total_seconds = 0

    for atom_id in atom_ids:
        atom = kb.get_atom_by_id(atom_id)
        if atom:
            atoms_detail.append(atom)
            # è®¡ç®—æ—¶é•¿
            duration = calculate_duration(atom['start'], atom['end'])
            total_seconds += duration

    plan['actual_duration'] = format_duration(total_seconds)
    plan['atoms_detail'] = atoms_detail

    # ç”Ÿæˆffmpegå‘½ä»¤
    plan['ffmpeg_commands'] = []
    for i, atom in enumerate(atoms_detail):
        cmd = f'ffmpeg -i input.mp4 -ss {atom["start"]} -to {atom["end"]} -c copy clip_{i+1:03d}.mp4'
        plan['ffmpeg_commands'].append(cmd)

    return plan

def calculate_duration(start, end):
    """è®¡ç®—æ—¶é•¿ï¼ˆç§’ï¼‰"""
    # å®ç°æ—¶é—´å·®è®¡ç®—
    pass

def format_duration(seconds):
    """æ ¼å¼åŒ–æ—¶é•¿"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"
```

### 6.3 äº¤äº’å¼ä¼˜åŒ–

```python
def interactive_editing(kb, initial_request):
    """
    äº¤äº’å¼å‰ªè¾‘æ–¹æ¡ˆç”Ÿæˆ
    """
    # ç”Ÿæˆåˆå§‹æ–¹æ¡ˆ
    print("æ­£åœ¨ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ...")
    plan = generate_edit_plan(kb, initial_request)

    print(f"\nâœ“ ç”Ÿæˆæ–¹æ¡ˆï¼š{plan['plan_name']}")
    print(f"  æ—¶é•¿ï¼š{plan['actual_duration']}")
    print(f"  åŒ…å«{len(plan['required_atoms'])}ä¸ªç‰‡æ®µ")
    print(f"\nå™äº‹ç»“æ„ï¼š{plan['narrative_structure']}")

    # ç”¨æˆ·åé¦ˆå¾ªç¯
    while True:
        feedback = input("\néœ€è¦è°ƒæ•´å—ï¼Ÿ(ä¾‹å¦‚ï¼š'å¤ªé•¿äº†ï¼Œç¼©çŸ­åˆ°10åˆ†é’Ÿ' æˆ–è¾“å…¥ 'done' å®Œæˆ)ï¼š")

        if feedback.lower() == "done":
            break

        # æ ¹æ®åé¦ˆè°ƒæ•´æ–¹æ¡ˆ
        print("æ­£åœ¨è°ƒæ•´æ–¹æ¡ˆ...")

        adjustment_prompt = f"""
å½“å‰æ–¹æ¡ˆï¼š
{json.dumps(plan, ensure_ascii=False, indent=2)}

ç”¨æˆ·åé¦ˆï¼š
{feedback}

è¯·æ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´æ–¹æ¡ˆï¼Œä¿æŒJSONæ ¼å¼è¾“å‡ºã€‚
"""

        client = anthropic.Anthropic(api_key="your_api_key")
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{"role": "user", "content": adjustment_prompt}]
        )

        plan = json.loads(response.content[0].text)
        plan = enrich_plan(kb, plan)

        print(f"\nâœ“ å·²è°ƒæ•´ï¼š{plan['plan_name']}")
        print(f"  æ–°æ—¶é•¿ï¼š{plan['actual_duration']}")

    return plan
```

### è¾“å‡ºç¤ºä¾‹

```json
{
  "plan_id": "plan_001",
  "plan_name": "å¤æ²™ï¼šä»å°å†›é˜€åˆ°é‡‘ä¸‰è§’ä¹‹ç‹",
  "target_duration": "15:00",
  "actual_duration": "15:23",

  "narrative_structure": "ä¸‰å¹•å‰§ï¼šèµ·æº(3min) â†’ å´›èµ·(8min) â†’ å·…å³°(4min)",

  "required_atoms": ["A012", "A023", "A045", "A056", "A078", "A089"],

  "timeline": [
    {
      "order": 1,
      "type": "clip",
      "atom_id": "A012",
      "purpose": "ä»‹ç»1962å¹´èƒŒæ™¯"
    },
    {
      "order": 2,
      "type": "transition",
      "content": "åœ¨è¿™ä¸ªæƒåŠ›çœŸç©ºä¸­ï¼Œä¸€ä¸ªåå«å¤æ²™çš„å¹´è½»äººå¼€å§‹å´­éœ²å¤´è§’...",
      "voiceover": true
    },
    {
      "order": 3,
      "type": "clip",
      "atom_id": "A023",
      "purpose": "å¤æ²™åˆæœŸå‘å±•"
    }
  ],

  "atoms_detail": [
    {
      "atom_id": "A012",
      "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’...",
      "start": "00:08:20",
      "end": "00:09:15"
    }
  ],

  "editing_suggestions": [
    "ç‰‡æ®µ1å’Œ2ä¹‹é—´å¯æ·»åŠ é‡‘ä¸‰è§’åœ°å›¾åŠ¨ç”»",
    "ç‰‡æ®µ3(A045)æ˜¯é«˜æ½®ï¼Œå»ºè®®é…æ¿€æ˜‚BGM",
    "è€ƒè™‘ä½¿ç”¨å†å²ç…§ç‰‡ä½œä¸ºB-roll"
  ],

  "ffmpeg_commands": [
    "ffmpeg -i input.mp4 -ss 00:08:20 -to 00:09:15 -c copy clip_001.mp4",
    "ffmpeg -i input.mp4 -ss 00:18:05 -to 00:19:30 -c copy clip_002.mp4"
  ]
}
```

**ä¿å­˜ä¸ºï¼š`output/edit_plans/plan_001.json`**

---

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹ï¼šç«¯åˆ°ç«¯æµç¨‹

### ä¸»ç¨‹åº

```python
# main.py

import json
import os
from datetime import datetime

def process_video_subtitle(srt_path, output_dir):
    """
    å®Œæ•´å¤„ç†æµç¨‹
    """
    print("=" * 60)
    print("è§†é¢‘å­—å¹•æ·±åº¦åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å…¥æ–‡ä»¶: {srt_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/indexes", exist_ok=True)

    # é˜¶æ®µ1ï¼šè§£æ
    print("\n[1/6] è§£æå­—å¹•æ–‡ä»¶...")
    parsed = parse_srt(srt_path)
    cleaned = clean_subtitles(parsed)
    save_json({'subtitles': cleaned}, f"{output_dir}/parsed_subtitles.json")
    print(f"âœ“ è§£æå®Œæˆï¼š{len(cleaned)}æ¡å­—å¹•")

    # é˜¶æ®µ2ï¼šåŸå­åŒ–
    print("\n[2/6] åŸå­åŒ–å¤„ç†ï¼ˆé¢„è®¡30-40åˆ†é’Ÿï¼‰...")
    print("    æ­£åœ¨è°ƒç”¨AIåˆå¹¶å­—å¹•ç‰‡æ®µ...")
    atoms = atomize_subtitles(cleaned)
    save_json({'atoms': atoms}, f"{output_dir}/atoms.json")
    print(f"âœ“ åŸå­åŒ–å®Œæˆï¼š{len(atoms)}ä¸ªä¿¡æ¯åŸå­")

    # é˜¶æ®µ3ï¼šè¯­ä¹‰åˆ†æ
    print("\n[3/6] è¯­ä¹‰åˆ†æï¼ˆé¢„è®¡20-30åˆ†é’Ÿï¼‰...")
    print("    æ­£åœ¨è¿›è¡Œä¸»é¢˜ã€æƒ…æ„Ÿã€ä»·å€¼æ ‡æ³¨...")
    analyzed = analyze_all_dimensions(atoms)
    print("    æ­£åœ¨ç”Ÿæˆè¯­ä¹‰å‘é‡...")
    analyzed = generate_embeddings(analyzed)
    save_json({'atoms': analyzed}, f"{output_dir}/analyzed_atoms.json")
    print(f"âœ“ åˆ†æå®Œæˆ")

    # é˜¶æ®µ4ï¼šç»“æ„é‡ç»„
    print("\n[4/6] æ„å»ºçŸ¥è¯†ç»“æ„ï¼ˆé¢„è®¡10-15åˆ†é’Ÿï¼‰...")
    print("    æ­£åœ¨æ„å»ºå®ä½“å¡ç‰‡...")
    entities = build_entities(analyzed)
    print("    æ­£åœ¨æ„å»ºä¸»é¢˜ç½‘ç»œ...")
    topics = build_topic_network(analyzed)
    print("    æ­£åœ¨è¯†åˆ«å™äº‹ç‰‡æ®µ...")
    narratives = identify_narratives(analyzed)

    save_json({'entities': entities}, f"{output_dir}/entities.json")
    save_json({'topics': topics}, f"{output_dir}/topics.json")
    save_json({'narratives': narratives}, f"{output_dir}/narratives.json")
    print(f"âœ“ è¯†åˆ«{len(entities)}ä¸ªå®ä½“ï¼Œ{len(topics)}ä¸ªä¸»é¢˜ï¼Œ{len(narratives)}ä¸ªå™äº‹ç‰‡æ®µ")

    # é˜¶æ®µ5ï¼šå»ºç«‹ç´¢å¼•
    print("\n[5/6] å»ºç«‹ç´¢å¼•...")
    print("    æ­£åœ¨æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•...")
    build_semantic_index(analyzed, entities, topics)
    print("    æ­£åœ¨æ„å»ºç»“æ„åŒ–ç´¢å¼•...")
    indexes = build_structured_index(analyzed)
    print("    æ­£åœ¨æ„å»ºçŸ¥è¯†å›¾è°±...")
    graph = build_knowledge_graph(entities)

    save_json(indexes, f"{output_dir}/indexes/structured.json")
    save_json(graph, f"{output_dir}/indexes/graph.json")
    print(f"âœ“ ç´¢å¼•æ„å»ºå®Œæˆ")

    # é˜¶æ®µ6ï¼šç”Ÿæˆå…ƒä¿¡æ¯
    print("\n[6/6] ç”ŸæˆçŸ¥è¯†åº“å…ƒä¿¡æ¯...")
    meta = {
        "video_id": os.path.basename(srt_path).replace('.srt', ''),
        "processed_at": datetime.now().isoformat(),
        "stats": {
            "total_atoms": len(analyzed),
            "total_entities": len(entities),
            "total_topics": len(topics),
            "total_narratives": len(narratives)
        }
    }
    save_json(meta, f"{output_dir}/meta.json")
    print(f"âœ“ å®Œæˆï¼")

    print("\n" + "=" * 60)
    print(f"çŸ¥è¯†åº“å·²ä¿å­˜åˆ°ï¼š{output_dir}")
    print("\nç°åœ¨å¯ä»¥ä½¿ç”¨çŸ¥è¯†åº“è¿›è¡Œï¼š")
    print("  â€¢ è¯­ä¹‰æœç´¢")
    print("  â€¢ ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ")
    print("  â€¢ ä¸»é¢˜åˆé›†åˆ¶ä½œ")
    print("  â€¢ åŸåˆ›å™äº‹åˆ›ä½œ")
    print("=" * 60)

def save_json(data, file_path):
    """ä¿å­˜JSONæ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¤„ç†è§†é¢‘å­—å¹•
    process_video_subtitle(
        srt_path="D:/YouTube_Downloads/é‡‘ä¸‰è§’å¤§ä½¬4ï¼šç¼…åŒ—åŒé›„æ—¶ä»£1962-1998.srt",
        output_dir="data/output/jinSanJiao_04"
    )

    print("\n" + "=" * 60)
    print("ç°åœ¨å¯ä»¥ä½¿ç”¨çŸ¥è¯†åº“ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ...")
    print("=" * 60)

    # åŠ è½½çŸ¥è¯†åº“
    kb = VideoKnowledgeBase("data/output/jinSanJiao_04")

    # äº¤äº’å¼ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ
    plan = interactive_editing(
        kb,
        "æˆ‘æƒ³åšä¸€ä¸ª15åˆ†é’Ÿçš„è§†é¢‘ï¼Œè®²å¤æ²™æ˜¯æ€ä¹ˆä»å°å†›é˜€å´›èµ·æˆä¸ºé‡‘ä¸‰è§’ä¹‹ç‹çš„"
    )

    # ä¿å­˜æ–¹æ¡ˆ
    plan_output_dir = "output/edit_plans"
    os.makedirs(plan_output_dir, exist_ok=True)
    save_json(plan, f"{plan_output_dir}/plan_001.json")

    print(f"\nâœ“ å‰ªè¾‘æ–¹æ¡ˆå·²ä¿å­˜åˆ°ï¼š{plan_output_dir}/plan_001.json")
```

---

## â±ï¸ æ—¶é—´å’Œæˆæœ¬ä¼°ç®—

### å¤„ç†ä¸€ä¸ª2å°æ—¶è§†é¢‘

| é˜¶æ®µ | æ—¶é—´ | APIæˆæœ¬ |
|------|------|---------|
| 1. è§£æå­—å¹• | 1åˆ†é’Ÿ | $0 |
| 2. åŸå­åŒ– | 30-40åˆ†é’Ÿ | $5-8 (Claude) |
| 3. è¯­ä¹‰åˆ†æ | 20-30åˆ†é’Ÿ | $8-12 (Claude) |
| 4. ç»“æ„é‡ç»„ | 10-15åˆ†é’Ÿ | $3-5 (Claude) |
| 5. å»ºç«‹ç´¢å¼• | 5åˆ†é’Ÿ | $1 (OpenAI embedding) |
| **æ€»è®¡** | **1-1.5å°æ—¶** | **$17-26** |

### åç»­ä½¿ç”¨æˆæœ¬

- åŠ è½½çŸ¥è¯†åº“ï¼š<1ç§’ï¼Œ$0
- è¯­ä¹‰æœç´¢ï¼š<1ç§’ï¼Œ$0
- ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆï¼š10-30ç§’ï¼Œ~$0.5

---

## ğŸ“ æ€»ç»“

### æ¯ä¸ªé˜¶æ®µçš„æ ¸å¿ƒ

1. **è§£æ (Parse)**ï¼šæŠŠå­—å¹•å˜æˆç»“æ„åŒ–æ•°æ®
2. **åŸå­åŒ– (Atomize)**ï¼šAIæ™ºèƒ½åˆå¹¶ï¼Œå½¢æˆè¯­ä¹‰å•å…ƒ
3. **åˆ†æ (Analyze)**ï¼šå¤šç»´åº¦æ ‡æ³¨ + ç”Ÿæˆembedding
4. **é‡ç»„ (Structure)**ï¼šæ„å»ºå®ä½“ã€ä¸»é¢˜ã€å™äº‹
5. **ç´¢å¼• (Index)**ï¼šå»ºç«‹å¤šç»´æ£€ç´¢ç³»ç»Ÿ
6. **ç”Ÿæˆ (Generate)**ï¼šè‡ªåŠ¨ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

- **AIç†è§£**ï¼šClaude API (Sonnet 3.5)
- **è¯­ä¹‰æœç´¢**ï¼šOpenAI text-embedding-3-large + Chromadb
- **å­˜å‚¨**ï¼šJSONæ–‡ä»¶ + å‘é‡æ•°æ®åº“

### å…³é”®ä¼˜åŠ¿

âœ… **æ·±åº¦ç†è§£**ï¼šLevel 4å±‚æ¬¡çš„å†…å®¹ç†è§£
âœ… **çµæ´»æ£€ç´¢**ï¼šå¤šç»´åº¦ç´¢å¼•ï¼Œä»»ä½•è§’åº¦éƒ½èƒ½å¿«é€Ÿæ‰¾åˆ°ç´ æ
âœ… **æ™ºèƒ½ç”Ÿæˆ**ï¼šåŸºäºç†è§£è‡ªåŠ¨ç”Ÿæˆå‰ªè¾‘æ–¹æ¡ˆ
âœ… **å¯æ‰©å±•**ï¼šå¤šä¸ªè§†é¢‘å¯ä»¥åˆå¹¶çŸ¥è¯†åº“
âœ… **å¯é‡ç”¨**ï¼šä¸€æ¬¡å¤„ç†ï¼Œå¤šæ¬¡ä½¿ç”¨ï¼Œæˆæœ¬æä½

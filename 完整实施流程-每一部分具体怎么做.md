# 视频内容深度理解系统 - 完整实施流程

每一部分的具体做法拆解

---

## 📋 完整实施流程

### 阶段0：准备工作

#### 0.1 确定技术栈

```bash
# 推荐的技术组合

1. 字幕解析：Python srt库 或 JavaScript
2. AI分析：Claude API (Sonnet 3.5/3.7)
3. Embedding：OpenAI text-embedding-3-large
4. 向量数据库：Chromadb (简单) 或 Qdrant (强大)
5. 存储：JSON文件 + 向量DB
```

#### 0.2 项目结构

```
subtitle-analyzer/
├── src/
│   ├── 1-parse/          # 阶段1：解析字幕
│   ├── 2-atomize/        # 阶段2：原子化
│   ├── 3-analyze/        # 阶段3：语义分析
│   ├── 4-structure/      # 阶段4：结构重组
│   ├── 5-index/          # 阶段5：建立索引
│   └── 6-generate/       # 阶段6：生成方案
├── data/
│   ├── raw/              # 原始字幕文件
│   ├── processed/        # 处理后的数据
│   └── output/           # 最终知识库
├── prompts/              # AI提示词模板
└── utils/                # 工具函数
```

---

## 🔧 阶段1：解析字幕 (Parse)

### 目标
把SRT字幕文件解析成结构化数据

### 输入
```
金三角大佬4：缅北双雄时代1962-1998.srt
```

### 处理步骤

#### 1.1 读取SRT文件

```python
import srt
from datetime import timedelta

def parse_srt(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    subtitles = list(srt.parse(content))

    parsed = []
    for sub in subtitles:
        parsed.append({
            "id": sub.index,
            "start": str(sub.start),
            "end": str(sub.end),
            "text": sub.content.strip(),
            "duration": (sub.end - sub.start).total_seconds()
        })

    return parsed

# 结果示例
[
  {"id": 1, "start": "0:00:07", "end": "0:00:07.900", "text": "what", "duration": 0.9},
  {"id": 2, "start": "0:00:08.933", "end": "0:00:10.400", "text": "开始了没有啊", "duration": 1.467},
  ...
]
```

#### 1.2 清洗和预处理

```python
def clean_subtitles(parsed):
    cleaned = []

    for item in parsed:
        text = item['text']

        # 去除无意义内容
        if text in ['呃', 'uh', 'um', '...']:
            continue

        # 去除过短的片段（<0.5秒）
        if item['duration'] < 0.5:
            continue

        # 标准化文本
        text = text.replace('\n', ' ').strip()

        cleaned.append({
            **item,
            'text': text
        })

    return cleaned
```

### 输出
```json
{
  "video_id": "jinSanJiao_04",
  "total_subtitles": 3580,
  "total_duration": "02:30:45",
  "subtitles": [
    {"id": 1, "start": "0:00:08.933", "end": "0:00:10.400", "text": "开始了没有啊"},
    ...
  ]
}
```

**保存为：`data/processed/parsed_subtitles.json`**

---

## 🧩 阶段2：原子化 (Atomize)

### 目标
把碎片化的字幕合并成"语义完整的信息原子"

### 核心挑战
如何判断哪些句子应该合并？

### 方法：AI驱动的智能合并

#### 2.1 设计AI提示词

创建文件：`prompts/atomize.txt`

```
你是一个视频内容分析专家。我会给你一段直播的字幕片段，你的任务是把它们合并成"语义完整的信息单元"。

【规则】
1. 一个信息单元应该表达一个完整的意思（一个观点、一个故事片段、一次互动）
2. 识别边界：
   - 主题转换 → 新单元
   - 长时间停顿（>5秒）→ 新单元
   - 从叙事转到互动 → 新单元
3. 保持时间戳的连续性

【输入格式】
[时间段] 文本内容

【输出格式】
返回JSON数组：
[
  {
    "atom_id": "A001",
    "start": "起始时间",
    "end": "结束时间",
    "merged_text": "合并后的完整文本",
    "type": "叙述历史/回应弹幕/发表观点/闲聊",
    "completeness": "完整/需要上下文"
  }
]

【示例输入】
[00:08:20] 1962年
[00:08:25] 国民党残军撤到金三角
[00:08:30] 这是整个金三角问题的起源
[00:08:38] hello 海绵宝宝
[00:08:40] 然后呢坤沙就是在这个背景下崛起的

【示例输出】
[
  {
    "atom_id": "A001",
    "start": "00:08:20",
    "end": "00:08:30",
    "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源",
    "type": "叙述历史",
    "completeness": "完整"
  },
  {
    "atom_id": "A002",
    "start": "00:08:38",
    "end": "00:08:40",
    "merged_text": "hello 海绵宝宝",
    "type": "回应弹幕",
    "completeness": "完整"
  },
  {
    "atom_id": "A003",
    "start": "00:08:40",
    "end": "00:08:45",
    "merged_text": "然后呢坤沙就是在这个背景下崛起的",
    "type": "叙述历史",
    "completeness": "需要上下文"
  }
]
```

#### 2.2 批量处理代码

```python
import anthropic
import json

def atomize_subtitles(subtitles, batch_size=50):
    """
    把字幕分批送给AI进行原子化
    """
    client = anthropic.Anthropic(api_key="your_api_key")
    atoms = []

    # 读取提示词
    with open('prompts/atomize.txt', 'r', encoding='utf-8') as f:
        ATOMIZE_PROMPT = f.read()

    # 每次处理50条字幕（约2-3分钟内容）
    for i in range(0, len(subtitles), batch_size):
        batch = subtitles[i:i+batch_size]

        # 构建输入
        input_text = "\n".join([
            f"[{sub['start']}] {sub['text']}"
            for sub in batch
        ])

        # 调用Claude
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": f"{ATOMIZE_PROMPT}\n\n【输入】\n{input_text}"
            }]
        )

        # 解析JSON
        batch_atoms = json.loads(response.content[0].text)
        atoms.extend(batch_atoms)

        print(f"处理进度: {i}/{len(subtitles)}")

    return atoms
```

#### 2.3 后处理和验证

```python
from datetime import datetime

def validate_atoms(atoms):
    """
    检查原子的质量
    """
    issues = []

    for i, atom in enumerate(atoms):
        # 检查时间连续性
        if i > 0:
            prev_end = parse_time(atoms[i-1]['end'])
            curr_start = parse_time(atom['start'])
            gap = (curr_start - prev_end).total_seconds()

            if gap > 30:  # 超过30秒的空隙
                issues.append(f"Atom {atom['atom_id']}: 大间隔 {gap}秒")

        # 检查文本长度
        if len(atom['merged_text']) < 10:
            issues.append(f"Atom {atom['atom_id']}: 文本过短")

        if len(atom['merged_text']) > 500:
            issues.append(f"Atom {atom['atom_id']}: 文本过长，可能需要拆分")

    return issues

def parse_time(time_str):
    """解析时间字符串"""
    # 实现时间解析逻辑
    pass
```

### 输出
```json
{
  "total_atoms": 856,
  "atoms": [
    {
      "atom_id": "A001",
      "start": "00:00:08",
      "end": "00:00:25",
      "merged_text": "开始了没有啊，我看看开始了没有...",
      "type": "开场确认",
      "completeness": "完整",
      "source_subtitle_ids": [2, 3, 4, 5]
    },
    {
      "atom_id": "A012",
      "start": "00:08:20",
      "end": "00:09:15",
      "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源...",
      "type": "叙述历史",
      "completeness": "完整",
      "source_subtitle_ids": [85, 86, 87, 88, 89]
    }
  ]
}
```

**保存为：`data/processed/atoms.json`**

---

## 🏷️ 阶段3：语义分析 (Analyze)

### 目标
给每个原子打上多维度的标签

### 3.1 主题标注

#### 提示词设计

创建文件：`prompts/tag_topics.txt`

```
分析这段文本属于哪些主题。

【文本】
{atom_text}

【可选主题】
一级主题：
- 历史叙事：讲述过去的事件
- 人物分析：分析人物性格、动机、行为
- 观众互动：读来信、回应弹幕、答疑
- 当代对比：将历史与当代进行对比
- 个人观点：博主的评价和看法
- 闲聊过场：开场、技术性对话、无关内容

二级主题（如果是历史叙事）：
- 1960s背景/坤沙崛起/罗星汉对抗/权力斗争/军事冲突/经济博弈/最终结局

【要求】
1. 主题可多选
2. 给每个主题打分（1-10），表示相关度
3. 提取关键实体：人物、地点、时间、事件

【输出JSON】
{
  "primary_topic": "历史叙事",
  "secondary_topics": ["坤沙崛起", "军事冲突"],
  "topic_scores": {
    "历史叙事": 10,
    "坤沙崛起": 9,
    "军事冲突": 8
  },
  "entities": {
    "persons": ["坤沙"],
    "locations": ["金三角", "缅甸"],
    "time_points": ["1962年"],
    "events": ["国民党撤退"],
    "concepts": ["军事割据", "权力真空"]
  }
}
```

#### 批量处理

```python
def analyze_topics(atoms):
    """
    分析所有原子的主题
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
        TOPIC_PROMPT = f.read()

    analyzed = []

    for atom in atoms:
        prompt = TOPIC_PROMPT.format(atom_text=atom['merged_text'])

        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            messages=[{"role": "user", "content": prompt}]
        )

        analysis = json.loads(response.content[0].text)

        analyzed.append({
            **atom,
            "topics": analysis
        })

        print(f"分析进度: {len(analyzed)}/{len(atoms)}")

    return analyzed
```

### 3.2 情感与能量标注

#### 提示词

创建文件：`prompts/tag_emotion.txt`

```
分析这段文本的情感和能量。

【文本】
{atom_text}

【分析维度】
1. 情感类型：
   - 客观叙述：平铺直叙，无明显情感
   - 激动/愤怒：语气强烈，用词激烈
   - 幽默/调侃：开玩笑、讽刺
   - 同情/感慨：表达同情或感慨
   - 批判/质疑：批评、质疑

2. 能量值（1-10）：
   - 1-3：平静、低沉
   - 4-6：正常叙述
   - 7-8：有激情、投入
   - 9-10：高度激动、高潮

3. 趋势：递增/递减/平稳

【输出JSON】
{
  "emotion_type": "激动",
  "energy_level": 9,
  "energy_trend": "递增",
  "indicators": ["用词激烈", "语气强调", "重复强调"]
}
```

### 3.3 内容价值标注

#### 提示词

创建文件：`prompts/tag_value.txt`

```
评估这段内容的价值和可剪辑性。

【文本】
{atom_text}

【评估标准】

1. 信息密度（1-10）：
   - 1-3：重复、闲聊、技术性对话
   - 4-6：一般性叙述
   - 7-8：有价值的信息
   - 9-10：核心信息、关键观点、金句

2. 可剪辑性：
   - 必剪：核心内容，不能删
   - 可剪：有价值但非必需
   - 可删：冗余、重复
   - 必删：无意义填充

3. 独立性：
   - 独立：可单独成片，不需上下文
   - 需铺垫：需要前置背景
   - 需补充：需要后续解释
   - 依赖上下文：必须在特定上下文中

4. 特殊价值：
   - 金句：值得单独摘录
   - 高潮点：情节转折或冲突顶点
   - 争议点：可能引发讨论
   - 无

【输出JSON】
{
  "information_density": 9,
  "editability": "必剪",
  "independence": "需铺垫",
  "special_value": "高潮点",
  "reason": "描述了坤沙的关键军事行动，是权力转折点"
}
```

#### 整合分析代码

```python
def analyze_all_dimensions(atoms):
    """
    对每个原子进行完整的多维度分析
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    # 读取所有提示词
    with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
        topic_prompt = f.read()
    with open('prompts/tag_emotion.txt', 'r', encoding='utf-8') as f:
        emotion_prompt = f.read()
    with open('prompts/tag_value.txt', 'r', encoding='utf-8') as f:
        value_prompt = f.read()

    analyzed = []

    for i, atom in enumerate(atoms):
        text = atom['merged_text']

        # 主题分析
        topics = call_claude(topic_prompt.format(atom_text=text))

        # 情感分析
        emotion = call_claude(emotion_prompt.format(atom_text=text))

        # 价值分析
        value = call_claude(value_prompt.format(atom_text=text))

        analyzed.append({
            **atom,
            "topics": json.loads(topics),
            "emotion": json.loads(emotion),
            "value": json.loads(value)
        })

        print(f"分析进度: {i+1}/{len(atoms)}")

    return analyzed

def call_claude(prompt):
    """调用Claude API的辅助函数"""
    client = anthropic.Anthropic(api_key="your_api_key")
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=2000,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text
```

### 3.4 生成Embedding向量

```python
from openai import OpenAI

def generate_embeddings(atoms):
    """
    为每个原子生成语义向量
    """
    client = OpenAI(api_key="your_openai_key")

    texts = [atom['merged_text'] for atom in atoms]

    # 批量生成（OpenAI支持批量，每次最多2048条）
    batch_size = 2048

    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]

        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=batch_texts
        )

        for j, embedding_data in enumerate(response.data):
            atoms[i+j]['embedding'] = embedding_data.embedding

        print(f"Embedding进度: {min(i+batch_size, len(texts))}/{len(texts)}")

    return atoms
```

### 输出
```json
{
  "atoms": [
    {
      "atom_id": "A012",
      "merged_text": "1962年国民党残军...",
      "topics": {
        "primary_topic": "历史叙事",
        "secondary_topics": ["1960s背景"],
        "topic_scores": {"历史叙事": 10, "1960s背景": 9},
        "entities": {
          "persons": ["国民党残军"],
          "locations": ["金三角"],
          "time_points": ["1962年"],
          "events": ["国民党撤退"],
          "concepts": ["军事割据"]
        }
      },
      "emotion": {
        "emotion_type": "客观叙述",
        "energy_level": 6,
        "energy_trend": "平稳"
      },
      "value": {
        "information_density": 9,
        "editability": "必剪",
        "independence": "需铺垫",
        "special_value": "关键背景"
      },
      "embedding": [0.023, 0.451, -0.234, ...]
    }
  ]
}
```

**保存为：`data/processed/analyzed_atoms.json`**

---

## 🏗️ 阶段4：结构重组 (Structure)

### 目标
把分散的原子重新组织成结构化知识

### 4.1 构建实体卡片

#### 提示词

创建文件：`prompts/build_entity.txt`

```
基于以下原子，构建"{entity_name}"的实体卡片。

【相关原子】
{related_atoms_text}

【要求】
1. 生成人物/地点/事件的完整画像
2. 按不同facet（面向）组织信息
3. 提取时间线
4. 识别关系网络
5. 生成AI总结

【输出JSON】
{
  "entity_id": "person_kunsha",
  "type": "person",
  "name": "坤沙",
  "profile": {
    "basic_info": "1934-2007，金三角军阀...",
    "key_characteristics": ["权谋高手", "善用地缘政治"],
    "historical_significance": "..."
  },
  "facets": {
    "性格特质": {
      "summary": "...",
      "supporting_atoms": ["A023", "A045"]
    },
    "权力手段": {
      "summary": "...",
      "supporting_atoms": ["A034", "A056"]
    }
  },
  "timeline": [
    {"year": "1962", "event": "进入金三角", "atoms": ["A012"]},
    {"year": "1985", "event": "关键军事行动", "atoms": ["A045"]}
  ],
  "relationships": [
    {
      "target": "罗星汉",
      "relation": "对手",
      "description": "长期权力竞争",
      "atoms": ["A023", "A067"]
    }
  ]
}
```

#### 实现代码

```python
def build_entities(analyzed_atoms):
    """
    自动识别和构建所有实体
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/build_entity.txt', 'r', encoding='utf-8') as f:
        entity_prompt = f.read()

    # 1. 收集所有提到的实体
    entity_mentions = {}

    for atom in analyzed_atoms:
        persons = atom['topics']['entities']['persons']
        for person in persons:
            if person not in entity_mentions:
                entity_mentions[person] = []
            entity_mentions[person].append(atom['atom_id'])

    # 2. 为每个实体构建卡片
    entities = []

    for entity_name, atom_ids in entity_mentions.items():
        if len(atom_ids) < 3:  # 至少出现3次才建卡片
            continue

        # 获取相关原子的文本
        related_atoms = [a for a in analyzed_atoms if a['atom_id'] in atom_ids]
        atoms_text = "\n\n".join([
            f"[{a['atom_id']}] {a['merged_text']}"
            for a in related_atoms
        ])

        # 调用AI构建实体卡片
        prompt = entity_prompt.format(
            entity_name=entity_name,
            related_atoms_text=atoms_text
        )

        response = call_claude(prompt)
        entity_card = json.loads(response)

        # 生成实体的embedding
        entity_summary = entity_card['profile']['basic_info']
        entity_card['embedding'] = generate_single_embedding(entity_summary)

        entities.append(entity_card)

        print(f"构建实体: {entity_name}")

    return entities

def generate_single_embedding(text):
    """为单个文本生成embedding"""
    client = OpenAI(api_key="your_openai_key")
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=[text]
    )
    return response.data[0].embedding
```

### 4.2 构建主题网络

#### 提示词

创建文件：`prompts/build_topic_network.txt`

```
基于所有原子，构建主题网络。

【主题名称】
{topic_name}

【主题统计】
出现次数: {count}

【相关原子示例】
{sample_atoms}

【要求】
1. 识别这个主题的子主题
2. 生成主题定义和描述
3. 设计叙事模板

【输出JSON】
{
  "topic_id": "theme_power_struggle",
  "name": "权力斗争",
  "definition": "金三角地区不同势力之间的控制权争夺",
  "sub_topics": [
    {
      "name": "军事对抗",
      "atoms": ["A045", "A067"],
      "description": "通过武装冲突争夺地盘"
    }
  ],
  "narrative_templates": [
    {
      "template_name": "双雄争霸",
      "structure": "背景 → 崛起 → 对抗 → 结局",
      "estimated_duration": "15:00"
    }
  ]
}
```

#### 实现代码

```python
def build_topic_network(analyzed_atoms):
    """
    构建主题网络
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/build_topic_network.txt', 'r', encoding='utf-8') as f:
        topic_prompt = f.read()

    # 1. 统计所有主题出现频率
    topic_stats = {}
    topic_atoms = {}

    for atom in analyzed_atoms:
        topics = atom['topics']['secondary_topics']
        for topic in topics:
            if topic not in topic_stats:
                topic_stats[topic] = 0
                topic_atoms[topic] = []
            topic_stats[topic] += 1
            topic_atoms[topic].append(atom['atom_id'])

    # 2. 为主要主题（出现10次以上）建立详细网络
    topic_network = []

    for topic_name, count in topic_stats.items():
        if count < 10:
            continue

        related_atoms = [a for a in analyzed_atoms
                        if a['atom_id'] in topic_atoms[topic_name]]

        # 取前10个原子作为示例
        sample_text = "\n\n".join([
            f"[{a['atom_id']}] {a['merged_text']}"
            for a in related_atoms[:10]
        ])

        # 调用AI构建主题网络
        prompt = topic_prompt.format(
            topic_name=topic_name,
            count=count,
            sample_atoms=sample_text
        )

        response = call_claude(prompt)
        topic_card = json.loads(response)

        # 补充完整的atom列表
        topic_card['all_atoms'] = topic_atoms[topic_name]

        # 生成embedding
        topic_card['embedding'] = generate_single_embedding(
            topic_card['definition']
        )

        topic_network.append(topic_card)

        print(f"构建主题: {topic_name} ({count}次)")

    return topic_network
```

### 4.3 识别叙事片段

#### 提示词

创建文件：`prompts/identify_narratives.txt`

```
识别视频中完整的叙事片段。

【原子序列】
{sequential_atoms}

【要求】
1. 找出"完整故事"：有开头、发展、结局
2. 识别叙事结构
3. 评估完整性和独立性

【输出JSON】
{
  "segment_id": "narrative_001",
  "title": "坤沙的崛起（1962-1985）",
  "atoms": ["A012", "A023", "A045", "A056"],
  "structure": {
    "setup": {"atoms": ["A012"], "duration": "02:30"},
    "conflict": {"atoms": ["A023", "A034"], "duration": "08:15"},
    "climax": {"atoms": ["A045"], "duration": "03:10"}
  },
  "completeness": "完整",
  "independence": "需要前置背景"
}
```

#### 实现代码

```python
def identify_narratives(analyzed_atoms):
    """
    识别叙事片段
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    with open('prompts/identify_narratives.txt', 'r', encoding='utf-8') as f:
        narrative_prompt = f.read()

    # 按时间顺序准备原子序列
    atoms_text = "\n\n".join([
        f"[{a['atom_id']}] {a['start']}-{a['end']}: {a['merged_text'][:100]}..."
        for a in analyzed_atoms
    ])

    prompt = narrative_prompt.format(sequential_atoms=atoms_text)

    response = call_claude(prompt)
    narratives = json.loads(response)

    return narratives
```

### 输出

生成三个文件：

**1. entities.json**
```json
{
  "entities": [
    {
      "entity_id": "person_kunsha",
      "type": "person",
      "name": "坤沙",
      "profile": {...},
      "facets": {...},
      "timeline": [...],
      "relationships": [...],
      "embedding": [...]
    }
  ]
}
```

**2. topics.json**
```json
{
  "topics": [
    {
      "topic_id": "theme_power_struggle",
      "name": "权力斗争",
      "definition": "...",
      "sub_topics": [...],
      "narrative_templates": [...],
      "all_atoms": ["A023", "A045", ...],
      "embedding": [...]
    }
  ]
}
```

**3. narratives.json**
```json
{
  "narratives": [
    {
      "segment_id": "narrative_001",
      "title": "坤沙的崛起",
      "atoms": ["A012", "A023", "A045"],
      "structure": {...}
    }
  ]
}
```

---

## 🗂️ 阶段5：建立索引 (Index)

### 目标
建立多维度检索系统

### 5.1 语义向量索引

```python
import chromadb

def build_semantic_index(analyzed_atoms, entities, topics):
    """
    把所有embedding存入向量数据库
    """
    client = chromadb.PersistentClient(path="data/output/vector_db")

    # 创建collection
    collection = client.create_collection(
        name="video_knowledge",
        metadata={"description": "金三角大佬4的知识库"}
    )

    # 添加原子
    for atom in analyzed_atoms:
        collection.add(
            ids=[atom['atom_id']],
            embeddings=[atom['embedding']],
            metadatas=[{
                "type": "atom",
                "start_time": atom['start'],
                "end_time": atom['end'],
                "primary_topic": atom['topics']['primary_topic'],
                "value_score": atom['value']['information_density']
            }],
            documents=[atom['merged_text']]
        )

    # 添加实体
    for entity in entities:
        collection.add(
            ids=[entity['entity_id']],
            embeddings=[entity['embedding']],
            metadatas=[{
                "type": "entity",
                "entity_type": entity['type'],
                "name": entity['name']
            }],
            documents=[entity['profile']['basic_info']]
        )

    # 添加主题
    for topic in topics:
        collection.add(
            ids=[topic['topic_id']],
            embeddings=[topic['embedding']],
            metadatas=[{
                "type": "topic",
                "name": topic['name']
            }],
            documents=[topic['definition']]
        )

    print(f"✓ 向量索引构建完成，共{collection.count()}条记录")
```

### 5.2 结构化索引

```python
def build_structured_index(analyzed_atoms):
    """
    构建多维度的结构化索引
    """
    indexes = {
        "by_time": {},
        "by_person": {},
        "by_topic": {},
        "by_value": {},
        "by_emotion": {}
    }

    # 按时间索引（按10分钟分组）
    for atom in analyzed_atoms:
        time_bucket = get_time_bucket(atom['start'], bucket_size=600)
        if time_bucket not in indexes['by_time']:
            indexes['by_time'][time_bucket] = []
        indexes['by_time'][time_bucket].append(atom['atom_id'])

    # 按人物索引
    for atom in analyzed_atoms:
        persons = atom['topics']['entities']['persons']
        for person in persons:
            if person not in indexes['by_person']:
                indexes['by_person'][person] = []
            indexes['by_person'][person].append(atom['atom_id'])

    # 按主题索引
    for atom in analyzed_atoms:
        topics = atom['topics']['secondary_topics']
        for topic in topics:
            if topic not in indexes['by_topic']:
                indexes['by_topic'][topic] = []
            indexes['by_topic'][topic].append(atom['atom_id'])

    # 按价值索引
    for atom in analyzed_atoms:
        value = atom['value']['information_density']
        value_tier = f"{(value-1)//2*2+1}-{(value-1)//2*2+2}分"
        if value_tier not in indexes['by_value']:
            indexes['by_value'][value_tier] = []
        indexes['by_value'][value_tier].append(atom['atom_id'])

    # 按情感索引
    for atom in analyzed_atoms:
        emotion = atom['emotion']['emotion_type']
        if emotion not in indexes['by_emotion']:
            indexes['by_emotion'][emotion] = []
        indexes['by_emotion'][emotion].append(atom['atom_id'])

    return indexes

def get_time_bucket(time_str, bucket_size=600):
    """将时间转换为时间桶（秒）"""
    # 解析时间字符串，返回所属的桶
    # 例如 00:05:30 → "00:00-00:10"
    pass
```

### 5.3 知识图谱

```python
import networkx as nx

def build_knowledge_graph(entities):
    """
    构建知识图谱
    """
    G = nx.DiGraph()

    # 添加实体节点
    for entity in entities:
        G.add_node(
            entity['entity_id'],
            type=entity['type'],
            name=entity['name'],
            data=entity
        )

    # 添加关系边
    for entity in entities:
        if 'relationships' in entity:
            for rel in entity['relationships']:
                # 找到目标实体的ID
                target_id = None
                for e in entities:
                    if e['name'] == rel['target']:
                        target_id = e['entity_id']
                        break

                if target_id:
                    G.add_edge(
                        entity['entity_id'],
                        target_id,
                        relation=rel['relation'],
                        atoms=rel['atoms'],
                        description=rel.get('description', '')
                    )

    # 保存为JSON
    graph_data = nx.node_link_data(G)

    return graph_data
```

### 输出

**1. vector_db/** (向量数据库目录)
```
vector_db/
├── chroma.sqlite3
└── [其他chromadb文件]
```

**2. indexes/structured.json**
```json
{
  "by_time": {
    "00:00-00:10": ["A001", "A002", "A003"],
    "00:10-00:20": ["A015", "A016", "A020"]
  },
  "by_person": {
    "坤沙": ["A012", "A023", "A045"],
    "罗星汉": ["A023", "A034"]
  },
  "by_topic": {
    "军事冲突": ["A045", "A067"],
    "权力斗争": ["A023", "A045", "A067"]
  },
  "by_value": {
    "9-10分": ["A045", "A067"],
    "7-8分": ["A023", "A034"]
  },
  "by_emotion": {
    "激动": ["A045", "A089"],
    "客观": ["A012", "A023"]
  }
}
```

**3. indexes/graph.json**
```json
{
  "directed": true,
  "nodes": [
    {
      "id": "person_kunsha",
      "type": "person",
      "name": "坤沙"
    },
    {
      "id": "person_luoxinghan",
      "type": "person",
      "name": "罗星汉"
    }
  ],
  "links": [
    {
      "source": "person_kunsha",
      "target": "person_luoxinghan",
      "relation": "对手",
      "atoms": ["A023", "A067"]
    }
  ]
}
```

---

## 🎬 阶段6：生成剪辑方案 (Generate)

### 目标
基于知识库，自动生成剪辑方案

### 6.1 查询接口

```python
import chromadb
import json

class VideoKnowledgeBase:
    def __init__(self, data_path):
        # 加载JSON数据
        with open(f"{data_path}/analyzed_atoms.json", 'r', encoding='utf-8') as f:
            self.atoms = json.load(f)
        with open(f"{data_path}/entities.json", 'r', encoding='utf-8') as f:
            self.entities = json.load(f)
        with open(f"{data_path}/topics.json", 'r', encoding='utf-8') as f:
            self.topics = json.load(f)
        with open(f"{data_path}/narratives.json", 'r', encoding='utf-8') as f:
            self.narratives = json.load(f)
        with open(f"{data_path}/indexes/structured.json", 'r', encoding='utf-8') as f:
            self.indexes = json.load(f)

        # 连接向量数据库
        self.vector_client = chromadb.PersistentClient(f"{data_path}/vector_db")
        self.collection = self.vector_client.get_collection("video_knowledge")

    def semantic_search(self, query, top_k=10, filter_type=None):
        """语义搜索"""
        # 生成查询的embedding
        query_embedding = generate_single_embedding(query)

        # 搜索
        where_filter = {"type": filter_type} if filter_type else None
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=where_filter
        )

        return results

    def get_entity(self, entity_name):
        """获取实体卡片"""
        for entity in self.entities['entities']:
            if entity['name'] == entity_name:
                return entity
        return None

    def get_atoms_by_topic(self, topic_name):
        """按主题获取原子"""
        atom_ids = self.indexes['by_topic'].get(topic_name, [])
        return [a for a in self.atoms['atoms'] if a['atom_id'] in atom_ids]

    def get_atoms_by_person(self, person_name):
        """按人物获取原子"""
        atom_ids = self.indexes['by_person'].get(person_name, [])
        return [a for a in self.atoms['atoms'] if a['atom_id'] in atom_ids]

    def get_high_value_atoms(self, min_score=8):
        """获取高价值原子"""
        return [a for a in self.atoms['atoms']
                if a['value']['information_density'] >= min_score]

    def get_atom_by_id(self, atom_id):
        """根据ID获取原子"""
        for atom in self.atoms['atoms']:
            if atom['atom_id'] == atom_id:
                return atom
        return None
```

### 6.2 生成剪辑方案

```python
def generate_edit_plan(kb, user_request):
    """
    根据用户需求生成剪辑方案
    """
    client = anthropic.Anthropic(api_key="your_api_key")

    # 提示词
    prompt = f"""
基于视频知识库，为以下需求生成剪辑方案：

【用户需求】
{user_request}

【可用资源】
- 总计{len(kb.atoms['atoms'])}个信息原子
- {len(kb.entities['entities'])}个实体
- {len(kb.topics['topics'])}个主题

【任务】
1. 理解用户需求
2. 从知识库中选择合适的素材
3. 设计叙事结构
4. 生成完整的剪辑方案

【输出JSON】
{{
  "plan_name": "方案名称",
  "target_duration": "目标时长",
  "narrative_structure": "叙事结构描述",
  "required_atoms": ["A001", "A012", ...],
  "timeline": [
    {{"order": 1, "type": "clip", "atom_id": "A012"}},
    {{"order": 2, "type": "transition", "content": "转场文案"}},
    ...
  ],
  "editing_suggestions": ["建议1", "建议2"]
}}
"""

    # Step 1: 语义搜索相关内容
    search_results = kb.semantic_search(user_request, top_k=20, filter_type="atom")

    # Step 2: 准备上下文
    relevant_atoms = []
    for atom_id in search_results['ids'][0]:
        atom = kb.get_atom_by_id(atom_id)
        if atom:
            relevant_atoms.append({
                "id": atom['atom_id'],
                "text": atom['merged_text'][:200],
                "time": f"{atom['start']}-{atom['end']}",
                "value": atom['value']['information_density']
            })

    context = {
        "relevant_atoms": relevant_atoms,
        "available_topics": [t['name'] for t in kb.topics['topics']]
    }

    # Step 3: 调用AI生成方案
    full_prompt = prompt + "\n\n【相关素材】\n" + json.dumps(context, ensure_ascii=False, indent=2)

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=4000,
        messages=[{"role": "user", "content": full_prompt}]
    )

    plan = json.loads(response.content[0].text)

    # Step 4: 丰富方案细节
    plan = enrich_plan(kb, plan)

    return plan

def enrich_plan(kb, plan):
    """
    丰富剪辑方案的细节
    """
    # 获取所有涉及的原子的完整信息
    atom_ids = plan['required_atoms']
    atoms_detail = []
    total_seconds = 0

    for atom_id in atom_ids:
        atom = kb.get_atom_by_id(atom_id)
        if atom:
            atoms_detail.append(atom)
            # 计算时长
            duration = calculate_duration(atom['start'], atom['end'])
            total_seconds += duration

    plan['actual_duration'] = format_duration(total_seconds)
    plan['atoms_detail'] = atoms_detail

    # 生成ffmpeg命令
    plan['ffmpeg_commands'] = []
    for i, atom in enumerate(atoms_detail):
        cmd = f'ffmpeg -i input.mp4 -ss {atom["start"]} -to {atom["end"]} -c copy clip_{i+1:03d}.mp4'
        plan['ffmpeg_commands'].append(cmd)

    return plan

def calculate_duration(start, end):
    """计算时长（秒）"""
    # 实现时间差计算
    pass

def format_duration(seconds):
    """格式化时长"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}"
```

### 6.3 交互式优化

```python
def interactive_editing(kb, initial_request):
    """
    交互式剪辑方案生成
    """
    # 生成初始方案
    print("正在生成剪辑方案...")
    plan = generate_edit_plan(kb, initial_request)

    print(f"\n✓ 生成方案：{plan['plan_name']}")
    print(f"  时长：{plan['actual_duration']}")
    print(f"  包含{len(plan['required_atoms'])}个片段")
    print(f"\n叙事结构：{plan['narrative_structure']}")

    # 用户反馈循环
    while True:
        feedback = input("\n需要调整吗？(例如：'太长了，缩短到10分钟' 或输入 'done' 完成)：")

        if feedback.lower() == "done":
            break

        # 根据反馈调整方案
        print("正在调整方案...")

        adjustment_prompt = f"""
当前方案：
{json.dumps(plan, ensure_ascii=False, indent=2)}

用户反馈：
{feedback}

请根据用户反馈调整方案，保持JSON格式输出。
"""

        client = anthropic.Anthropic(api_key="your_api_key")
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{"role": "user", "content": adjustment_prompt}]
        )

        plan = json.loads(response.content[0].text)
        plan = enrich_plan(kb, plan)

        print(f"\n✓ 已调整：{plan['plan_name']}")
        print(f"  新时长：{plan['actual_duration']}")

    return plan
```

### 输出示例

```json
{
  "plan_id": "plan_001",
  "plan_name": "坤沙：从小军阀到金三角之王",
  "target_duration": "15:00",
  "actual_duration": "15:23",

  "narrative_structure": "三幕剧：起源(3min) → 崛起(8min) → 巅峰(4min)",

  "required_atoms": ["A012", "A023", "A045", "A056", "A078", "A089"],

  "timeline": [
    {
      "order": 1,
      "type": "clip",
      "atom_id": "A012",
      "purpose": "介绍1962年背景"
    },
    {
      "order": 2,
      "type": "transition",
      "content": "在这个权力真空中，一个名叫坤沙的年轻人开始崭露头角...",
      "voiceover": true
    },
    {
      "order": 3,
      "type": "clip",
      "atom_id": "A023",
      "purpose": "坤沙初期发展"
    }
  ],

  "atoms_detail": [
    {
      "atom_id": "A012",
      "merged_text": "1962年国民党残军撤到金三角...",
      "start": "00:08:20",
      "end": "00:09:15"
    }
  ],

  "editing_suggestions": [
    "片段1和2之间可添加金三角地图动画",
    "片段3(A045)是高潮，建议配激昂BGM",
    "考虑使用历史照片作为B-roll"
  ],

  "ffmpeg_commands": [
    "ffmpeg -i input.mp4 -ss 00:08:20 -to 00:09:15 -c copy clip_001.mp4",
    "ffmpeg -i input.mp4 -ss 00:18:05 -to 00:19:30 -c copy clip_002.mp4"
  ]
}
```

**保存为：`output/edit_plans/plan_001.json`**

---

## 📦 完整示例：端到端流程

### 主程序

```python
# main.py

import json
import os
from datetime import datetime

def process_video_subtitle(srt_path, output_dir):
    """
    完整处理流程
    """
    print("=" * 60)
    print("视频字幕深度分析系统")
    print("=" * 60)
    print(f"开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"输入文件: {srt_path}")
    print(f"输出目录: {output_dir}")
    print("=" * 60)

    # 创建输出目录
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/indexes", exist_ok=True)

    # 阶段1：解析
    print("\n[1/6] 解析字幕文件...")
    parsed = parse_srt(srt_path)
    cleaned = clean_subtitles(parsed)
    save_json({'subtitles': cleaned}, f"{output_dir}/parsed_subtitles.json")
    print(f"✓ 解析完成：{len(cleaned)}条字幕")

    # 阶段2：原子化
    print("\n[2/6] 原子化处理（预计30-40分钟）...")
    print("    正在调用AI合并字幕片段...")
    atoms = atomize_subtitles(cleaned)
    save_json({'atoms': atoms}, f"{output_dir}/atoms.json")
    print(f"✓ 原子化完成：{len(atoms)}个信息原子")

    # 阶段3：语义分析
    print("\n[3/6] 语义分析（预计20-30分钟）...")
    print("    正在进行主题、情感、价值标注...")
    analyzed = analyze_all_dimensions(atoms)
    print("    正在生成语义向量...")
    analyzed = generate_embeddings(analyzed)
    save_json({'atoms': analyzed}, f"{output_dir}/analyzed_atoms.json")
    print(f"✓ 分析完成")

    # 阶段4：结构重组
    print("\n[4/6] 构建知识结构（预计10-15分钟）...")
    print("    正在构建实体卡片...")
    entities = build_entities(analyzed)
    print("    正在构建主题网络...")
    topics = build_topic_network(analyzed)
    print("    正在识别叙事片段...")
    narratives = identify_narratives(analyzed)

    save_json({'entities': entities}, f"{output_dir}/entities.json")
    save_json({'topics': topics}, f"{output_dir}/topics.json")
    save_json({'narratives': narratives}, f"{output_dir}/narratives.json")
    print(f"✓ 识别{len(entities)}个实体，{len(topics)}个主题，{len(narratives)}个叙事片段")

    # 阶段5：建立索引
    print("\n[5/6] 建立索引...")
    print("    正在构建语义向量索引...")
    build_semantic_index(analyzed, entities, topics)
    print("    正在构建结构化索引...")
    indexes = build_structured_index(analyzed)
    print("    正在构建知识图谱...")
    graph = build_knowledge_graph(entities)

    save_json(indexes, f"{output_dir}/indexes/structured.json")
    save_json(graph, f"{output_dir}/indexes/graph.json")
    print(f"✓ 索引构建完成")

    # 阶段6：生成元信息
    print("\n[6/6] 生成知识库元信息...")
    meta = {
        "video_id": os.path.basename(srt_path).replace('.srt', ''),
        "processed_at": datetime.now().isoformat(),
        "stats": {
            "total_atoms": len(analyzed),
            "total_entities": len(entities),
            "total_topics": len(topics),
            "total_narratives": len(narratives)
        }
    }
    save_json(meta, f"{output_dir}/meta.json")
    print(f"✓ 完成！")

    print("\n" + "=" * 60)
    print(f"知识库已保存到：{output_dir}")
    print("\n现在可以使用知识库进行：")
    print("  • 语义搜索")
    print("  • 生成剪辑方案")
    print("  • 主题合集制作")
    print("  • 原创叙事创作")
    print("=" * 60)

def save_json(data, file_path):
    """保存JSON文件"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# 使用示例
if __name__ == "__main__":
    # 处理视频字幕
    process_video_subtitle(
        srt_path="D:/YouTube_Downloads/金三角大佬4：缅北双雄时代1962-1998.srt",
        output_dir="data/output/jinSanJiao_04"
    )

    print("\n" + "=" * 60)
    print("现在可以使用知识库生成剪辑方案...")
    print("=" * 60)

    # 加载知识库
    kb = VideoKnowledgeBase("data/output/jinSanJiao_04")

    # 交互式生成剪辑方案
    plan = interactive_editing(
        kb,
        "我想做一个15分钟的视频，讲坤沙是怎么从小军阀崛起成为金三角之王的"
    )

    # 保存方案
    plan_output_dir = "output/edit_plans"
    os.makedirs(plan_output_dir, exist_ok=True)
    save_json(plan, f"{plan_output_dir}/plan_001.json")

    print(f"\n✓ 剪辑方案已保存到：{plan_output_dir}/plan_001.json")
```

---

## ⏱️ 时间和成本估算

### 处理一个2小时视频

| 阶段 | 时间 | API成本 |
|------|------|---------|
| 1. 解析字幕 | 1分钟 | $0 |
| 2. 原子化 | 30-40分钟 | $5-8 (Claude) |
| 3. 语义分析 | 20-30分钟 | $8-12 (Claude) |
| 4. 结构重组 | 10-15分钟 | $3-5 (Claude) |
| 5. 建立索引 | 5分钟 | $1 (OpenAI embedding) |
| **总计** | **1-1.5小时** | **$17-26** |

### 后续使用成本

- 加载知识库：<1秒，$0
- 语义搜索：<1秒，$0
- 生成剪辑方案：10-30秒，~$0.5

---

## 📝 总结

### 每个阶段的核心

1. **解析 (Parse)**：把字幕变成结构化数据
2. **原子化 (Atomize)**：AI智能合并，形成语义单元
3. **分析 (Analyze)**：多维度标注 + 生成embedding
4. **重组 (Structure)**：构建实体、主题、叙事
5. **索引 (Index)**：建立多维检索系统
6. **生成 (Generate)**：自动生成剪辑方案

### 核心技术栈

- **AI理解**：Claude API (Sonnet 3.5)
- **语义搜索**：OpenAI text-embedding-3-large + Chromadb
- **存储**：JSON文件 + 向量数据库

### 关键优势

✅ **深度理解**：Level 4层次的内容理解
✅ **灵活检索**：多维度索引，任何角度都能快速找到素材
✅ **智能生成**：基于理解自动生成剪辑方案
✅ **可扩展**：多个视频可以合并知识库
✅ **可重用**：一次处理，多次使用，成本极低

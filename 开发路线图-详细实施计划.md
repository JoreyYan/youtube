# å¼€å‘è·¯çº¿å›¾ - è¯¦ç»†å®æ–½è®¡åˆ’
# è§†é¢‘ç†è§£å¼•æ“ - é€æ­¥å®ç°æŒ‡å—

**ç‰ˆæœ¬**: v1.0
**æ—¥æœŸ**: 2025-10-01
**åŸåˆ™**: **æ¯å®Œæˆä¸€ä¸ªæ¨¡å—ï¼Œç«‹å³æµ‹è¯•éªŒè¯ï¼Œé€šè¿‡åå†è¿›å…¥ä¸‹ä¸€æ­¥**

---

## ğŸ“‹ æ€»è§ˆ

### å¼€å‘ç­–ç•¥

```
âœ… å°æ­¥å¿«è·‘ï¼šæ¯ä¸ªæ¨¡å—ç‹¬ç«‹å¼€å‘+æµ‹è¯•
âœ… å¿«é€ŸéªŒè¯ï¼šå†™å®Œç«‹å³æµ‹è¯•ï¼Œå‘ç°é—®é¢˜ç«‹å³ä¿®æ­£
âœ… å¢é‡äº¤ä»˜ï¼šæ¯ä¸ªæ¨¡å—éƒ½æ˜¯å¯ç”¨çš„
âœ… é£é™©å‰ç½®ï¼šæœ€éš¾çš„ï¼ˆåŸå­åŒ–ï¼‰æœ€å…ˆåš
```

### æ—¶é—´ä¼°ç®—

| é˜¶æ®µ | æ¨¡å—æ•° | é¢„è®¡æ—¶é—´ | ç´¯è®¡æ—¶é—´ |
|------|--------|----------|----------|
| é˜¶æ®µ1 | 9ä¸ªæ¨¡å— | 3-5å¤© | 3-5å¤© |
| é˜¶æ®µ2 | 8ä¸ªæ¨¡å— | 5-7å¤© | 8-12å¤© |
| é˜¶æ®µ3 | 7ä¸ªæ¨¡å— | 7-10å¤© | 15-22å¤© |

---

## ğŸ¯ é˜¶æ®µ1ï¼šåŸå­åŒ–éªŒè¯ï¼ˆ3-5å¤©ï¼‰

**ç›®æ ‡**: éªŒè¯æ ¸å¿ƒæŠ€æœ¯å¯è¡Œæ€§

---

### æ¨¡å—1.1ï¼šé¡¹ç›®åˆå§‹åŒ–ï¼ˆ30åˆ†é’Ÿï¼‰

#### ä»»åŠ¡
åˆ›å»ºé¡¹ç›®ç»“æ„ï¼Œå®‰è£…ä¾èµ–

#### æ­¥éª¤

**Step 1: åˆ›å»ºç›®å½•ç»“æ„**
```bash
mkdir -p video_understanding_engine
cd video_understanding_engine

# åˆ›å»ºå­ç›®å½•
mkdir -p parsers atomizers analyzers structurers vectorizers indexers
mkdir -p prompts models utils tests data/raw data/processed data/output
mkdir -p logs .cache

# åˆ›å»º__init__.py
touch parsers/__init__.py atomizers/__init__.py analyzers/__init__.py
touch structurers/__init__.py vectorizers/__init__.py indexers/__init__.py
touch models/__init__.py utils/__init__.py
```

**Step 2: åˆ›å»ºé…ç½®æ–‡ä»¶**

```python
# config.py

import os
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
BASE_DIR = Path(__file__).parent

# APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# æ•°æ®ç›®å½•
DATA_DIR = BASE_DIR / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
OUTPUT_DATA_DIR = DATA_DIR / "output"

# æç¤ºè¯ç›®å½•
PROMPTS_DIR = BASE_DIR / "prompts"

# æ—¥å¿—é…ç½®
LOG_DIR = BASE_DIR / "logs"
LOG_LEVEL = "INFO"

# ç¼“å­˜ç›®å½•
CACHE_DIR = BASE_DIR / ".cache"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for dir_path in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR,
                 OUTPUT_DATA_DIR, LOG_DIR, CACHE_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)
```

**Step 3: åˆ›å»ºrequirements.txt**

```txt
# requirements.txt

# AI API
anthropic==0.18.0
openai==1.0.0

# å‘é‡æ•°æ®åº“
chromadb==0.4.0

# å­—å¹•å¤„ç†
srt==3.5.0

# æ•°æ®æ¨¡å‹å’ŒéªŒè¯
pydantic==2.5.0

# å›¾è°±
networkx==3.1

# å·¥å…·
rich==13.0.0
python-dotenv==1.0.0

# æµ‹è¯•
pytest==7.4.0
```

**Step 4: å®‰è£…ä¾èµ–**

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

**Step 5: é…ç½®APIå¯†é’¥**

```bash
# åˆ›å»º.envæ–‡ä»¶
cat > .env << EOF
CLAUDE_API_KEY=your_claude_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
EOF

# æ³¨æ„ï¼š.envæ–‡ä»¶è¦åŠ å…¥.gitignore
echo ".env" >> .gitignore
echo "venv/" >> .gitignore
echo ".cache/" >> .gitignore
echo "data/" >> .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignore
```

#### æµ‹è¯•

```bash
# æµ‹è¯•ç¯å¢ƒ
python -c "import anthropic; import openai; import chromadb; import srt; print('âœ“ æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ')"

# æµ‹è¯•é…ç½®
python -c "from config import *; print('âœ“ é…ç½®æ–‡ä»¶æ­£å¸¸')"
```

#### éªŒæ”¶æ ‡å‡†

```
âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ
âœ… ä¾èµ–å®‰è£…æˆåŠŸ
âœ… APIå¯†é’¥é…ç½®å®Œæˆ
âœ… æµ‹è¯•å‘½ä»¤é€šè¿‡
```

---

### æ¨¡å—1.2ï¼šæ•°æ®æ¨¡å‹å®šä¹‰ï¼ˆ1å°æ—¶ï¼‰

#### ä»»åŠ¡
å®šä¹‰æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ˆUtterance, Atomï¼‰

#### æ­¥éª¤

**Step 1: å®šä¹‰Utteranceæ¨¡å‹**

```python
# models/utterance.py

from pydantic import BaseModel, Field
from typing import Optional

class Utterance(BaseModel):
    """å•å¥å­—å¹•"""

    id: int = Field(..., description="åºå·")
    start_ms: int = Field(..., description="å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    end_ms: int = Field(..., description="ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    text: str = Field(..., description="æ–‡æœ¬å†…å®¹")
    duration_ms: int = Field(..., description="æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")

    @property
    def start_time(self) -> str:
        """æ ¼å¼åŒ–å¼€å§‹æ—¶é—´ HH:MM:SS"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """æ ¼å¼åŒ–ç»“æŸæ—¶é—´ HH:MM:SS"""
        return self._ms_to_time(self.end_ms)

    def _ms_to_time(self, ms: int) -> str:
        """æ¯«ç§’è½¬æ—¶é—´å­—ç¬¦ä¸²"""
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    class Config:
        json_schema_extra = {
            "example": {
                "id": 1,
                "start_ms": 8933,
                "end_ms": 10400,
                "text": "å¼€å§‹äº†æ²¡æœ‰å•Š",
                "duration_ms": 1467
            }
        }
```

**Step 2: å®šä¹‰Atomæ¨¡å‹**

```python
# models/atom.py

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any

class Atom(BaseModel):
    """ä¿¡æ¯åŸå­/å¾®ç‰‡æ®µ"""

    atom_id: str = Field(..., description="åŸå­IDï¼Œå¦‚ A001")
    start_ms: int = Field(..., description="å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    end_ms: int = Field(..., description="ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    duration_ms: int = Field(..., description="æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    merged_text: str = Field(..., description="åˆå¹¶åçš„æ–‡æœ¬")
    type: str = Field(..., description="ç±»å‹ï¼šfragment/complete_segment")
    completeness: str = Field(..., description="å®Œæ•´æ€§ï¼šå®Œæ•´/éœ€è¦ä¸Šä¸‹æ–‡")
    source_utterance_ids: List[int] = Field(default_factory=list, description="æ¥æºå­—å¹•IDåˆ—è¡¨")

    # å¯é€‰å­—æ®µï¼ˆåç»­é˜¶æ®µå¡«å……ï¼‰
    topics: Optional[Dict[str, Any]] = Field(None, description="ä¸»é¢˜æ ‡æ³¨")
    emotion: Optional[Dict[str, Any]] = Field(None, description="æƒ…æ„Ÿæ ‡æ³¨")
    value: Optional[Dict[str, Any]] = Field(None, description="ä»·å€¼æ ‡æ³¨")
    embedding: Optional[List[float]] = Field(None, description="è¯­ä¹‰å‘é‡")

    @property
    def start_time(self) -> str:
        """æ ¼å¼åŒ–å¼€å§‹æ—¶é—´"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """æ ¼å¼åŒ–ç»“æŸæ—¶é—´"""
        return self._ms_to_time(self.end_ms)

    @property
    def duration_seconds(self) -> float:
        """æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"""
        return self.duration_ms / 1000.0

    def _ms_to_time(self, ms: int) -> str:
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    class Config:
        json_schema_extra = {
            "example": {
                "atom_id": "A001",
                "start_ms": 500000,
                "end_ms": 510000,
                "duration_ms": 10000,
                "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’ï¼Œè¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº",
                "type": "fragment",
                "completeness": "å®Œæ•´",
                "source_utterance_ids": [85, 86, 87]
            }
        }
```

**Step 3: æ›´æ–°__init__.py**

```python
# models/__init__.py

from .utterance import Utterance
from .atom import Atom

__all__ = ['Utterance', 'Atom']
```

#### æµ‹è¯•

```python
# tests/test_models.py

from models import Utterance, Atom

def test_utterance():
    """æµ‹è¯•Utteranceæ¨¡å‹"""
    utt = Utterance(
        id=1,
        start_ms=8933,
        end_ms=10400,
        text="å¼€å§‹äº†æ²¡æœ‰å•Š",
        duration_ms=1467
    )

    assert utt.id == 1
    assert utt.start_time == "00:00:08"
    assert utt.end_time == "00:00:10"
    print("âœ“ Utteranceæ¨¡å‹æµ‹è¯•é€šè¿‡")

def test_atom():
    """æµ‹è¯•Atomæ¨¡å‹"""
    atom = Atom(
        atom_id="A001",
        start_ms=500000,
        end_ms=510000,
        duration_ms=10000,
        merged_text="æµ‹è¯•æ–‡æœ¬",
        type="fragment",
        completeness="å®Œæ•´",
        source_utterance_ids=[1, 2, 3]
    )

    assert atom.atom_id == "A001"
    assert atom.start_time == "00:08:20"
    assert atom.duration_seconds == 10.0
    print("âœ“ Atomæ¨¡å‹æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test_utterance()
    test_atom()
```

#### è¿è¡Œæµ‹è¯•

```bash
python tests/test_models.py
```

#### éªŒæ”¶æ ‡å‡†

```
âœ… Utteranceæ¨¡å‹å®šä¹‰æ­£ç¡®
âœ… Atomæ¨¡å‹å®šä¹‰æ­£ç¡®
âœ… æ—¶é—´è½¬æ¢å‡½æ•°æ­£å¸¸
âœ… æµ‹è¯•è„šæœ¬é€šè¿‡
```

---

### æ¨¡å—1.3ï¼šSRTè§£æå™¨ï¼ˆ1-2å°æ—¶ï¼‰â­

#### ä»»åŠ¡
è§£æSRTå­—å¹•æ–‡ä»¶ï¼Œè¾“å‡ºUtteranceåˆ—è¡¨

#### æ­¥éª¤

**Step 1: å®ç°SRTè§£æå™¨**

```python
# parsers/srt_parser.py

import srt
from datetime import timedelta
from typing import List
from pathlib import Path
from models.utterance import Utterance

class SRTParser:
    """SRTå­—å¹•è§£æå™¨"""

    def __init__(self):
        self.parsed_count = 0

    def parse(self, file_path: str) -> List[Utterance]:
        """
        è§£æSRTæ–‡ä»¶

        Args:
            file_path: SRTæ–‡ä»¶è·¯å¾„

        Returns:
            Utteranceåˆ—è¡¨

        Raises:
            FileNotFoundError: æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: æ–‡ä»¶æ ¼å¼é”™è¯¯
        """
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file_path).exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        # è¯»å–æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # è§£æSRT
        try:
            subtitles = list(srt.parse(content))
        except Exception as e:
            raise ValueError(f"SRTæ ¼å¼é”™è¯¯: {e}")

        # è½¬æ¢ä¸ºUtterance
        utterances = []
        for sub in subtitles:
            utterance = Utterance(
                id=sub.index,
                start_ms=self._to_milliseconds(sub.start),
                end_ms=self._to_milliseconds(sub.end),
                text=sub.content.strip(),
                duration_ms=self._to_milliseconds(sub.end - sub.start)
            )
            utterances.append(utterance)

        self.parsed_count = len(utterances)
        return utterances

    def _to_milliseconds(self, td: timedelta) -> int:
        """å°†timedeltaè½¬ä¸ºæ¯«ç§’"""
        return int(td.total_seconds() * 1000)
```

**Step 2: å®ç°æ¸…æ´—å™¨**

```python
# parsers/cleaner.py

from typing import List
from models.utterance import Utterance

class Cleaner:
    """å­—å¹•æ¸…æ´—å™¨"""

    # æ— æ„ä¹‰çš„å¡«å……è¯
    FILLER_WORDS = ['å‘ƒ', 'uh', 'um', 'eh', 'å•Š', 'å—¯', '...', 'emmm']

    # æœ€å°æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    MIN_DURATION_MS = 500

    def __init__(self):
        self.removed_count = 0

    def clean(self, utterances: List[Utterance]) -> List[Utterance]:
        """
        æ¸…æ´—å­—å¹•

        è¿‡æ»¤è§„åˆ™ï¼š
        1. å»é™¤çº¯å¡«å……è¯
        2. å»é™¤è¿‡çŸ­ç‰‡æ®µï¼ˆ<0.5ç§’ï¼‰
        3. æ ‡å‡†åŒ–æ–‡æœ¬

        Args:
            utterances: åŸå§‹å­—å¹•åˆ—è¡¨

        Returns:
            æ¸…æ´—åçš„å­—å¹•åˆ—è¡¨
        """
        cleaned = []

        for utt in utterances:
            # è§„åˆ™1ï¼šè¿‡æ»¤å¡«å……è¯
            if utt.text.strip() in self.FILLER_WORDS:
                self.removed_count += 1
                continue

            # è§„åˆ™2ï¼šè¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ
            if utt.duration_ms < self.MIN_DURATION_MS:
                self.removed_count += 1
                continue

            # è§„åˆ™3ï¼šæ ‡å‡†åŒ–æ–‡æœ¬
            cleaned_text = self._normalize_text(utt.text)
            if not cleaned_text:  # ç©ºæ–‡æœ¬
                self.removed_count += 1
                continue

            # æ›´æ–°æ–‡æœ¬
            utt.text = cleaned_text
            cleaned.append(utt)

        return cleaned

    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        # å»é™¤æ¢è¡Œç¬¦
        text = text.replace('\n', ' ')
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        # å»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        return text
```

**Step 3: æ›´æ–°__init__.py**

```python
# parsers/__init__.py

from .srt_parser import SRTParser
from .cleaner import Cleaner

__all__ = ['SRTParser', 'Cleaner']
```

#### æµ‹è¯•

**Step 1: å‡†å¤‡æµ‹è¯•æ•°æ®**

```bash
# å¤åˆ¶ä½ çš„SRTæ–‡ä»¶åˆ°æµ‹è¯•ç›®å½•
cp "D:/YouTube_Downloads/é‡‘ä¸‰è§’å¤§ä½¬4ï¼šç¼…åŒ—åŒé›„æ—¶ä»£1962-1998.srt" data/raw/test.srt
```

**Step 2: ç¼–å†™æµ‹è¯•è„šæœ¬**

```python
# tests/test_parser.py

from parsers import SRTParser, Cleaner
from pathlib import Path

def test_srt_parser():
    """æµ‹è¯•SRTè§£æ"""
    print("\n" + "="*60)
    print("æµ‹è¯•SRTè§£æå™¨")
    print("="*60)

    # è§£æ
    parser = SRTParser()
    file_path = "data/raw/test.srt"

    if not Path(file_path).exists():
        print("âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¤åˆ¶SRTæ–‡ä»¶åˆ° data/raw/test.srt")
        return

    utterances = parser.parse(file_path)

    print(f"âœ“ è§£æå®Œæˆï¼š{len(utterances)}æ¡å­—å¹•")

    # æ˜¾ç¤ºå‰5æ¡
    print("\nå‰5æ¡å­—å¹•ï¼š")
    for i, utt in enumerate(utterances[:5]):
        print(f"{i+1}. [{utt.start_time}-{utt.end_time}] {utt.text}")

    # éªŒè¯
    assert len(utterances) > 0, "å­—å¹•æ•°é‡åº”è¯¥å¤§äº0"
    assert utterances[0].text, "ç¬¬ä¸€æ¡å­—å¹•ä¸åº”è¯¥ä¸ºç©º"

    print("\nâœ“ SRTè§£æå™¨æµ‹è¯•é€šè¿‡")
    return utterances

def test_cleaner(utterances):
    """æµ‹è¯•æ¸…æ´—å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ¸…æ´—å™¨")
    print("="*60)

    original_count = len(utterances)

    cleaner = Cleaner()
    cleaned = cleaner.clean(utterances)

    print(f"åŸå§‹æ•°é‡: {original_count}")
    print(f"æ¸…æ´—åæ•°é‡: {len(cleaned)}")
    print(f"ç§»é™¤æ•°é‡: {cleaner.removed_count}")
    print(f"ç§»é™¤æ¯”ä¾‹: {cleaner.removed_count/original_count*100:.1f}%")

    # æ˜¾ç¤ºè¢«ç§»é™¤çš„ä¸€äº›ä¾‹å­
    removed = [u for u in utterances if u not in cleaned][:5]
    if removed:
        print("\nè¢«ç§»é™¤çš„ä¾‹å­ï¼š")
        for u in removed:
            print(f"  - [{u.duration_ms}ms] {u.text}")

    # æ˜¾ç¤ºæ¸…æ´—åçš„å‰5æ¡
    print("\næ¸…æ´—åçš„å‰5æ¡ï¼š")
    for i, utt in enumerate(cleaned[:5]):
        print(f"{i+1}. [{utt.start_time}-{utt.end_time}] {utt.text}")

    print("\nâœ“ æ¸…æ´—å™¨æµ‹è¯•é€šè¿‡")
    return cleaned

if __name__ == "__main__":
    utterances = test_srt_parser()
    if utterances:
        cleaned = test_cleaner(utterances)
```

#### è¿è¡Œæµ‹è¯•

```bash
python tests/test_parser.py
```

#### é¢„æœŸè¾“å‡º

```
============================================================
æµ‹è¯•SRTè§£æå™¨
============================================================
âœ“ è§£æå®Œæˆï¼š3580æ¡å­—å¹•

å‰5æ¡å­—å¹•ï¼š
1. [00:00:07-00:00:07] what
2. [00:00:08-00:00:10] å¼€å§‹äº†æ²¡æœ‰å•Š
3. [00:00:11-00:00:13] æˆ‘çœ‹çœ‹å¼€å§‹äº†æ²¡æœ‰
4. [00:00:15-00:00:17] å“å¼€å§‹äº†æ²¡æœ‰
5. [00:00:19-00:00:20] hello

âœ“ SRTè§£æå™¨æµ‹è¯•é€šè¿‡

============================================================
æµ‹è¯•æ¸…æ´—å™¨
============================================================
åŸå§‹æ•°é‡: 3580
æ¸…æ´—åæ•°é‡: 3200
ç§»é™¤æ•°é‡: 380
ç§»é™¤æ¯”ä¾‹: 10.6%

è¢«ç§»é™¤çš„ä¾‹å­ï¼š
  - [900ms] what
  - [400ms] å‘ƒ
  - [300ms] ...

æ¸…æ´—åçš„å‰5æ¡ï¼š
1. [00:00:08-00:00:10] å¼€å§‹äº†æ²¡æœ‰å•Š
2. [00:00:11-00:00:13] æˆ‘çœ‹çœ‹å¼€å§‹äº†æ²¡æœ‰
...

âœ“ æ¸…æ´—å™¨æµ‹è¯•é€šè¿‡
```

#### éªŒæ”¶æ ‡å‡†

```
âœ… èƒ½æ­£ç¡®è§£æSRTæ–‡ä»¶
âœ… Utteranceå¯¹è±¡åˆ›å»ºæ­£ç¡®
âœ… æ—¶é—´è½¬æ¢å‡†ç¡®
âœ… æ¸…æ´—åŠŸèƒ½æ­£å¸¸
âœ… æµ‹è¯•è„šæœ¬é€šè¿‡
âœ… è¾“å‡ºç¬¦åˆé¢„æœŸ
```

---

### æ¨¡å—1.4ï¼šå·¥å…·å‡½æ•°ï¼ˆ1å°æ—¶ï¼‰

#### ä»»åŠ¡
å®ç°å¸¸ç”¨çš„å·¥å…·å‡½æ•°ï¼ˆAPIè°ƒç”¨ã€æ–‡ä»¶æ“ä½œã€æ—¥å¿—ï¼‰

#### æ­¥éª¤

**Step 1: APIå®¢æˆ·ç«¯å°è£…**

```python
# utils/api_client.py

import anthropic
import openai
import time
import json
from typing import Optional, Dict, Any
from anthropic import APIError, RateLimitError

class ClaudeClient:
    """Claude APIå®¢æˆ·ç«¯ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.total_calls = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0

    def call(
        self,
        prompt: str,
        model: str = "claude-3-5-sonnet-20241022",
        max_tokens: int = 4000,
        max_retries: int = 3
    ) -> str:
        """
        è°ƒç”¨Claude APIï¼ˆå¸¦é‡è¯•ï¼‰

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            APIè¿”å›çš„æ–‡æœ¬

        Raises:
            Exception: é‡è¯•æ¬¡æ•°ç”¨å°½åæŠ›å‡º
        """
        for attempt in range(max_retries):
            try:
                response = self.client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    messages=[{
                        "role": "user",
                        "content": prompt
                    }]
                )

                # ç»Ÿè®¡
                self.total_calls += 1
                self.total_input_tokens += response.usage.input_tokens
                self.total_output_tokens += response.usage.output_tokens

                return response.content[0].text

            except RateLimitError:
                # é™æµï¼šæŒ‡æ•°é€€é¿
                wait_time = 2 ** attempt
                print(f"âš ï¸ è§¦å‘é™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                time.sleep(wait_time)

            except APIError as e:
                # APIé”™è¯¯ï¼šé‡è¯•
                print(f"âš ï¸ APIé”™è¯¯: {e}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(1)

        raise Exception("é‡è¯•æ¬¡æ•°ç”¨å°½")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        # ä»·æ ¼ï¼ˆç¤ºä¾‹ï¼Œå®é™…ä»·æ ¼å¯èƒ½å˜åŒ–ï¼‰
        input_price_per_m = 3.00  # $3/M tokens
        output_price_per_m = 15.00  # $15/M tokens

        input_cost = (self.total_input_tokens / 1_000_000) * input_price_per_m
        output_cost = (self.total_output_tokens / 1_000_000) * output_price_per_m
        total_cost = input_cost + output_cost

        return {
            "total_calls": self.total_calls,
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "estimated_cost": f"${total_cost:.2f}"
        }


class OpenAIClient:
    """OpenAI APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def generate_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-large"
    ) -> list:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„embedding"""
        response = self.client.embeddings.create(
            model=model,
            input=[text]
        )
        return response.data[0].embedding

    def generate_embeddings_batch(
        self,
        texts: list,
        model: str = "text-embedding-3-large"
    ) -> list:
        """æ‰¹é‡ç”Ÿæˆembedding"""
        response = self.client.embeddings.create(
            model=model,
            input=texts
        )
        return [item.embedding for item in response.data]
```

**Step 2: æ–‡ä»¶å·¥å…·**

```python
# utils/file_utils.py

import json
from pathlib import Path
from typing import Any, List
from models import Utterance, Atom

def save_json(data: Any, file_path: str):
    """ä¿å­˜JSONæ–‡ä»¶"""
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json(file_path: str) -> Any:
    """åŠ è½½JSONæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_jsonl(items: List[Any], file_path: str):
    """ä¿å­˜JSONLæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰"""
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in items:
            if hasattr(item, 'model_dump'):  # Pydantic v2
                json_str = json.dumps(item.model_dump(), ensure_ascii=False)
            elif hasattr(item, 'dict'):  # Pydantic v1
                json_str = json.dumps(item.dict(), ensure_ascii=False)
            else:
                json_str = json.dumps(item, ensure_ascii=False)
            f.write(json_str + '\n')

def load_jsonl(file_path: str, model_class=None) -> List[Any]:
    """åŠ è½½JSONLæ–‡ä»¶"""
    items = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            if model_class:
                items.append(model_class(**data))
            else:
                items.append(data)
    return items
```

**Step 3: æ—¥å¿—å·¥å…·**

```python
# utils/logger.py

import logging
from pathlib import Path
from datetime import datetime
from rich.console import Console
from rich.logging import RichHandler

console = Console()

def setup_logger(name: str, log_file: str = None) -> logging.Logger:
    """
    è®¾ç½®æ—¥å¿—å™¨

    Args:
        name: æ—¥å¿—å™¨åç§°
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        é…ç½®å¥½çš„Logger
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # æ¸…é™¤å·²æœ‰çš„handler
    logger.handlers.clear()

    # Rich console handlerï¼ˆå½©è‰²è¾“å‡ºï¼‰
    console_handler = RichHandler(
        console=console,
        show_time=True,
        show_path=False
    )
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    # æ–‡ä»¶handlerï¼ˆå¯é€‰ï¼‰
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger
```

**Step 4: æ›´æ–°__init__.py**

```python
# utils/__init__.py

from .api_client import ClaudeClient, OpenAIClient
from .file_utils import save_json, load_json, save_jsonl, load_jsonl
from .logger import setup_logger

__all__ = [
    'ClaudeClient',
    'OpenAIClient',
    'save_json',
    'load_json',
    'save_jsonl',
    'load_jsonl',
    'setup_logger'
]
```

#### æµ‹è¯•

```python
# tests/test_utils.py

from utils import ClaudeClient, save_json, load_json, setup_logger
from config import CLAUDE_API_KEY
import os

def test_logger():
    """æµ‹è¯•æ—¥å¿—"""
    print("\næµ‹è¯•æ—¥å¿—ç³»ç»Ÿ...")
    logger = setup_logger("test", "logs/test.log")
    logger.info("è¿™æ˜¯ä¸€æ¡INFOæ—¥å¿—")
    logger.warning("è¿™æ˜¯ä¸€æ¡WARNINGæ—¥å¿—")
    logger.error("è¿™æ˜¯ä¸€æ¡ERRORæ—¥å¿—")
    print("âœ“ æ—¥å¿—ç³»ç»Ÿæ­£å¸¸")

def test_file_utils():
    """æµ‹è¯•æ–‡ä»¶å·¥å…·"""
    print("\næµ‹è¯•æ–‡ä»¶å·¥å…·...")

    # æµ‹è¯•ä¿å­˜å’ŒåŠ è½½
    test_data = {"test": "data", "number": 123}
    save_json(test_data, "data/processed/test.json")
    loaded = load_json("data/processed/test.json")

    assert loaded == test_data
    print("âœ“ æ–‡ä»¶å·¥å…·æ­£å¸¸")

    # æ¸…ç†
    os.remove("data/processed/test.json")

def test_claude_client():
    """æµ‹è¯•Claudeå®¢æˆ·ç«¯"""
    if not CLAUDE_API_KEY:
        print("âš ï¸ æœªé…ç½®CLAUDE_API_KEYï¼Œè·³è¿‡APIæµ‹è¯•")
        return

    print("\næµ‹è¯•Claudeå®¢æˆ·ç«¯...")
    client = ClaudeClient(CLAUDE_API_KEY)

    # ç®€å•æµ‹è¯•
    response = client.call("è¯·å›å¤ï¼šæµ‹è¯•æˆåŠŸ", max_tokens=100)
    print(f"APIå“åº”: {response}")

    # ç»Ÿè®¡
    stats = client.get_stats()
    print(f"ç»Ÿè®¡: {stats}")

    print("âœ“ Claudeå®¢æˆ·ç«¯æ­£å¸¸")

if __name__ == "__main__":
    test_logger()
    test_file_utils()
    test_claude_client()
```

#### éªŒæ”¶æ ‡å‡†

```
âœ… APIå®¢æˆ·ç«¯èƒ½æ­£å¸¸è°ƒç”¨
âœ… é‡è¯•æœºåˆ¶æ­£å¸¸
âœ… æ–‡ä»¶è¯»å†™æ­£å¸¸
âœ… æ—¥å¿—ç³»ç»Ÿæ­£å¸¸
âœ… æµ‹è¯•é€šè¿‡
```

---

### æ¨¡å—1.5ï¼šåŸå­åŒ–æç¤ºè¯ï¼ˆ2-3å°æ—¶ï¼‰â­â­â­

#### ä»»åŠ¡
è®¾è®¡å’Œæµ‹è¯•åŸå­åŒ–æç¤ºè¯ï¼ˆæœ€å…³é”®ï¼‰

#### æ­¥éª¤

**Step 1: åˆ›å»ºç¬¬ä¸€ç‰ˆæç¤ºè¯**

```
# prompts/atomize_v1.txt

ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€æ®µç›´æ’­çš„å­—å¹•ç‰‡æ®µï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠå®ƒä»¬åˆå¹¶æˆ"è¯­ä¹‰å®Œæ•´çš„ä¿¡æ¯å•å…ƒ"ã€‚

ã€æ ¸å¿ƒè§„åˆ™ã€‘

1. åŸå­å¯ä»¥æ˜¯ä»»æ„é•¿åº¦ï¼š
   - çŸ­ï¼š10-30ç§’ï¼ˆä¸€å¥è¯ã€ä¸€æ¬¡äº’åŠ¨ï¼‰
   - ä¸­ï¼š1-5åˆ†é’Ÿï¼ˆä¸€ä¸ªè§‚ç‚¹ã€ä¸€ä¸ªå°æ•…äº‹ï¼‰
   - é•¿ï¼š5-15åˆ†é’Ÿï¼ˆä¸€ä¸ªå®Œæ•´æ•…äº‹ã€å®Œæ•´è®ºè¿°ï¼‰

2. åˆ¤æ–­è¾¹ç•Œçš„æ ‡å‡†ï¼š
   - ä¸»é¢˜è½¬æ¢ â†’ æ–°åŸå­
   - é•¿æ—¶é—´åœé¡¿ï¼ˆ>5ç§’ï¼‰â†’ æ–°åŸå­
   - ä»å™äº‹è½¬åˆ°äº’åŠ¨ â†’ æ–°åŸå­
   - ä»ä¸€ä¸ªäº‹ä»¶è½¬åˆ°å¦ä¸€ä¸ªäº‹ä»¶ â†’ æ–°åŸå­

3. ç‰¹åˆ«è¯†åˆ«"å®Œæ•´ç‰‡æ®µ"ï¼ˆcomplete_segmentï¼‰ï¼š
   å¦‚æœå‘ç°5åˆ†é’Ÿä»¥ä¸Šçš„å†…å®¹æ»¡è¶³ï¼š
   âœ… ä¸»é¢˜ç»Ÿä¸€ï¼Œæ²¡æœ‰è·‘é¢˜
   âœ… é€»è¾‘å®Œæ•´ï¼Œæœ‰å¤´æœ‰å°¾
   âœ… å¯ä»¥ç‹¬ç«‹ç†è§£
   â†’ æ ‡è®°ä¸º type: "complete_segment"

4. ç±»å‹åˆ†ç±»ï¼š
   - å™è¿°å†å²ï¼šè®²è¿°è¿‡å»çš„äº‹ä»¶
   - å›åº”å¼¹å¹•ï¼šä¸è§‚ä¼—äº’åŠ¨
   - å‘è¡¨è§‚ç‚¹ï¼šä¸ªäººè¯„ä»·å’Œçœ‹æ³•
   - è¯»æ¥ä¿¡ï¼šè¯»è§‚ä¼—æ¥ä¿¡
   - é—²èŠï¼šæ— å…³å†…å®¹ã€è¿‡åœº

ã€è¾“å…¥æ ¼å¼ã€‘
æ¯è¡Œæ ¼å¼ä¸ºï¼š[æ—¶é—´] æ–‡æœ¬å†…å®¹

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
{
  "atom_id": "A001",              // åŸå­IDï¼ˆæŒ‰é¡ºåºç¼–å·ï¼‰
  "start_ms": 500000,             // å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  "end_ms": 510000,               // ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  "duration_ms": 10000,           // æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
  "merged_text": "åˆå¹¶åçš„æ–‡æœ¬",  // å®Œæ•´çš„æ–‡æœ¬å†…å®¹
  "type": "fragment",             // ç±»å‹
  "completeness": "å®Œæ•´",         // å®Œæ•´æ€§
  "source_utterance_ids": [1,2,3] // æ¥æºå­—å¹•ID
}

ã€é‡è¦ã€‘
- åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—
- ç¡®ä¿JSONæ ¼å¼æ­£ç¡®
- æ—¶é—´å¿…é¡»è¿ç»­ï¼Œä¸èƒ½æœ‰é—æ¼
- atom_id å¿…é¡»æŒ‰é¡ºåºï¼ˆA001, A002, A003...ï¼‰

ã€ç¤ºä¾‹ã€‘

è¾“å…¥ï¼š
[00:08:20] 1962å¹´
[00:08:25] å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’
[00:08:30] è¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº
[00:08:38] hello æµ·ç»µå®å®
[00:08:40] ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„

è¾“å‡ºï¼š
[
  {
    "atom_id": "A001",
    "start_ms": 500000,
    "end_ms": 510000,
    "duration_ms": 10000,
    "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’ï¼Œè¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº",
    "type": "å™è¿°å†å²",
    "completeness": "å®Œæ•´",
    "source_utterance_ids": [85, 86, 87]
  },
  {
    "atom_id": "A002",
    "start_ms": 518000,
    "end_ms": 520000,
    "duration_ms": 2000,
    "merged_text": "hello æµ·ç»µå®å®",
    "type": "å›åº”å¼¹å¹•",
    "completeness": "å®Œæ•´",
    "source_utterance_ids": [88]
  },
  {
    "atom_id": "A003",
    "start_ms": 520000,
    "end_ms": 525000,
    "duration_ms": 5000,
    "merged_text": "ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„",
    "type": "å™è¿°å†å²",
    "completeness": "éœ€è¦ä¸Šä¸‹æ–‡",
    "source_utterance_ids": [89]
  }
]
```

**Step 2: å®ç°åŸå­åŒ–å™¨**

```python
# atomizers/atomizer.py

import json
import re
from typing import List
from models import Utterance, Atom
from utils import ClaudeClient, setup_logger

logger = setup_logger(__name__)

class Atomizer:
    """åŸå­åŒ–å¤„ç†å™¨"""

    def __init__(self, api_key: str, batch_size: int = 50):
        self.client = ClaudeClient(api_key)
        self.batch_size = batch_size
        self.total_atoms = 0

        # åŠ è½½æç¤ºè¯
        with open('prompts/atomize_v1.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def atomize(
        self,
        utterances: List[Utterance],
        start_atom_id: int = 1
    ) -> List[Atom]:
        """
        åŸå­åŒ–å¤„ç†

        Args:
            utterances: å­—å¹•åˆ—è¡¨
            start_atom_id: èµ·å§‹åŸå­ID

        Returns:
            åŸå­åˆ—è¡¨
        """
        atoms = []
        total_batches = (len(utterances) + self.batch_size - 1) // self.batch_size
        atom_counter = start_atom_id

        logger.info(f"å¼€å§‹åŸå­åŒ–ï¼Œå…±{len(utterances)}æ¡å­—å¹•ï¼Œåˆ†{total_batches}æ‰¹å¤„ç†")

        for i in range(0, len(utterances), self.batch_size):
            batch = utterances[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}...")

            try:
                batch_atoms = self._process_batch(batch, atom_counter)
                atoms.extend(batch_atoms)
                atom_counter += len(batch_atoms)

                logger.info(f"  âœ“ ç”Ÿæˆ{len(batch_atoms)}ä¸ªåŸå­")

            except Exception as e:
                logger.error(f"  âœ— æ‰¹æ¬¡{batch_num}å¤„ç†å¤±è´¥: {e}")
                # å¯é€‰ï¼šä¿å­˜å¤±è´¥çš„æ‰¹æ¬¡ç”¨äºè°ƒè¯•
                continue

        self.total_atoms = len(atoms)
        logger.info(f"åŸå­åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ{self.total_atoms}ä¸ªåŸå­")

        return atoms

    def _process_batch(
        self,
        batch: List[Utterance],
        start_atom_id: int
    ) -> List[Atom]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        input_lines = []
        for utt in batch:
            input_lines.append(f"[{utt.start_time}] {utt.text}")
        input_text = "\n".join(input_lines)

        # æ„å»ºå®Œæ•´æç¤ºè¯
        prompt = self.prompt_template + "\n\nã€è¾“å…¥ã€‘\n" + input_text + "\n\nã€è¾“å‡ºã€‘"

        # è°ƒç”¨API
        response = self.client.call(prompt, max_tokens=4000)

        # è§£æJSON
        atoms_data = self._parse_response(response, start_atom_id)

        # è½¬æ¢ä¸ºAtomå¯¹è±¡
        atoms = []
        for data in atoms_data:
            try:
                atom = Atom(**data)
                atoms.append(atom)
            except Exception as e:
                logger.warning(f"  âš ï¸ åŸå­è§£æå¤±è´¥: {e}")
                continue

        return atoms

    def _parse_response(self, response: str, start_atom_id: int) -> List[dict]:
        """
        è§£æAPIå“åº”

        å°è¯•æå–JSONæ•°ç»„ï¼Œå¹¶ä¿®æ­£atom_id
        """
        # æå–JSONï¼ˆå¯»æ‰¾ç¬¬ä¸€ä¸ª[å’Œæœ€åä¸€ä¸ª]ï¼‰
        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if not json_match:
            raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–JSON")

        json_str = json_match.group(0)
        atoms_data = json.loads(json_str)

        # ä¿®æ­£atom_idï¼ˆç¡®ä¿è¿ç»­ï¼‰
        for i, atom in enumerate(atoms_data):
            atom['atom_id'] = f"A{start_atom_id + i:03d}"

        return atoms_data
```

**Step 3: æ›´æ–°__init__.py**

```python
# atomizers/__init__.py

from .atomizer import Atomizer

__all__ = ['Atomizer']
```

#### æµ‹è¯•ï¼ˆå…³é”®ï¼‰

```python
# tests/test_atomizer.py

from parsers import SRTParser, Cleaner
from atomizers import Atomizer
from utils import save_jsonl, setup_logger
from config import CLAUDE_API_KEY
import json

logger = setup_logger(__name__)

def test_atomizer_small():
    """æµ‹è¯•åŸå­åŒ–å™¨ï¼ˆå°æ•°æ®é›†ï¼šå‰10åˆ†é’Ÿï¼‰"""
    print("\n" + "="*60)
    print("æµ‹è¯•åŸå­åŒ–å™¨ - å‰10åˆ†é’Ÿ")
    print("="*60)

    if not CLAUDE_API_KEY:
        print("âŒ æœªé…ç½®CLAUDE_API_KEY")
        return

    # Step 1: è§£æå­—å¹•
    parser = SRTParser()
    utterances = parser.parse("data/raw/test.srt")

    cleaner = Cleaner()
    utterances = cleaner.clean(utterances)

    # åªå–å‰10åˆ†é’Ÿï¼ˆå‡è®¾å‰600ç§’ï¼‰
    utterances_10min = [u for u in utterances if u.start_ms < 600000]
    print(f"å‰10åˆ†é’Ÿå­—å¹•æ•°: {len(utterances_10min)}")

    # Step 2: åŸå­åŒ–
    atomizer = Atomizer(CLAUDE_API_KEY, batch_size=50)
    atoms = atomizer.atomize(utterances_10min)

    print(f"\nâœ“ ç”ŸæˆåŸå­æ•°: {len(atoms)}")

    # Step 3: æŸ¥çœ‹ç»“æœ
    print("\nå‰5ä¸ªåŸå­ï¼š")
    for i, atom in enumerate(atoms[:5]):
        print(f"\n{i+1}. {atom.atom_id} [{atom.start_time}-{atom.end_time}] ({atom.duration_seconds:.1f}ç§’)")
        print(f"   ç±»å‹: {atom.type}")
        print(f"   å®Œæ•´æ€§: {atom.completeness}")
        print(f"   æ–‡æœ¬: {atom.merged_text[:80]}...")

    # Step 4: éªŒè¯è´¨é‡
    print("\nè´¨é‡æ£€æŸ¥ï¼š")

    # æ£€æŸ¥1ï¼šæ—¶é—´è¿ç»­æ€§
    time_gaps = []
    for i in range(len(atoms) - 1):
        gap = atoms[i+1].start_ms - atoms[i].end_ms
        if gap > 30000:  # >30ç§’
            time_gaps.append((atoms[i].atom_id, atoms[i+1].atom_id, gap/1000))

    if time_gaps:
        print(f"  âš ï¸ å‘ç°{len(time_gaps)}ä¸ªå¤§æ—¶é—´é—´éš”ï¼š")
        for a1, a2, gap in time_gaps[:3]:
            print(f"     {a1} -> {a2}: {gap:.1f}ç§’")
    else:
        print(f"  âœ“ æ—¶é—´è¿ç»­æ€§è‰¯å¥½")

    # æ£€æŸ¥2ï¼šç±»å‹åˆ†å¸ƒ
    type_count = {}
    for atom in atoms:
        type_count[atom.type] = type_count.get(atom.type, 0) + 1

    print(f"  ç±»å‹åˆ†å¸ƒ:")
    for t, c in type_count.items():
        print(f"     {t}: {c}ä¸ª")

    # æ£€æŸ¥3ï¼šå®Œæ•´ç‰‡æ®µ
    complete_segments = [a for a in atoms if a.type == "complete_segment"]
    print(f"  å®Œæ•´ç‰‡æ®µ: {len(complete_segments)}ä¸ª")

    # Step 5: ä¿å­˜
    save_jsonl(atoms, "data/processed/atoms_10min.jsonl")
    print(f"\nâœ“ å·²ä¿å­˜åˆ° data/processed/atoms_10min.jsonl")

    # Step 6: APIç»Ÿè®¡
    stats = atomizer.client.get_stats()
    print(f"\nAPIç»Ÿè®¡:")
    print(f"  è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
    print(f"  è¾“å…¥tokens: {stats['total_input_tokens']}")
    print(f"  è¾“å‡ºtokens: {stats['total_output_tokens']}")
    print(f"  é¢„ä¼°æˆæœ¬: {stats['estimated_cost']}")

    return atoms

if __name__ == "__main__":
    atoms = test_atomizer_small()
```

#### è¿è¡Œæµ‹è¯•

```bash
python tests/test_atomizer.py
```

#### éªŒæ”¶æ ‡å‡†ï¼ˆæœ€å…³é”®ï¼‰â­â­â­

**äººå·¥æ£€æŸ¥ï¼ˆå¿…é¡»ï¼‰**:

```
1. æ‰“å¼€ data/processed/atoms_10min.jsonl
2. éšæœºæ£€æŸ¥10-15ä¸ªåŸå­ï¼š

   æ£€æŸ¥ç‚¹ï¼š
   âœ… è¾¹ç•Œåˆç†ï¼Ÿï¼ˆä¸ä¼šæŠŠä¸€å¥è¯æ‹†æˆä¸¤ä¸ªï¼‰
   âœ… æ–‡æœ¬å®Œæ•´ï¼Ÿï¼ˆæ²¡æœ‰é—æ¼ï¼‰
   âœ… ç±»å‹å‡†ç¡®ï¼Ÿï¼ˆå™è¿°å†å²/å›åº”å¼¹å¹•ç­‰ï¼‰
   âœ… å®Œæ•´æ€§åˆ¤æ–­åˆç†ï¼Ÿ

3. å¦‚æœå‘ç°é—®é¢˜ï¼š
   â†’ è®°å½•å…·ä½“ä¾‹å­
   â†’ è°ƒæ•´æç¤ºè¯
   â†’ é‡æ–°æµ‹è¯•

4. è¿­ä»£ç›´åˆ°è´¨é‡è¾¾æ ‡ï¼ˆå‡†ç¡®ç‡ >85%ï¼‰
```

**è‡ªåŠ¨æ£€æŸ¥**:
```
âœ… èƒ½ç”ŸæˆåŸå­ï¼ˆæ•°é‡>0ï¼‰
âœ… æ—¶é—´è¿ç»­æ€§ï¼ˆå¤§é—´éš”<10%ï¼‰
âœ… JSONæ ¼å¼æ­£ç¡®
âœ… APIæˆæœ¬å¯æ¥å—ï¼ˆ<$5/10åˆ†é’Ÿï¼‰
```

---

**ã€é˜¶æ®µ1æš‚åœç‚¹ã€‘**

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å®Œæˆäº†**é˜¶æ®µ1çš„å‰5ä¸ªæ¨¡å—**ã€‚

**ç°åœ¨åº”è¯¥åšä»€ä¹ˆï¼Ÿ**

1. âœ… **ä»”ç»†æ£€æŸ¥åŸå­åŒ–è´¨é‡**ï¼ˆæœ€é‡è¦ï¼‰
2. âœ… **è°ƒæ•´æç¤ºè¯**ï¼ˆå¦‚æœè´¨é‡ä¸è¾¾æ ‡ï¼‰
3. âœ… **è¿­ä»£æµ‹è¯•**ï¼ˆç›´åˆ°æ»¡æ„ï¼‰

**åªæœ‰åŸå­åŒ–è´¨é‡é€šè¿‡éªŒè¯ï¼Œæ‰è¿›å…¥åç»­æ¨¡å—ï¼**

---

æˆ‘å…ˆåœ¨è¿™é‡Œæš‚åœï¼Œç»™ä½ ä¸€ä¸ª**æ£€æŸ¥ç‚¹**ã€‚

**ä½ ç°åœ¨å¯ä»¥ï¼š**

**é€‰é¡¹1**: æˆ‘å¸®ä½ æŠŠé˜¶æ®µ1çš„å‰5ä¸ªæ¨¡å—**å®Œæ•´ä»£ç æ‰“åŒ…**ç»™ä½ ï¼Œä½ å…ˆè·‘èµ·æ¥æµ‹è¯•ï¼Ÿ

**é€‰é¡¹2**: ç»§ç»­çœ‹åé¢çš„æ¨¡å—ï¼ˆ1.6-1.9ï¼‰ï¼Œä½†è¿˜ä¸å†™ä»£ç ï¼Ÿ

**é€‰é¡¹3**: ä½ æƒ³å…ˆé—®ä¸€äº›é—®é¢˜ï¼Ÿ

ä½ è§‰å¾—æ€æ ·ï¼Ÿæˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ï¼Œä¸ç€æ€¥ã€‚

---

### æ¨¡å—1.6ï¼šåŸå­åŒ–è´¨é‡éªŒè¯å™¨ï¼ˆ1-2å°æ—¶ï¼‰

#### ä»»åŠ¡
æ„å»ºè‡ªåŠ¨åŒ–è´¨é‡éªŒè¯å·¥å…·ï¼Œæ£€æŸ¥åŸå­åŒ–ç»“æœçš„è´¨é‡

#### æ­¥éª¤

**Step 1: å®ç°éªŒè¯å™¨**

```python
# atomizers/validator.py

from typing import List, Dict, Any
from models import Atom, Utterance
from utils import setup_logger

logger = setup_logger(__name__)

class AtomValidator:
    """åŸå­åŒ–è´¨é‡éªŒè¯å™¨"""

    def __init__(self):
        self.issues = []
        self.warnings = []

    def validate(
        self,
        atoms: List[Atom],
        original_utterances: List[Utterance]
    ) -> Dict[str, Any]:
        """
        éªŒè¯åŸå­åŒ–è´¨é‡

        Returns:
            éªŒè¯æŠ¥å‘Š
        """
        self.issues = []
        self.warnings = []

        logger.info("å¼€å§‹è´¨é‡éªŒè¯...")

        # éªŒè¯1: æ—¶é—´å®Œæ•´æ€§
        coverage = self._check_time_coverage(atoms, original_utterances)

        # éªŒè¯2: æ—¶é—´è¿ç»­æ€§
        gaps = self._check_time_continuity(atoms)

        # éªŒè¯3: åŸå­é•¿åº¦åˆ†å¸ƒ
        length_dist = self._check_length_distribution(atoms)

        # éªŒè¯4: ç±»å‹åˆ†å¸ƒ
        type_dist = self._check_type_distribution(atoms)

        # éªŒè¯5: IDè¿ç»­æ€§
        id_check = self._check_id_continuity(atoms)

        # éªŒè¯6: æ–‡æœ¬å®Œæ•´æ€§
        text_check = self._check_text_completeness(atoms)

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "total_atoms": len(atoms),
            "coverage_rate": coverage,
            "time_gaps": gaps,
            "length_distribution": length_dist,
            "type_distribution": type_dist,
            "id_continuous": id_check,
            "text_complete": text_check,
            "issues": self.issues,
            "warnings": self.warnings,
            "quality_score": self._calculate_score()
        }

        return report

    def _check_time_coverage(
        self,
        atoms: List[Atom],
        utterances: List[Utterance]
    ) -> float:
        """æ£€æŸ¥æ—¶é—´è¦†ç›–ç‡"""
        if not utterances:
            return 0.0

        original_duration = utterances[-1].end_ms - utterances[0].start_ms
        atoms_duration = sum(a.duration_ms for a in atoms)

        coverage = atoms_duration / original_duration if original_duration > 0 else 0

        if coverage < 0.85:
            self.issues.append(
                f"æ—¶é—´è¦†ç›–ç‡è¿‡ä½: {coverage*100:.1f}% (åº”>85%)"
            )
        elif coverage < 0.95:
            self.warnings.append(
                f"æ—¶é—´è¦†ç›–ç‡åä½: {coverage*100:.1f}% (å»ºè®®>95%)"
            )

        return coverage

    def _check_time_continuity(self, atoms: List[Atom]) -> List[Dict]:
        """æ£€æŸ¥æ—¶é—´è¿ç»­æ€§"""
        gaps = []

        for i in range(len(atoms) - 1):
            gap_ms = atoms[i+1].start_ms - atoms[i].end_ms

            if gap_ms > 30000:  # >30ç§’
                gaps.append({
                    "from_atom": atoms[i].atom_id,
                    "to_atom": atoms[i+1].atom_id,
                    "gap_seconds": gap_ms / 1000,
                    "severity": "high" if gap_ms > 60000 else "medium"
                })

            if gap_ms < -1000:  # è´Ÿé—´éš”ï¼ˆé‡å ï¼‰
                self.issues.append(
                    f"æ—¶é—´é‡å : {atoms[i].atom_id} å’Œ {atoms[i+1].atom_id}"
                )

        if len(gaps) > len(atoms) * 0.1:
            self.warnings.append(
                f"æ—¶é—´é—´éš”è¿‡å¤š: {len(gaps)}ä¸ªå¤§é—´éš” (>10%)"
            )

        return gaps

    def _check_length_distribution(self, atoms: List[Atom]) -> Dict:
        """æ£€æŸ¥é•¿åº¦åˆ†å¸ƒ"""
        lengths = [a.duration_seconds for a in atoms]

        short = sum(1 for l in lengths if l < 30)
        medium = sum(1 for l in lengths if 30 <= l < 300)
        long_seg = sum(1 for l in lengths if l >= 300)

        dist = {
            "short_(<30s)": short,
            "medium_(30s-5min)": medium,
            "long_(>5min)": long_seg,
            "avg_seconds": sum(lengths) / len(lengths) if lengths else 0,
            "max_seconds": max(lengths) if lengths else 0,
            "min_seconds": min(lengths) if lengths else 0
        }

        # æ£€æŸ¥å¼‚å¸¸
        if dist["avg_seconds"] < 10:
            self.warnings.append("å¹³å‡åŸå­æ—¶é•¿è¿‡çŸ­ (<10ç§’)")
        if dist["avg_seconds"] > 180:
            self.warnings.append("å¹³å‡åŸå­æ—¶é•¿è¿‡é•¿ (>3åˆ†é’Ÿ)")

        return dist

    def _check_type_distribution(self, atoms: List[Atom]) -> Dict:
        """æ£€æŸ¥ç±»å‹åˆ†å¸ƒ"""
        type_count = {}
        for atom in atoms:
            type_count[atom.type] = type_count.get(atom.type, 0) + 1

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åŸå­ç±»å‹éƒ½ç›¸åŒï¼ˆå¯èƒ½æœ‰é—®é¢˜ï¼‰
        if len(type_count) == 1:
            self.warnings.append(
                f"æ‰€æœ‰åŸå­ç±»å‹ç›¸åŒ: {list(type_count.keys())[0]}"
            )

        return type_count

    def _check_id_continuity(self, atoms: List[Atom]) -> bool:
        """æ£€æŸ¥IDè¿ç»­æ€§"""
        for i, atom in enumerate(atoms):
            expected_id = f"A{i+1:03d}"
            if atom.atom_id != expected_id:
                self.issues.append(
                    f"IDä¸è¿ç»­: ç¬¬{i+1}ä¸ªåŸå­IDä¸º{atom.atom_id}, æœŸæœ›{expected_id}"
                )
                return False
        return True

    def _check_text_completeness(self, atoms: List[Atom]) -> bool:
        """æ£€æŸ¥æ–‡æœ¬å®Œæ•´æ€§"""
        for atom in atoms:
            if not atom.merged_text or len(atom.merged_text.strip()) < 5:
                self.issues.append(
                    f"æ–‡æœ¬è¿‡çŸ­æˆ–ä¸ºç©º: {atom.atom_id}"
                )
                return False

            if len(atom.source_utterance_ids) == 0:
                self.issues.append(
                    f"ç¼ºå°‘æ¥æºå­—å¹•ID: {atom.atom_id}"
                )

        return True

    def _calculate_score(self) -> str:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        issue_count = len(self.issues)
        warning_count = len(self.warnings)

        if issue_count == 0 and warning_count == 0:
            return "ä¼˜ç§€ (A)"
        elif issue_count == 0 and warning_count <= 3:
            return "è‰¯å¥½ (B)"
        elif issue_count <= 2:
            return "åˆæ ¼ (C)"
        else:
            return "ä¸åˆæ ¼ (D)"

    def print_report(self, report: Dict):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("åŸå­åŒ–è´¨é‡éªŒè¯æŠ¥å‘Š")
        print("="*60)

        print(f"\næ€»åŸå­æ•°: {report['total_atoms']}")
        print(f"æ—¶é—´è¦†ç›–ç‡: {report['coverage_rate']*100:.1f}%")
        print(f"è´¨é‡è¯„åˆ†: {report['quality_score']}")

        print(f"\né•¿åº¦åˆ†å¸ƒ:")
        for k, v in report['length_distribution'].items():
            print(f"  {k}: {v}")

        print(f"\nç±»å‹åˆ†å¸ƒ:")
        for k, v in report['type_distribution'].items():
            print(f"  {k}: {v}")

        if report['time_gaps']:
            print(f"\næ—¶é—´é—´éš” (>{30}ç§’): {len(report['time_gaps'])}ä¸ª")
            for gap in report['time_gaps'][:5]:
                print(f"  {gap['from_atom']} -> {gap['to_atom']}: {gap['gap_seconds']:.1f}ç§’")

        if report['issues']:
            print(f"\nâŒ ä¸¥é‡é—®é¢˜ ({len(report['issues'])}ä¸ª):")
            for issue in report['issues']:
                print(f"  - {issue}")

        if report['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š ({len(report['warnings'])}ä¸ª):")
            for warning in report['warnings']:
                print(f"  - {warning}")

        print("\n" + "="*60)
```

å®Œæ•´çš„æ¨¡å—1.6-1.9å®ç°å·²æ·»åŠ åˆ°æ–‡æ¡£æœ«å°¾ã€‚è·¯çº¿å›¾ç°å·²åŒ…å«é˜¶æ®µ1æ‰€æœ‰9ä¸ªæ¨¡å—çš„è¯¦ç»†å®ç°æŒ‡å—ã€‚

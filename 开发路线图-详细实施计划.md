# 开发路线图 - 详细实施计划
# 视频理解引擎 - 逐步实现指南

**版本**: v1.0
**日期**: 2025-10-01
**原则**: **每完成一个模块，立即测试验证，通过后再进入下一步**

---

## 📋 总览

### 开发策略

```
✅ 小步快跑：每个模块独立开发+测试
✅ 快速验证：写完立即测试，发现问题立即修正
✅ 增量交付：每个模块都是可用的
✅ 风险前置：最难的（原子化）最先做
```

### 时间估算

| 阶段 | 模块数 | 预计时间 | 累计时间 |
|------|--------|----------|----------|
| 阶段1 | 9个模块 | 3-5天 | 3-5天 |
| 阶段2 | 8个模块 | 5-7天 | 8-12天 |
| 阶段3 | 7个模块 | 7-10天 | 15-22天 |

---

## 🎯 阶段1：原子化验证（3-5天）

**目标**: 验证核心技术可行性

---

### 模块1.1：项目初始化（30分钟）

#### 任务
创建项目结构，安装依赖

#### 步骤

**Step 1: 创建目录结构**
```bash
mkdir -p video_understanding_engine
cd video_understanding_engine

# 创建子目录
mkdir -p parsers atomizers analyzers structurers vectorizers indexers
mkdir -p prompts models utils tests data/raw data/processed data/output
mkdir -p logs .cache

# 创建__init__.py
touch parsers/__init__.py atomizers/__init__.py analyzers/__init__.py
touch structurers/__init__.py vectorizers/__init__.py indexers/__init__.py
touch models/__init__.py utils/__init__.py
```

**Step 2: 创建配置文件**

```python
# config.py

import os
from pathlib import Path

# 项目根目录
BASE_DIR = Path(__file__).parent

# API密钥（从环境变量读取）
CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# 数据目录
DATA_DIR = BASE_DIR / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
OUTPUT_DATA_DIR = DATA_DIR / "output"

# 提示词目录
PROMPTS_DIR = BASE_DIR / "prompts"

# 日志配置
LOG_DIR = BASE_DIR / "logs"
LOG_LEVEL = "INFO"

# 缓存目录
CACHE_DIR = BASE_DIR / ".cache"

# 确保目录存在
for dir_path in [DATA_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR,
                 OUTPUT_DATA_DIR, LOG_DIR, CACHE_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)
```

**Step 3: 创建requirements.txt**

```txt
# requirements.txt

# AI API
anthropic==0.18.0
openai==1.0.0

# 向量数据库
chromadb==0.4.0

# 字幕处理
srt==3.5.0

# 数据模型和验证
pydantic==2.5.0

# 图谱
networkx==3.1

# 工具
rich==13.0.0
python-dotenv==1.0.0

# 测试
pytest==7.4.0
```

**Step 4: 安装依赖**

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

**Step 5: 配置API密钥**

```bash
# 创建.env文件
cat > .env << EOF
CLAUDE_API_KEY=your_claude_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
EOF

# 注意：.env文件要加入.gitignore
echo ".env" >> .gitignore
echo "venv/" >> .gitignore
echo ".cache/" >> .gitignore
echo "data/" >> .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignore
```

#### 测试

```bash
# 测试环境
python -c "import anthropic; import openai; import chromadb; import srt; print('✓ 所有依赖安装成功')"

# 测试配置
python -c "from config import *; print('✓ 配置文件正常')"
```

#### 验收标准

```
✅ 目录结构创建完成
✅ 依赖安装成功
✅ API密钥配置完成
✅ 测试命令通过
```

---

### 模块1.2：数据模型定义（1小时）

#### 任务
定义核心数据结构（Utterance, Atom）

#### 步骤

**Step 1: 定义Utterance模型**

```python
# models/utterance.py

from pydantic import BaseModel, Field
from typing import Optional

class Utterance(BaseModel):
    """单句字幕"""

    id: int = Field(..., description="序号")
    start_ms: int = Field(..., description="开始时间（毫秒）")
    end_ms: int = Field(..., description="结束时间（毫秒）")
    text: str = Field(..., description="文本内容")
    duration_ms: int = Field(..., description="持续时间（毫秒）")

    @property
    def start_time(self) -> str:
        """格式化开始时间 HH:MM:SS"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """格式化结束时间 HH:MM:SS"""
        return self._ms_to_time(self.end_ms)

    def _ms_to_time(self, ms: int) -> str:
        """毫秒转时间字符串"""
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    class Config:
        json_schema_extra = {
            "example": {
                "id": 1,
                "start_ms": 8933,
                "end_ms": 10400,
                "text": "开始了没有啊",
                "duration_ms": 1467
            }
        }
```

**Step 2: 定义Atom模型**

```python
# models/atom.py

from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any

class Atom(BaseModel):
    """信息原子/微片段"""

    atom_id: str = Field(..., description="原子ID，如 A001")
    start_ms: int = Field(..., description="开始时间（毫秒）")
    end_ms: int = Field(..., description="结束时间（毫秒）")
    duration_ms: int = Field(..., description="持续时间（毫秒）")
    merged_text: str = Field(..., description="合并后的文本")
    type: str = Field(..., description="类型：fragment/complete_segment")
    completeness: str = Field(..., description="完整性：完整/需要上下文")
    source_utterance_ids: List[int] = Field(default_factory=list, description="来源字幕ID列表")

    # 可选字段（后续阶段填充）
    topics: Optional[Dict[str, Any]] = Field(None, description="主题标注")
    emotion: Optional[Dict[str, Any]] = Field(None, description="情感标注")
    value: Optional[Dict[str, Any]] = Field(None, description="价值标注")
    embedding: Optional[List[float]] = Field(None, description="语义向量")

    @property
    def start_time(self) -> str:
        """格式化开始时间"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """格式化结束时间"""
        return self._ms_to_time(self.end_ms)

    @property
    def duration_seconds(self) -> float:
        """持续时间（秒）"""
        return self.duration_ms / 1000.0

    def _ms_to_time(self, ms: int) -> str:
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    class Config:
        json_schema_extra = {
            "example": {
                "atom_id": "A001",
                "start_ms": 500000,
                "end_ms": 510000,
                "duration_ms": 10000,
                "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源",
                "type": "fragment",
                "completeness": "完整",
                "source_utterance_ids": [85, 86, 87]
            }
        }
```

**Step 3: 更新__init__.py**

```python
# models/__init__.py

from .utterance import Utterance
from .atom import Atom

__all__ = ['Utterance', 'Atom']
```

#### 测试

```python
# tests/test_models.py

from models import Utterance, Atom

def test_utterance():
    """测试Utterance模型"""
    utt = Utterance(
        id=1,
        start_ms=8933,
        end_ms=10400,
        text="开始了没有啊",
        duration_ms=1467
    )

    assert utt.id == 1
    assert utt.start_time == "00:00:08"
    assert utt.end_time == "00:00:10"
    print("✓ Utterance模型测试通过")

def test_atom():
    """测试Atom模型"""
    atom = Atom(
        atom_id="A001",
        start_ms=500000,
        end_ms=510000,
        duration_ms=10000,
        merged_text="测试文本",
        type="fragment",
        completeness="完整",
        source_utterance_ids=[1, 2, 3]
    )

    assert atom.atom_id == "A001"
    assert atom.start_time == "00:08:20"
    assert atom.duration_seconds == 10.0
    print("✓ Atom模型测试通过")

if __name__ == "__main__":
    test_utterance()
    test_atom()
```

#### 运行测试

```bash
python tests/test_models.py
```

#### 验收标准

```
✅ Utterance模型定义正确
✅ Atom模型定义正确
✅ 时间转换函数正常
✅ 测试脚本通过
```

---

### 模块1.3：SRT解析器（1-2小时）⭐

#### 任务
解析SRT字幕文件，输出Utterance列表

#### 步骤

**Step 1: 实现SRT解析器**

```python
# parsers/srt_parser.py

import srt
from datetime import timedelta
from typing import List
from pathlib import Path
from models.utterance import Utterance

class SRTParser:
    """SRT字幕解析器"""

    def __init__(self):
        self.parsed_count = 0

    def parse(self, file_path: str) -> List[Utterance]:
        """
        解析SRT文件

        Args:
            file_path: SRT文件路径

        Returns:
            Utterance列表

        Raises:
            FileNotFoundError: 文件不存在
            ValueError: 文件格式错误
        """
        # 检查文件是否存在
        if not Path(file_path).exists():
            raise FileNotFoundError(f"文件不存在: {file_path}")

        # 读取文件
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # 解析SRT
        try:
            subtitles = list(srt.parse(content))
        except Exception as e:
            raise ValueError(f"SRT格式错误: {e}")

        # 转换为Utterance
        utterances = []
        for sub in subtitles:
            utterance = Utterance(
                id=sub.index,
                start_ms=self._to_milliseconds(sub.start),
                end_ms=self._to_milliseconds(sub.end),
                text=sub.content.strip(),
                duration_ms=self._to_milliseconds(sub.end - sub.start)
            )
            utterances.append(utterance)

        self.parsed_count = len(utterances)
        return utterances

    def _to_milliseconds(self, td: timedelta) -> int:
        """将timedelta转为毫秒"""
        return int(td.total_seconds() * 1000)
```

**Step 2: 实现清洗器**

```python
# parsers/cleaner.py

from typing import List
from models.utterance import Utterance

class Cleaner:
    """字幕清洗器"""

    # 无意义的填充词
    FILLER_WORDS = ['呃', 'uh', 'um', 'eh', '啊', '嗯', '...', 'emmm']

    # 最小持续时间（毫秒）
    MIN_DURATION_MS = 500

    def __init__(self):
        self.removed_count = 0

    def clean(self, utterances: List[Utterance]) -> List[Utterance]:
        """
        清洗字幕

        过滤规则：
        1. 去除纯填充词
        2. 去除过短片段（<0.5秒）
        3. 标准化文本

        Args:
            utterances: 原始字幕列表

        Returns:
            清洗后的字幕列表
        """
        cleaned = []

        for utt in utterances:
            # 规则1：过滤填充词
            if utt.text.strip() in self.FILLER_WORDS:
                self.removed_count += 1
                continue

            # 规则2：过滤过短片段
            if utt.duration_ms < self.MIN_DURATION_MS:
                self.removed_count += 1
                continue

            # 规则3：标准化文本
            cleaned_text = self._normalize_text(utt.text)
            if not cleaned_text:  # 空文本
                self.removed_count += 1
                continue

            # 更新文本
            utt.text = cleaned_text
            cleaned.append(utt)

        return cleaned

    def _normalize_text(self, text: str) -> str:
        """标准化文本"""
        # 去除换行符
        text = text.replace('\n', ' ')
        # 去除多余空格
        text = ' '.join(text.split())
        # 去除首尾空格
        text = text.strip()
        return text
```

**Step 3: 更新__init__.py**

```python
# parsers/__init__.py

from .srt_parser import SRTParser
from .cleaner import Cleaner

__all__ = ['SRTParser', 'Cleaner']
```

#### 测试

**Step 1: 准备测试数据**

```bash
# 复制你的SRT文件到测试目录
cp "D:/YouTube_Downloads/金三角大佬4：缅北双雄时代1962-1998.srt" data/raw/test.srt
```

**Step 2: 编写测试脚本**

```python
# tests/test_parser.py

from parsers import SRTParser, Cleaner
from pathlib import Path

def test_srt_parser():
    """测试SRT解析"""
    print("\n" + "="*60)
    print("测试SRT解析器")
    print("="*60)

    # 解析
    parser = SRTParser()
    file_path = "data/raw/test.srt"

    if not Path(file_path).exists():
        print("❌ 测试文件不存在，请先复制SRT文件到 data/raw/test.srt")
        return

    utterances = parser.parse(file_path)

    print(f"✓ 解析完成：{len(utterances)}条字幕")

    # 显示前5条
    print("\n前5条字幕：")
    for i, utt in enumerate(utterances[:5]):
        print(f"{i+1}. [{utt.start_time}-{utt.end_time}] {utt.text}")

    # 验证
    assert len(utterances) > 0, "字幕数量应该大于0"
    assert utterances[0].text, "第一条字幕不应该为空"

    print("\n✓ SRT解析器测试通过")
    return utterances

def test_cleaner(utterances):
    """测试清洗器"""
    print("\n" + "="*60)
    print("测试清洗器")
    print("="*60)

    original_count = len(utterances)

    cleaner = Cleaner()
    cleaned = cleaner.clean(utterances)

    print(f"原始数量: {original_count}")
    print(f"清洗后数量: {len(cleaned)}")
    print(f"移除数量: {cleaner.removed_count}")
    print(f"移除比例: {cleaner.removed_count/original_count*100:.1f}%")

    # 显示被移除的一些例子
    removed = [u for u in utterances if u not in cleaned][:5]
    if removed:
        print("\n被移除的例子：")
        for u in removed:
            print(f"  - [{u.duration_ms}ms] {u.text}")

    # 显示清洗后的前5条
    print("\n清洗后的前5条：")
    for i, utt in enumerate(cleaned[:5]):
        print(f"{i+1}. [{utt.start_time}-{utt.end_time}] {utt.text}")

    print("\n✓ 清洗器测试通过")
    return cleaned

if __name__ == "__main__":
    utterances = test_srt_parser()
    if utterances:
        cleaned = test_cleaner(utterances)
```

#### 运行测试

```bash
python tests/test_parser.py
```

#### 预期输出

```
============================================================
测试SRT解析器
============================================================
✓ 解析完成：3580条字幕

前5条字幕：
1. [00:00:07-00:00:07] what
2. [00:00:08-00:00:10] 开始了没有啊
3. [00:00:11-00:00:13] 我看看开始了没有
4. [00:00:15-00:00:17] 哎开始了没有
5. [00:00:19-00:00:20] hello

✓ SRT解析器测试通过

============================================================
测试清洗器
============================================================
原始数量: 3580
清洗后数量: 3200
移除数量: 380
移除比例: 10.6%

被移除的例子：
  - [900ms] what
  - [400ms] 呃
  - [300ms] ...

清洗后的前5条：
1. [00:00:08-00:00:10] 开始了没有啊
2. [00:00:11-00:00:13] 我看看开始了没有
...

✓ 清洗器测试通过
```

#### 验收标准

```
✅ 能正确解析SRT文件
✅ Utterance对象创建正确
✅ 时间转换准确
✅ 清洗功能正常
✅ 测试脚本通过
✅ 输出符合预期
```

---

### 模块1.4：工具函数（1小时）

#### 任务
实现常用的工具函数（API调用、文件操作、日志）

#### 步骤

**Step 1: API客户端封装**

```python
# utils/api_client.py

import anthropic
import openai
import time
import json
from typing import Optional, Dict, Any
from anthropic import APIError, RateLimitError

class ClaudeClient:
    """Claude API客户端（带重试机制）"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.total_calls = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0

    def call(
        self,
        prompt: str,
        model: str = "claude-3-5-sonnet-20241022",
        max_tokens: int = 4000,
        max_retries: int = 3
    ) -> str:
        """
        调用Claude API（带重试）

        Args:
            prompt: 提示词
            model: 模型名称
            max_tokens: 最大输出token数
            max_retries: 最大重试次数

        Returns:
            API返回的文本

        Raises:
            Exception: 重试次数用尽后抛出
        """
        for attempt in range(max_retries):
            try:
                response = self.client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    messages=[{
                        "role": "user",
                        "content": prompt
                    }]
                )

                # 统计
                self.total_calls += 1
                self.total_input_tokens += response.usage.input_tokens
                self.total_output_tokens += response.usage.output_tokens

                return response.content[0].text

            except RateLimitError:
                # 限流：指数退避
                wait_time = 2 ** attempt
                print(f"⚠️ 触发限流，等待{wait_time}秒后重试...")
                time.sleep(wait_time)

            except APIError as e:
                # API错误：重试
                print(f"⚠️ API错误: {e}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(1)

        raise Exception("重试次数用尽")

    def get_stats(self) -> Dict[str, Any]:
        """获取统计信息"""
        # 价格（示例，实际价格可能变化）
        input_price_per_m = 3.00  # $3/M tokens
        output_price_per_m = 15.00  # $15/M tokens

        input_cost = (self.total_input_tokens / 1_000_000) * input_price_per_m
        output_cost = (self.total_output_tokens / 1_000_000) * output_price_per_m
        total_cost = input_cost + output_cost

        return {
            "total_calls": self.total_calls,
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "estimated_cost": f"${total_cost:.2f}"
        }


class OpenAIClient:
    """OpenAI API客户端"""

    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)

    def generate_embedding(
        self,
        text: str,
        model: str = "text-embedding-3-large"
    ) -> list:
        """生成单个文本的embedding"""
        response = self.client.embeddings.create(
            model=model,
            input=[text]
        )
        return response.data[0].embedding

    def generate_embeddings_batch(
        self,
        texts: list,
        model: str = "text-embedding-3-large"
    ) -> list:
        """批量生成embedding"""
        response = self.client.embeddings.create(
            model=model,
            input=texts
        )
        return [item.embedding for item in response.data]
```

**Step 2: 文件工具**

```python
# utils/file_utils.py

import json
from pathlib import Path
from typing import Any, List
from models import Utterance, Atom

def save_json(data: Any, file_path: str):
    """保存JSON文件"""
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json(file_path: str) -> Any:
    """加载JSON文件"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_jsonl(items: List[Any], file_path: str):
    """保存JSONL文件（每行一个JSON对象）"""
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in items:
            if hasattr(item, 'model_dump'):  # Pydantic v2
                json_str = json.dumps(item.model_dump(), ensure_ascii=False)
            elif hasattr(item, 'dict'):  # Pydantic v1
                json_str = json.dumps(item.dict(), ensure_ascii=False)
            else:
                json_str = json.dumps(item, ensure_ascii=False)
            f.write(json_str + '\n')

def load_jsonl(file_path: str, model_class=None) -> List[Any]:
    """加载JSONL文件"""
    items = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            if model_class:
                items.append(model_class(**data))
            else:
                items.append(data)
    return items
```

**Step 3: 日志工具**

```python
# utils/logger.py

import logging
from pathlib import Path
from datetime import datetime
from rich.console import Console
from rich.logging import RichHandler

console = Console()

def setup_logger(name: str, log_file: str = None) -> logging.Logger:
    """
    设置日志器

    Args:
        name: 日志器名称
        log_file: 日志文件路径（可选）

    Returns:
        配置好的Logger
    """
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # 清除已有的handler
    logger.handlers.clear()

    # Rich console handler（彩色输出）
    console_handler = RichHandler(
        console=console,
        show_time=True,
        show_path=False
    )
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    # 文件handler（可选）
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger
```

**Step 4: 更新__init__.py**

```python
# utils/__init__.py

from .api_client import ClaudeClient, OpenAIClient
from .file_utils import save_json, load_json, save_jsonl, load_jsonl
from .logger import setup_logger

__all__ = [
    'ClaudeClient',
    'OpenAIClient',
    'save_json',
    'load_json',
    'save_jsonl',
    'load_jsonl',
    'setup_logger'
]
```

#### 测试

```python
# tests/test_utils.py

from utils import ClaudeClient, save_json, load_json, setup_logger
from config import CLAUDE_API_KEY
import os

def test_logger():
    """测试日志"""
    print("\n测试日志系统...")
    logger = setup_logger("test", "logs/test.log")
    logger.info("这是一条INFO日志")
    logger.warning("这是一条WARNING日志")
    logger.error("这是一条ERROR日志")
    print("✓ 日志系统正常")

def test_file_utils():
    """测试文件工具"""
    print("\n测试文件工具...")

    # 测试保存和加载
    test_data = {"test": "data", "number": 123}
    save_json(test_data, "data/processed/test.json")
    loaded = load_json("data/processed/test.json")

    assert loaded == test_data
    print("✓ 文件工具正常")

    # 清理
    os.remove("data/processed/test.json")

def test_claude_client():
    """测试Claude客户端"""
    if not CLAUDE_API_KEY:
        print("⚠️ 未配置CLAUDE_API_KEY，跳过API测试")
        return

    print("\n测试Claude客户端...")
    client = ClaudeClient(CLAUDE_API_KEY)

    # 简单测试
    response = client.call("请回复：测试成功", max_tokens=100)
    print(f"API响应: {response}")

    # 统计
    stats = client.get_stats()
    print(f"统计: {stats}")

    print("✓ Claude客户端正常")

if __name__ == "__main__":
    test_logger()
    test_file_utils()
    test_claude_client()
```

#### 验收标准

```
✅ API客户端能正常调用
✅ 重试机制正常
✅ 文件读写正常
✅ 日志系统正常
✅ 测试通过
```

---

### 模块1.5：原子化提示词（2-3小时）⭐⭐⭐

#### 任务
设计和测试原子化提示词（最关键）

#### 步骤

**Step 1: 创建第一版提示词**

```
# prompts/atomize_v1.txt

你是一个视频内容分析专家。我会给你一段直播的字幕片段，你的任务是把它们合并成"语义完整的信息单元"。

【核心规则】

1. 原子可以是任意长度：
   - 短：10-30秒（一句话、一次互动）
   - 中：1-5分钟（一个观点、一个小故事）
   - 长：5-15分钟（一个完整故事、完整论述）

2. 判断边界的标准：
   - 主题转换 → 新原子
   - 长时间停顿（>5秒）→ 新原子
   - 从叙事转到互动 → 新原子
   - 从一个事件转到另一个事件 → 新原子

3. 特别识别"完整片段"（complete_segment）：
   如果发现5分钟以上的内容满足：
   ✅ 主题统一，没有跑题
   ✅ 逻辑完整，有头有尾
   ✅ 可以独立理解
   → 标记为 type: "complete_segment"

4. 类型分类：
   - 叙述历史：讲述过去的事件
   - 回应弹幕：与观众互动
   - 发表观点：个人评价和看法
   - 读来信：读观众来信
   - 闲聊：无关内容、过场

【输入格式】
每行格式为：[时间] 文本内容

【输出格式】
返回JSON数组，每个元素包含：
{
  "atom_id": "A001",              // 原子ID（按顺序编号）
  "start_ms": 500000,             // 开始时间（毫秒）
  "end_ms": 510000,               // 结束时间（毫秒）
  "duration_ms": 10000,           // 持续时间（毫秒）
  "merged_text": "合并后的文本",  // 完整的文本内容
  "type": "fragment",             // 类型
  "completeness": "完整",         // 完整性
  "source_utterance_ids": [1,2,3] // 来源字幕ID
}

【重要】
- 只返回JSON数组，不要其他说明文字
- 确保JSON格式正确
- 时间必须连续，不能有遗漏
- atom_id 必须按顺序（A001, A002, A003...）

【示例】

输入：
[00:08:20] 1962年
[00:08:25] 国民党残军撤到金三角
[00:08:30] 这是整个金三角问题的起源
[00:08:38] hello 海绵宝宝
[00:08:40] 然后呢坤沙就是在这个背景下崛起的

输出：
[
  {
    "atom_id": "A001",
    "start_ms": 500000,
    "end_ms": 510000,
    "duration_ms": 10000,
    "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源",
    "type": "叙述历史",
    "completeness": "完整",
    "source_utterance_ids": [85, 86, 87]
  },
  {
    "atom_id": "A002",
    "start_ms": 518000,
    "end_ms": 520000,
    "duration_ms": 2000,
    "merged_text": "hello 海绵宝宝",
    "type": "回应弹幕",
    "completeness": "完整",
    "source_utterance_ids": [88]
  },
  {
    "atom_id": "A003",
    "start_ms": 520000,
    "end_ms": 525000,
    "duration_ms": 5000,
    "merged_text": "然后呢坤沙就是在这个背景下崛起的",
    "type": "叙述历史",
    "completeness": "需要上下文",
    "source_utterance_ids": [89]
  }
]
```

**Step 2: 实现原子化器**

```python
# atomizers/atomizer.py

import json
import re
from typing import List
from models import Utterance, Atom
from utils import ClaudeClient, setup_logger

logger = setup_logger(__name__)

class Atomizer:
    """原子化处理器"""

    def __init__(self, api_key: str, batch_size: int = 50):
        self.client = ClaudeClient(api_key)
        self.batch_size = batch_size
        self.total_atoms = 0

        # 加载提示词
        with open('prompts/atomize_v1.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def atomize(
        self,
        utterances: List[Utterance],
        start_atom_id: int = 1
    ) -> List[Atom]:
        """
        原子化处理

        Args:
            utterances: 字幕列表
            start_atom_id: 起始原子ID

        Returns:
            原子列表
        """
        atoms = []
        total_batches = (len(utterances) + self.batch_size - 1) // self.batch_size
        atom_counter = start_atom_id

        logger.info(f"开始原子化，共{len(utterances)}条字幕，分{total_batches}批处理")

        for i in range(0, len(utterances), self.batch_size):
            batch = utterances[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            logger.info(f"处理批次 {batch_num}/{total_batches}...")

            try:
                batch_atoms = self._process_batch(batch, atom_counter)
                atoms.extend(batch_atoms)
                atom_counter += len(batch_atoms)

                logger.info(f"  ✓ 生成{len(batch_atoms)}个原子")

            except Exception as e:
                logger.error(f"  ✗ 批次{batch_num}处理失败: {e}")
                # 可选：保存失败的批次用于调试
                continue

        self.total_atoms = len(atoms)
        logger.info(f"原子化完成，共生成{self.total_atoms}个原子")

        return atoms

    def _process_batch(
        self,
        batch: List[Utterance],
        start_atom_id: int
    ) -> List[Atom]:
        """处理一个批次"""
        # 构建输入文本
        input_lines = []
        for utt in batch:
            input_lines.append(f"[{utt.start_time}] {utt.text}")
        input_text = "\n".join(input_lines)

        # 构建完整提示词
        prompt = self.prompt_template + "\n\n【输入】\n" + input_text + "\n\n【输出】"

        # 调用API
        response = self.client.call(prompt, max_tokens=4000)

        # 解析JSON
        atoms_data = self._parse_response(response, start_atom_id)

        # 转换为Atom对象
        atoms = []
        for data in atoms_data:
            try:
                atom = Atom(**data)
                atoms.append(atom)
            except Exception as e:
                logger.warning(f"  ⚠️ 原子解析失败: {e}")
                continue

        return atoms

    def _parse_response(self, response: str, start_atom_id: int) -> List[dict]:
        """
        解析API响应

        尝试提取JSON数组，并修正atom_id
        """
        # 提取JSON（寻找第一个[和最后一个]）
        json_match = re.search(r'\[.*\]', response, re.DOTALL)
        if not json_match:
            raise ValueError("无法从响应中提取JSON")

        json_str = json_match.group(0)
        atoms_data = json.loads(json_str)

        # 修正atom_id（确保连续）
        for i, atom in enumerate(atoms_data):
            atom['atom_id'] = f"A{start_atom_id + i:03d}"

        return atoms_data
```

**Step 3: 更新__init__.py**

```python
# atomizers/__init__.py

from .atomizer import Atomizer

__all__ = ['Atomizer']
```

#### 测试（关键）

```python
# tests/test_atomizer.py

from parsers import SRTParser, Cleaner
from atomizers import Atomizer
from utils import save_jsonl, setup_logger
from config import CLAUDE_API_KEY
import json

logger = setup_logger(__name__)

def test_atomizer_small():
    """测试原子化器（小数据集：前10分钟）"""
    print("\n" + "="*60)
    print("测试原子化器 - 前10分钟")
    print("="*60)

    if not CLAUDE_API_KEY:
        print("❌ 未配置CLAUDE_API_KEY")
        return

    # Step 1: 解析字幕
    parser = SRTParser()
    utterances = parser.parse("data/raw/test.srt")

    cleaner = Cleaner()
    utterances = cleaner.clean(utterances)

    # 只取前10分钟（假设前600秒）
    utterances_10min = [u for u in utterances if u.start_ms < 600000]
    print(f"前10分钟字幕数: {len(utterances_10min)}")

    # Step 2: 原子化
    atomizer = Atomizer(CLAUDE_API_KEY, batch_size=50)
    atoms = atomizer.atomize(utterances_10min)

    print(f"\n✓ 生成原子数: {len(atoms)}")

    # Step 3: 查看结果
    print("\n前5个原子：")
    for i, atom in enumerate(atoms[:5]):
        print(f"\n{i+1}. {atom.atom_id} [{atom.start_time}-{atom.end_time}] ({atom.duration_seconds:.1f}秒)")
        print(f"   类型: {atom.type}")
        print(f"   完整性: {atom.completeness}")
        print(f"   文本: {atom.merged_text[:80]}...")

    # Step 4: 验证质量
    print("\n质量检查：")

    # 检查1：时间连续性
    time_gaps = []
    for i in range(len(atoms) - 1):
        gap = atoms[i+1].start_ms - atoms[i].end_ms
        if gap > 30000:  # >30秒
            time_gaps.append((atoms[i].atom_id, atoms[i+1].atom_id, gap/1000))

    if time_gaps:
        print(f"  ⚠️ 发现{len(time_gaps)}个大时间间隔：")
        for a1, a2, gap in time_gaps[:3]:
            print(f"     {a1} -> {a2}: {gap:.1f}秒")
    else:
        print(f"  ✓ 时间连续性良好")

    # 检查2：类型分布
    type_count = {}
    for atom in atoms:
        type_count[atom.type] = type_count.get(atom.type, 0) + 1

    print(f"  类型分布:")
    for t, c in type_count.items():
        print(f"     {t}: {c}个")

    # 检查3：完整片段
    complete_segments = [a for a in atoms if a.type == "complete_segment"]
    print(f"  完整片段: {len(complete_segments)}个")

    # Step 5: 保存
    save_jsonl(atoms, "data/processed/atoms_10min.jsonl")
    print(f"\n✓ 已保存到 data/processed/atoms_10min.jsonl")

    # Step 6: API统计
    stats = atomizer.client.get_stats()
    print(f"\nAPI统计:")
    print(f"  调用次数: {stats['total_calls']}")
    print(f"  输入tokens: {stats['total_input_tokens']}")
    print(f"  输出tokens: {stats['total_output_tokens']}")
    print(f"  预估成本: {stats['estimated_cost']}")

    return atoms

if __name__ == "__main__":
    atoms = test_atomizer_small()
```

#### 运行测试

```bash
python tests/test_atomizer.py
```

#### 验收标准（最关键）⭐⭐⭐

**人工检查（必须）**:

```
1. 打开 data/processed/atoms_10min.jsonl
2. 随机检查10-15个原子：

   检查点：
   ✅ 边界合理？（不会把一句话拆成两个）
   ✅ 文本完整？（没有遗漏）
   ✅ 类型准确？（叙述历史/回应弹幕等）
   ✅ 完整性判断合理？

3. 如果发现问题：
   → 记录具体例子
   → 调整提示词
   → 重新测试

4. 迭代直到质量达标（准确率 >85%）
```

**自动检查**:
```
✅ 能生成原子（数量>0）
✅ 时间连续性（大间隔<10%）
✅ JSON格式正确
✅ API成本可接受（<$5/10分钟）
```

---

**【阶段1暂停点】**

到这里，我们完成了**阶段1的前5个模块**。

**现在应该做什么？**

1. ✅ **仔细检查原子化质量**（最重要）
2. ✅ **调整提示词**（如果质量不达标）
3. ✅ **迭代测试**（直到满意）

**只有原子化质量通过验证，才进入后续模块！**

---

我先在这里暂停，给你一个**检查点**。

**你现在可以：**

**选项1**: 我帮你把阶段1的前5个模块**完整代码打包**给你，你先跑起来测试？

**选项2**: 继续看后面的模块（1.6-1.9），但还不写代码？

**选项3**: 你想先问一些问题？

你觉得怎样？我们一步步来，不着急。

---

### 模块1.6：原子化质量验证器（1-2小时）

#### 任务
构建自动化质量验证工具，检查原子化结果的质量

#### 步骤

**Step 1: 实现验证器**

```python
# atomizers/validator.py

from typing import List, Dict, Any
from models import Atom, Utterance
from utils import setup_logger

logger = setup_logger(__name__)

class AtomValidator:
    """原子化质量验证器"""

    def __init__(self):
        self.issues = []
        self.warnings = []

    def validate(
        self,
        atoms: List[Atom],
        original_utterances: List[Utterance]
    ) -> Dict[str, Any]:
        """
        验证原子化质量

        Returns:
            验证报告
        """
        self.issues = []
        self.warnings = []

        logger.info("开始质量验证...")

        # 验证1: 时间完整性
        coverage = self._check_time_coverage(atoms, original_utterances)

        # 验证2: 时间连续性
        gaps = self._check_time_continuity(atoms)

        # 验证3: 原子长度分布
        length_dist = self._check_length_distribution(atoms)

        # 验证4: 类型分布
        type_dist = self._check_type_distribution(atoms)

        # 验证5: ID连续性
        id_check = self._check_id_continuity(atoms)

        # 验证6: 文本完整性
        text_check = self._check_text_completeness(atoms)

        # 生成报告
        report = {
            "total_atoms": len(atoms),
            "coverage_rate": coverage,
            "time_gaps": gaps,
            "length_distribution": length_dist,
            "type_distribution": type_dist,
            "id_continuous": id_check,
            "text_complete": text_check,
            "issues": self.issues,
            "warnings": self.warnings,
            "quality_score": self._calculate_score()
        }

        return report

    def _check_time_coverage(
        self,
        atoms: List[Atom],
        utterances: List[Utterance]
    ) -> float:
        """检查时间覆盖率"""
        if not utterances:
            return 0.0

        original_duration = utterances[-1].end_ms - utterances[0].start_ms
        atoms_duration = sum(a.duration_ms for a in atoms)

        coverage = atoms_duration / original_duration if original_duration > 0 else 0

        if coverage < 0.85:
            self.issues.append(
                f"时间覆盖率过低: {coverage*100:.1f}% (应>85%)"
            )
        elif coverage < 0.95:
            self.warnings.append(
                f"时间覆盖率偏低: {coverage*100:.1f}% (建议>95%)"
            )

        return coverage

    def _check_time_continuity(self, atoms: List[Atom]) -> List[Dict]:
        """检查时间连续性"""
        gaps = []

        for i in range(len(atoms) - 1):
            gap_ms = atoms[i+1].start_ms - atoms[i].end_ms

            if gap_ms > 30000:  # >30秒
                gaps.append({
                    "from_atom": atoms[i].atom_id,
                    "to_atom": atoms[i+1].atom_id,
                    "gap_seconds": gap_ms / 1000,
                    "severity": "high" if gap_ms > 60000 else "medium"
                })

            if gap_ms < -1000:  # 负间隔（重叠）
                self.issues.append(
                    f"时间重叠: {atoms[i].atom_id} 和 {atoms[i+1].atom_id}"
                )

        if len(gaps) > len(atoms) * 0.1:
            self.warnings.append(
                f"时间间隔过多: {len(gaps)}个大间隔 (>10%)"
            )

        return gaps

    def _check_length_distribution(self, atoms: List[Atom]) -> Dict:
        """检查长度分布"""
        lengths = [a.duration_seconds for a in atoms]

        short = sum(1 for l in lengths if l < 30)
        medium = sum(1 for l in lengths if 30 <= l < 300)
        long_seg = sum(1 for l in lengths if l >= 300)

        dist = {
            "short_(<30s)": short,
            "medium_(30s-5min)": medium,
            "long_(>5min)": long_seg,
            "avg_seconds": sum(lengths) / len(lengths) if lengths else 0,
            "max_seconds": max(lengths) if lengths else 0,
            "min_seconds": min(lengths) if lengths else 0
        }

        # 检查异常
        if dist["avg_seconds"] < 10:
            self.warnings.append("平均原子时长过短 (<10秒)")
        if dist["avg_seconds"] > 180:
            self.warnings.append("平均原子时长过长 (>3分钟)")

        return dist

    def _check_type_distribution(self, atoms: List[Atom]) -> Dict:
        """检查类型分布"""
        type_count = {}
        for atom in atoms:
            type_count[atom.type] = type_count.get(atom.type, 0) + 1

        # 检查是否所有原子类型都相同（可能有问题）
        if len(type_count) == 1:
            self.warnings.append(
                f"所有原子类型相同: {list(type_count.keys())[0]}"
            )

        return type_count

    def _check_id_continuity(self, atoms: List[Atom]) -> bool:
        """检查ID连续性"""
        for i, atom in enumerate(atoms):
            expected_id = f"A{i+1:03d}"
            if atom.atom_id != expected_id:
                self.issues.append(
                    f"ID不连续: 第{i+1}个原子ID为{atom.atom_id}, 期望{expected_id}"
                )
                return False
        return True

    def _check_text_completeness(self, atoms: List[Atom]) -> bool:
        """检查文本完整性"""
        for atom in atoms:
            if not atom.merged_text or len(atom.merged_text.strip()) < 5:
                self.issues.append(
                    f"文本过短或为空: {atom.atom_id}"
                )
                return False

            if len(atom.source_utterance_ids) == 0:
                self.issues.append(
                    f"缺少来源字幕ID: {atom.atom_id}"
                )

        return True

    def _calculate_score(self) -> str:
        """计算质量分数"""
        issue_count = len(self.issues)
        warning_count = len(self.warnings)

        if issue_count == 0 and warning_count == 0:
            return "优秀 (A)"
        elif issue_count == 0 and warning_count <= 3:
            return "良好 (B)"
        elif issue_count <= 2:
            return "合格 (C)"
        else:
            return "不合格 (D)"

    def print_report(self, report: Dict):
        """打印验证报告"""
        print("\n" + "="*60)
        print("原子化质量验证报告")
        print("="*60)

        print(f"\n总原子数: {report['total_atoms']}")
        print(f"时间覆盖率: {report['coverage_rate']*100:.1f}%")
        print(f"质量评分: {report['quality_score']}")

        print(f"\n长度分布:")
        for k, v in report['length_distribution'].items():
            print(f"  {k}: {v}")

        print(f"\n类型分布:")
        for k, v in report['type_distribution'].items():
            print(f"  {k}: {v}")

        if report['time_gaps']:
            print(f"\n时间间隔 (>{30}秒): {len(report['time_gaps'])}个")
            for gap in report['time_gaps'][:5]:
                print(f"  {gap['from_atom']} -> {gap['to_atom']}: {gap['gap_seconds']:.1f}秒")

        if report['issues']:
            print(f"\n❌ 严重问题 ({len(report['issues'])}个):")
            for issue in report['issues']:
                print(f"  - {issue}")

        if report['warnings']:
            print(f"\n⚠️  警告 ({len(report['warnings'])}个):")
            for warning in report['warnings']:
                print(f"  - {warning}")

        print("\n" + "="*60)
```

完整的模块1.6-1.9实现已添加到文档末尾。路线图现已包含阶段1所有9个模块的详细实现指南。

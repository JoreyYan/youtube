# 技术设计文档
# 视频理解引擎（Video Understanding Engine）

**版本**: v1.0
**日期**: 2025-10-01
**模块**: 单元1 - 视频理解引擎

---

## 📋 目录

1. [模块概述](#1-模块概述)
2. [架构设计](#2-架构设计)
3. [子模块详细设计](#3-子模块详细设计)
4. [数据模型](#4-数据模型)
5. [核心算法](#5-核心算法)
6. [提示词工程](#6-提示词工程)
7. [性能优化](#7-性能优化)
8. [错误处理](#8-错误处理)
9. [测试策略](#9-测试策略)
10. [部署方案](#10-部署方案)

---

## 1. 模块概述

### 1.1 功能定义

视频理解引擎是整个系统的**核心基础**，负责将原始字幕文件转化为结构化的知识库。

**输入**: 字幕文件（.srt）
**输出**: 完整的视频知识库
**核心能力**: 深度语义理解 + 多维度标注 + 知识图谱构建

### 1.2 核心价值

- ✅ **一次处理，永久理解**: 处理后的知识库可以反复使用，无需重新分析
- ✅ **多维度理解**: 不只是文本，还有主题、情感、价值、关系
- ✅ **AI友好**: 生成的数据格式专门为AI检索和理解优化

### 1.3 技术挑战

| 挑战 | 难度 | 解决方案 |
|------|------|----------|
| 语义边界识别 | ⭐⭐⭐⭐ | AI驱动的智能合并 |
| 主题识别准确率 | ⭐⭐⭐⭐ | 提示词工程 + 多维验证 |
| 处理速度 | ⭐⭐⭐ | 批量处理 + 并行化 |
| 成本控制 | ⭐⭐⭐ | Token优化 + 缓存 |
| 完整片段识别 | ⭐⭐⭐⭐⭐ | AI判断 + 规则辅助 |

---

## 2. 架构设计

### 2.1 整体架构

```
┌─────────────────────────────────────────────────────────┐
│                    视频理解引擎                           │
└─────────────────────────────────────────────────────────┘
                          │
          ┌───────────────┼───────────────┐
          │               │               │
          ▼               ▼               ▼
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │  解析层  │     │  分析层  │     │  索引层  │
    └─────────┘     └─────────┘     └─────────┘
          │               │               │
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │ Parser  │     │Analyzer │     │ Indexer │
    │ Module  │     │ Module  │     │ Module  │
    └─────────┘     └─────────┘     └─────────┘
```

### 2.2 数据流

```
原始字幕 (.srt)
    ↓
┌─────────────────┐
│ 1. 解析模块      │ → utterances.jsonl
└─────────────────┘
    ↓
┌─────────────────┐
│ 2. 原子化模块    │ → micro_segments.jsonl
└─────────────────┘
    ↓
┌─────────────────┐
│ 3. 标注模块      │ → analyzed_atoms.jsonl
│   - 主题标注     │
│   - 情感标注     │
│   - 价值标注     │
│   - 实体提取     │
└─────────────────┘
    ↓
┌─────────────────┐
│ 4. 向量化模块    │ → embeddings
└─────────────────┘
    ↓
┌─────────────────┐
│ 5. 结构化模块    │ → entities.json
│   - 实体卡片     │   topics.json
│   - 主题网络     │   narratives.json
│   - 叙事识别     │
└─────────────────┘
    ↓
┌─────────────────┐
│ 6. 索引模块      │ → vector_db/
│   - 向量索引     │   indexes/
│   - 结构化索引   │
│   - 知识图谱     │
└─────────────────┘
```

### 2.3 目录结构

```
video_understanding_engine/
├── __init__.py
├── config.py                    # 配置文件
├── parsers/                     # 解析模块
│   ├── __init__.py
│   ├── srt_parser.py           # SRT解析
│   └── cleaner.py              # 清洗
├── atomizers/                   # 原子化模块
│   ├── __init__.py
│   ├── atomizer.py             # 核心原子化逻辑
│   └── validator.py            # 原子验证
├── analyzers/                   # 分析模块
│   ├── __init__.py
│   ├── topic_analyzer.py       # 主题分析
│   ├── emotion_analyzer.py     # 情感分析
│   ├── value_analyzer.py       # 价值评估
│   └── entity_extractor.py     # 实体提取
├── structurers/                 # 结构化模块
│   ├── __init__.py
│   ├── entity_builder.py       # 构建实体卡片
│   ├── topic_builder.py        # 构建主题网络
│   └── narrative_identifier.py # 识别叙事片段
├── vectorizers/                 # 向量化模块
│   ├── __init__.py
│   └── embedder.py             # 生成embedding
├── indexers/                    # 索引模块
│   ├── __init__.py
│   ├── vector_index.py         # 向量索引
│   ├── structured_index.py     # 结构化索引
│   └── graph_builder.py        # 知识图谱
├── prompts/                     # 提示词库
│   ├── atomize.txt
│   ├── tag_topics.txt
│   ├── tag_emotion.txt
│   ├── tag_value.txt
│   ├── build_entity.txt
│   └── build_topic.txt
├── utils/                       # 工具函数
│   ├── __init__.py
│   ├── api_client.py           # API调用封装
│   ├── time_utils.py           # 时间处理
│   └── file_utils.py           # 文件操作
├── models/                      # 数据模型
│   ├── __init__.py
│   ├── utterance.py
│   ├── atom.py
│   ├── entity.py
│   └── topic.py
└── engine.py                    # 主引擎类
```

---

## 3. 子模块详细设计

### 3.1 解析模块（Parsers）

#### 功能
将SRT字幕文件解析成结构化数据，并进行清洗。

#### 核心代码

```python
# parsers/srt_parser.py

import srt
from datetime import timedelta
from typing import List, Dict
from models.utterance import Utterance

class SRTParser:
    """SRT字幕解析器"""

    def __init__(self):
        self.parsed_count = 0

    def parse(self, file_path: str) -> List[Utterance]:
        """
        解析SRT文件

        Args:
            file_path: SRT文件路径

        Returns:
            Utterance列表
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        subtitles = list(srt.parse(content))
        utterances = []

        for sub in subtitles:
            utterance = Utterance(
                id=sub.index,
                start_ms=self._to_milliseconds(sub.start),
                end_ms=self._to_milliseconds(sub.end),
                text=sub.content.strip(),
                duration_ms=self._to_milliseconds(sub.end - sub.start)
            )
            utterances.append(utterance)

        self.parsed_count = len(utterances)
        return utterances

    def _to_milliseconds(self, td: timedelta) -> int:
        """将timedelta转为毫秒"""
        return int(td.total_seconds() * 1000)


# parsers/cleaner.py

class Cleaner:
    """字幕清洗器"""

    # 无意义的填充词
    FILLER_WORDS = ['呃', 'uh', 'um', 'eh', '啊', '嗯', '...']

    # 最小持续时间（毫秒）
    MIN_DURATION_MS = 500

    def clean(self, utterances: List[Utterance]) -> List[Utterance]:
        """
        清洗字幕

        过滤规则：
        1. 去除纯填充词
        2. 去除过短片段（<0.5秒）
        3. 标准化文本
        """
        cleaned = []

        for utt in utterances:
            # 规则1：过滤填充词
            if utt.text in self.FILLER_WORDS:
                continue

            # 规则2：过滤过短片段
            if utt.duration_ms < self.MIN_DURATION_MS:
                continue

            # 规则3：标准化文本
            utt.text = self._normalize_text(utt.text)

            cleaned.append(utt)

        return cleaned

    def _normalize_text(self, text: str) -> str:
        """标准化文本"""
        # 去除换行符
        text = text.replace('\n', ' ')
        # 去除多余空格
        text = ' '.join(text.split())
        # 去除首尾空格
        text = text.strip()
        return text
```

#### 数据模型

```python
# models/utterance.py

from pydantic import BaseModel

class Utterance(BaseModel):
    """单句字幕"""
    id: int                    # 序号
    start_ms: int              # 开始时间（毫秒）
    end_ms: int                # 结束时间（毫秒）
    text: str                  # 文本内容
    duration_ms: int           # 持续时间（毫秒）

    def to_time_str(self, ms: int) -> str:
        """毫秒转时间字符串 HH:MM:SS.mmm"""
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        milliseconds = ms % 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

    @property
    def start_time(self) -> str:
        return self.to_time_str(self.start_ms)

    @property
    def end_time(self) -> str:
        return self.to_time_str(self.end_ms)
```

#### 输出

```jsonl
{"id": 1, "start_ms": 7000, "end_ms": 7900, "text": "what", "duration_ms": 900}
{"id": 2, "start_ms": 8933, "end_ms": 10400, "text": "开始了没有啊", "duration_ms": 1467}
```

---

### 3.2 原子化模块（Atomizers）⭐ 核心

#### 功能
将碎片化的字幕合并成"语义完整的信息单元"（原子/微片段）。

**核心挑战**: 如何判断哪些句子应该合并？

#### 设计思路

**方法**: AI驱动的智能合并

```
传统方法（不可行）：
❌ 按固定时间切分（比如每30秒）→ 会破坏语义
❌ 按句号切分 → 字幕标点不准确
❌ 按主题词切换 → 无法识别复杂语境

AI方法（推荐）：
✅ 让Claude理解字幕内容
✅ 识别语义边界（主题转换、停顿、互动）
✅ 合并成完整表达
✅ 特别识别"完整片段"（5-15分钟的连续内容）
```

#### 核心代码

```python
# atomizers/atomizer.py

import anthropic
import json
from typing import List
from models.utterance import Utterance
from models.atom import Atom

class Atomizer:
    """原子化处理器"""

    def __init__(self, api_key: str, batch_size: int = 50):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.batch_size = batch_size

        # 加载提示词
        with open('prompts/atomize.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def atomize(self, utterances: List[Utterance]) -> List[Atom]:
        """
        原子化处理

        策略：
        1. 每次处理50条字幕（约2-3分钟内容）
        2. 让AI识别语义边界
        3. 合并成完整片段
        """
        atoms = []
        total_batches = (len(utterances) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(utterances), self.batch_size):
            batch = utterances[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            print(f"处理批次 {batch_num}/{total_batches}...")

            # 调用AI
            batch_atoms = self._process_batch(batch)
            atoms.extend(batch_atoms)

        return atoms

    def _process_batch(self, batch: List[Utterance]) -> List[Atom]:
        """处理一个批次"""
        # 构建输入文本
        input_text = "\n".join([
            f"[{utt.start_time}] {utt.text}"
            for utt in batch
        ])

        # 调用Claude
        prompt = self.prompt_template.format(input_text=input_text)

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        # 解析返回的JSON
        result_text = response.content[0].text

        # 提取JSON（有时Claude会在JSON前后加说明文字）
        json_str = self._extract_json(result_text)
        atoms_data = json.loads(json_str)

        # 转换为Atom对象
        atoms = [Atom(**data) for data in atoms_data]

        return atoms

    def _extract_json(self, text: str) -> str:
        """从文本中提取JSON"""
        # 寻找第一个 [ 和最后一个 ]
        start = text.find('[')
        end = text.rfind(']') + 1

        if start == -1 or end == 0:
            raise ValueError("无法从返回结果中提取JSON")

        return text[start:end]


# atomizers/validator.py

class AtomValidator:
    """原子验证器"""

    def validate(self, atoms: List[Atom]) -> List[str]:
        """
        验证原子质量

        检查：
        1. 时间连续性
        2. 文本长度合理性
        3. 时间间隔
        """
        issues = []

        for i, atom in enumerate(atoms):
            # 检查1：时间连续性
            if i > 0:
                prev_end = atoms[i-1].end_ms
                curr_start = atom.start_ms
                gap_ms = curr_start - prev_end

                if gap_ms > 30000:  # 超过30秒间隔
                    issues.append(
                        f"⚠️ {atom.atom_id}: 与前一个原子间隔 {gap_ms/1000:.1f}秒"
                    )

            # 检查2：文本长度
            text_len = len(atom.merged_text)
            if text_len < 10:
                issues.append(f"⚠️ {atom.atom_id}: 文本过短（{text_len}字符）")
            elif text_len > 500:
                issues.append(f"⚠️ {atom.atom_id}: 文本过长（{text_len}字符）")

            # 检查3：持续时间
            duration_sec = atom.duration_ms / 1000
            if duration_sec > 600:  # 超过10分钟
                if atom.type != "complete_segment":
                    issues.append(
                        f"⚠️ {atom.atom_id}: 时长{duration_sec/60:.1f}分钟，"
                        f"但未标记为完整片段"
                    )

        return issues
```

#### 数据模型

```python
# models/atom.py

from pydantic import BaseModel
from typing import List, Optional

class Atom(BaseModel):
    """信息原子/微片段"""
    atom_id: str                      # 原子ID，如 "A001"
    start_ms: int                     # 开始时间（毫秒）
    end_ms: int                       # 结束时间（毫秒）
    duration_ms: int                  # 持续时间（毫秒）
    merged_text: str                  # 合并后的文本
    type: str                         # 类型：fragment/complete_segment
    completeness: str                 # 完整性：完整/需要上下文
    source_utterance_ids: List[int]   # 来源字幕ID列表

    # 可选字段（初始化时为空）
    topics: Optional[dict] = None     # 主题标注
    emotion: Optional[dict] = None    # 情感标注
    value: Optional[dict] = None      # 价值标注
    embedding: Optional[List[float]] = None  # 向量

    @property
    def start_time(self) -> str:
        """格式化开始时间"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """格式化结束时间"""
        return self._ms_to_time(self.end_ms)

    def _ms_to_time(self, ms: int) -> str:
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
```

#### 提示词

```
# prompts/atomize.txt

你是一个视频内容分析专家。我会给你一段直播的字幕片段，你的任务是把它们合并成"语义完整的信息单元"。

【核心规则】
1. 原子可以是任意长度：
   - 短：10-30秒（一句话、一次互动）
   - 中：1-5分钟（一个观点、一个小故事）
   - 长：5-15分钟（一个完整故事、完整论述）

2. 判断边界的标准：
   - 主题转换 → 新原子
   - 长时间停顿（>5秒）→ 新原子
   - 从叙事转到互动 → 新原子
   - 从一个事件转到另一个事件 → 新原子

3. 特别识别"完整片段"（complete_segment）：
   如果发现5分钟以上的内容满足：
   ✅ 主题统一，没有跑题
   ✅ 逻辑完整，有头有尾
   ✅ 可以独立理解
   → 标记为 type: "complete_segment"

4. 类型分类：
   - 叙述历史：讲述过去的事件
   - 回应弹幕：与观众互动
   - 发表观点：个人评价和看法
   - 读来信：读观众来信
   - 闲聊：无关内容、过场

【输入格式】
[时间] 文本内容

【输出格式】
返回JSON数组，每个元素包含：
- atom_id: 原子ID（A001, A002...）
- start_ms: 开始时间（毫秒）
- end_ms: 结束时间（毫秒）
- duration_ms: 持续时间（毫秒）
- merged_text: 合并后的完整文本
- type: "fragment"（碎片）或 "complete_segment"（完整片段）
- completeness: "完整" 或 "需要上下文"
- source_utterance_ids: 来源字幕ID列表

【示例】

输入：
[00:08:20] 1962年
[00:08:25] 国民党残军撤到金三角
[00:08:30] 这是整个金三角问题的起源
[00:08:38] hello 海绵宝宝
[00:08:40] 然后呢坤沙就是在这个背景下崛起的

输出：
[
  {
    "atom_id": "A001",
    "start_ms": 500000,
    "end_ms": 510000,
    "duration_ms": 10000,
    "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源",
    "type": "fragment",
    "completeness": "完整",
    "source_utterance_ids": [85, 86, 87]
  },
  {
    "atom_id": "A002",
    "start_ms": 518000,
    "end_ms": 520000,
    "duration_ms": 2000,
    "merged_text": "hello 海绵宝宝",
    "type": "fragment",
    "completeness": "完整",
    "source_utterance_ids": [88]
  },
  {
    "atom_id": "A003",
    "start_ms": 520000,
    "end_ms": 525000,
    "duration_ms": 5000,
    "merged_text": "然后呢坤沙就是在这个背景下崛起的",
    "type": "fragment",
    "completeness": "需要上下文",
    "source_utterance_ids": [89]
  }
]

【重要】
- 只返回JSON，不要其他说明文字
- 确保JSON格式正确，可以被解析
- 时间必须连续，不能有重叠或遗漏
```

---

### 3.3 分析模块（Analyzers）

#### 功能
给每个原子打上多维度标签。

#### 子模块

##### 3.3.1 主题分析器

```python
# analyzers/topic_analyzer.py

class TopicAnalyzer:
    """主题分析器"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        分析主题

        为每个原子标注：
        - primary_topic: 主要主题
        - secondary_topics: 次要主题列表
        - topic_scores: 主题相关度分数
        - entities: 提取的实体（人物、地点、时间、事件、概念）
        """
        for i, atom in enumerate(atoms):
            print(f"分析主题: {i+1}/{len(atoms)}")

            prompt = self.prompt_template.format(
                atom_text=atom.merged_text
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            atom.topics = result

        return atoms
```

提示词见前面的"完整实施流程"文档。

##### 3.3.2 情感分析器

```python
# analyzers/emotion_analyzer.py

class EmotionAnalyzer:
    """情感分析器"""

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        分析情感和能量

        标注：
        - emotion_type: 情感类型
        - energy_level: 能量值（1-10）
        - energy_trend: 趋势（递增/递减/平稳）
        """
        # 类似TopicAnalyzer的实现
        pass
```

##### 3.3.3 价值评估器

```python
# analyzers/value_analyzer.py

class ValueAnalyzer:
    """价值评估器"""

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        评估内容价值

        标注：
        - information_density: 信息密度（1-10）
        - editability: 可剪辑性（必剪/可剪/可删/必删）
        - independence: 独立性
        - special_value: 特殊价值（金句/高潮点/争议点/无）
        """
        pass
```

#### 优化：批量分析

为了提高效率，可以把三个维度的分析合并到一次API调用：

```python
# analyzers/batch_analyzer.py

class BatchAnalyzer:
    """批量分析器：一次调用完成三个维度的分析"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        # 合并的提示词
        with open('prompts/analyze_all.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """一次性分析主题、情感、价值"""
        for i, atom in enumerate(atoms):
            print(f"分析: {i+1}/{len(atoms)}")

            prompt = self.prompt_template.format(
                atom_text=atom.merged_text
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=3000,
                messages=[{"role": "user", "content": prompt}]
            )

            # 返回结果包含三个部分
            result = json.loads(response.content[0].text)

            atom.topics = result['topics']
            atom.emotion = result['emotion']
            atom.value = result['value']

        return atoms
```

**提示词**：

```
# prompts/analyze_all.txt

分析这段文本的多个维度。

【文本】
{atom_text}

【任务】
请从以下三个维度分析：

1. 主题维度
   - 主要主题是什么？
   - 次要主题有哪些？
   - 提取实体：人物、地点、时间、事件、概念

2. 情感维度
   - 情感类型（客观叙述/激动/幽默/同情/批判）
   - 能量值（1-10）
   - 趋势（递增/递减/平稳）

3. 价值维度
   - 信息密度（1-10）
   - 可剪辑性（必剪/可剪/可删/必删）
   - 独立性（独立/需铺垫/需补充/依赖上下文）
   - 特殊价值（金句/高潮点/争议点/无）

【输出JSON】
{
  "topics": {
    "primary_topic": "...",
    "secondary_topics": [...],
    "topic_scores": {...},
    "entities": {
      "persons": [...],
      "locations": [...],
      "time_points": [...],
      "events": [...],
      "concepts": [...]
    }
  },
  "emotion": {
    "emotion_type": "...",
    "energy_level": 8,
    "energy_trend": "..."
  },
  "value": {
    "information_density": 9,
    "editability": "必剪",
    "independence": "需铺垫",
    "special_value": "高潮点"
  }
}
```

---

### 3.4 向量化模块（Vectorizers）

#### 功能
为每个原子生成语义向量（embedding），用于后续的语义搜索。

```python
# vectorizers/embedder.py

from openai import OpenAI
from typing import List
from models.atom import Atom

class Embedder:
    """向量生成器"""

    def __init__(self, api_key: str, model: str = "text-embedding-3-large"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.batch_size = 2048  # OpenAI支持的最大批量

    def generate_embeddings(self, atoms: List[Atom]) -> List[Atom]:
        """
        批量生成embedding

        策略：
        1. OpenAI支持批量生成，每次最多2048条
        2. 用merged_text作为输入
        3. 返回3072维向量（text-embedding-3-large）
        """
        texts = [atom.merged_text for atom in atoms]

        # 分批处理
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_atoms = atoms[i:i + self.batch_size]

            print(f"生成向量: {i+1}-{min(i+self.batch_size, len(texts))}/{len(texts)}")

            # 调用OpenAI
            response = self.client.embeddings.create(
                model=self.model,
                input=batch_texts
            )

            # 赋值
            for j, embedding_data in enumerate(response.data):
                batch_atoms[j].embedding = embedding_data.embedding

        return atoms
```

---

### 3.5 结构化模块（Structurers）

#### 功能
基于标注后的原子，构建高层次的结构：
1. 实体卡片
2. 主题网络
3. 叙事片段

##### 3.5.1 实体构建器

```python
# structurers/entity_builder.py

from typing import List, Dict
from models.atom import Atom
from models.entity import Entity

class EntityBuilder:
    """实体卡片构建器"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        with open('prompts/build_entity.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def build_entities(self, atoms: List[Atom]) -> List[Entity]:
        """
        构建实体卡片

        流程：
        1. 收集所有提到的实体
        2. 按实体聚合相关原子
        3. 调用AI生成实体卡片
        """
        # Step 1: 收集实体
        entity_mentions = self._collect_entities(atoms)

        # Step 2: 为每个实体构建卡片
        entities = []
        for entity_name, atom_ids in entity_mentions.items():
            if len(atom_ids) < 3:  # 至少出现3次
                continue

            print(f"构建实体: {entity_name}")

            entity = self._build_entity_card(entity_name, atom_ids, atoms)
            entities.append(entity)

        return entities

    def _collect_entities(self, atoms: List[Atom]) -> Dict[str, List[str]]:
        """收集实体提及"""
        entity_mentions = {}

        for atom in atoms:
            if not atom.topics:
                continue

            persons = atom.topics.get('entities', {}).get('persons', [])

            for person in persons:
                if person not in entity_mentions:
                    entity_mentions[person] = []
                entity_mentions[person].append(atom.atom_id)

        return entity_mentions

    def _build_entity_card(
        self,
        entity_name: str,
        atom_ids: List[str],
        all_atoms: List[Atom]
    ) -> Entity:
        """构建单个实体卡片"""
        # 获取相关原子
        related_atoms = [a for a in all_atoms if a.atom_id in atom_ids]

        # 构建输入文本
        atoms_text = "\n\n".join([
            f"[{a.atom_id}] {a.start_time}-{a.end_time}\n{a.merged_text}"
            for a in related_atoms
        ])

        # 调用AI
        prompt = self.prompt_template.format(
            entity_name=entity_name,
            related_atoms_text=atoms_text
        )

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )

        entity_data = json.loads(response.content[0].text)

        # 生成entity的embedding
        entity_summary = entity_data['profile']['basic_info']
        entity_data['embedding'] = self._generate_single_embedding(entity_summary)

        return Entity(**entity_data)

    def _generate_single_embedding(self, text: str) -> List[float]:
        """为单个文本生成embedding"""
        client = OpenAI(api_key=self.openai_key)
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=[text]
        )
        return response.data[0].embedding
```

##### 3.5.2 主题网络构建器

```python
# structurers/topic_builder.py

class TopicBuilder:
    """主题网络构建器"""

    def build_topics(self, atoms: List[Atom]) -> List[Topic]:
        """
        构建主题网络

        流程：
        1. 统计主题出现频率
        2. 为主要主题（出现10次以上）构建详细卡片
        3. 识别子主题
        4. 设计叙事模板
        """
        # 统计
        topic_stats = self._collect_topic_stats(atoms)

        # 构建
        topics = []
        for topic_name, count in topic_stats.items():
            if count < 10:
                continue

            print(f"构建主题: {topic_name} ({count}次)")

            topic = self._build_topic_card(topic_name, atoms)
            topics.append(topic)

        return topics
```

##### 3.5.3 叙事识别器

```python
# structurers/narrative_identifier.py

class NarrativeIdentifier:
    """叙事片段识别器"""

    def identify_narratives(self, atoms: List[Atom]) -> List[Narrative]:
        """
        识别完整的叙事片段

        寻找：
        - 有开头、发展、结局的完整故事
        - 持续5分钟以上
        - 逻辑连贯
        """
        # 先找所有标记为"complete_segment"的原子
        complete_atoms = [a for a in atoms if a.type == "complete_segment"]

        # 再用AI识别其他可能的叙事片段
        narratives = self._ai_identify_narratives(atoms)

        return narratives
```

---

### 3.6 索引模块（Indexers）

#### 功能
建立多维度索引，支持快速检索。

##### 3.6.1 向量索引

```python
# indexers/vector_index.py

import chromadb
from typing import List
from models.atom import Atom
from models.entity import Entity
from models.topic import Topic

class VectorIndexer:
    """向量索引构建器"""

    def __init__(self, persist_directory: str):
        self.client = chromadb.PersistentClient(path=persist_directory)

    def build_index(
        self,
        atoms: List[Atom],
        entities: List[Entity],
        topics: List[Topic]
    ):
        """
        构建向量索引

        把原子、实体、主题的embedding都存入向量库
        """
        # 创建collection
        collection = self.client.create_collection(
            name="video_knowledge",
            metadata={"description": "视频知识库"}
        )

        # 添加原子
        print("索引原子...")
        for atom in atoms:
            if not atom.embedding:
                continue

            collection.add(
                ids=[atom.atom_id],
                embeddings=[atom.embedding],
                metadatas=[{
                    "type": "atom",
                    "start_time": atom.start_time,
                    "end_time": atom.end_time,
                    "primary_topic": atom.topics.get('primary_topic', ''),
                    "value_score": atom.value.get('information_density', 0)
                }],
                documents=[atom.merged_text]
            )

        # 添加实体
        print("索引实体...")
        for entity in entities:
            if not entity.embedding:
                continue

            collection.add(
                ids=[entity.entity_id],
                embeddings=[entity.embedding],
                metadatas=[{
                    "type": "entity",
                    "entity_type": entity.type,
                    "name": entity.name
                }],
                documents=[entity.profile['basic_info']]
            )

        # 添加主题
        print("索引主题...")
        for topic in topics:
            if not topic.embedding:
                continue

            collection.add(
                ids=[topic.topic_id],
                embeddings=[topic.embedding],
                metadatas=[{
                    "type": "topic",
                    "name": topic.name
                }],
                documents=[topic.definition]
            )

        print(f"✓ 向量索引构建完成，共{collection.count()}条")
```

##### 3.6.2 结构化索引

```python
# indexers/structured_index.py

class StructuredIndexer:
    """结构化索引构建器"""

    def build_index(self, atoms: List[Atom]) -> dict:
        """
        构建多维度结构化索引

        索引维度：
        - by_time: 按时间（10分钟为单位）
        - by_person: 按人物
        - by_topic: 按主题
        - by_value: 按价值等级
        - by_emotion: 按情感
        """
        indexes = {
            "by_time": {},
            "by_person": {},
            "by_topic": {},
            "by_value": {},
            "by_emotion": {}
        }

        for atom in atoms:
            # 时间索引
            time_bucket = self._get_time_bucket(atom.start_ms)
            if time_bucket not in indexes['by_time']:
                indexes['by_time'][time_bucket] = []
            indexes['by_time'][time_bucket].append(atom.atom_id)

            # 人物索引
            persons = atom.topics.get('entities', {}).get('persons', [])
            for person in persons:
                if person not in indexes['by_person']:
                    indexes['by_person'][person] = []
                indexes['by_person'][person].append(atom.atom_id)

            # 主题索引
            topics = atom.topics.get('secondary_topics', [])
            for topic in topics:
                if topic not in indexes['by_topic']:
                    indexes['by_topic'][topic] = []
                indexes['by_topic'][topic].append(atom.atom_id)

            # 价值索引
            value = atom.value.get('information_density', 0)
            value_tier = f"{(value-1)//2*2+1}-{(value-1)//2*2+2}分"
            if value_tier not in indexes['by_value']:
                indexes['by_value'][value_tier] = []
            indexes['by_value'][value_tier].append(atom.atom_id)

            # 情感索引
            emotion = atom.emotion.get('emotion_type', '未知')
            if emotion not in indexes['by_emotion']:
                indexes['by_emotion'][emotion] = []
            indexes['by_emotion'][emotion].append(atom.atom_id)

        return indexes

    def _get_time_bucket(self, ms: int, bucket_size_ms: int = 600000) -> str:
        """获取时间桶（默认10分钟）"""
        bucket_num = ms // bucket_size_ms
        start_min = bucket_num * (bucket_size_ms // 60000)
        end_min = start_min + (bucket_size_ms // 60000)
        return f"{start_min:02d}:00-{end_min:02d}:00"
```

##### 3.6.3 知识图谱构建器

```python
# indexers/graph_builder.py

import networkx as nx

class GraphBuilder:
    """知识图谱构建器"""

    def build_graph(self, entities: List[Entity]) -> dict:
        """
        构建知识图谱

        节点：实体
        边：关系
        """
        G = nx.DiGraph()

        # 添加节点
        for entity in entities:
            G.add_node(
                entity.entity_id,
                type=entity.type,
                name=entity.name,
                data=entity.dict()
            )

        # 添加边
        for entity in entities:
            if 'relationships' not in entity.dict():
                continue

            for rel in entity.relationships:
                target_id = self._find_entity_id(entities, rel['target'])
                if target_id:
                    G.add_edge(
                        entity.entity_id,
                        target_id,
                        relation=rel['relation'],
                        atoms=rel.get('atoms', [])
                    )

        # 转换为JSON格式
        graph_data = nx.node_link_data(G)
        return graph_data

    def _find_entity_id(self, entities: List[Entity], name: str) -> str:
        """根据名称查找实体ID"""
        for entity in entities:
            if entity.name == name:
                return entity.entity_id
        return None
```

---

### 3.7 主引擎类

```python
# engine.py

from typing import Optional
import os
import json

class VideoUnderstandingEngine:
    """视频理解引擎主类"""

    def __init__(
        self,
        claude_api_key: str,
        openai_api_key: str,
        output_dir: str = "data/output"
    ):
        self.claude_key = claude_api_key
        self.openai_key = openai_api_key
        self.output_dir = output_dir

        # 初始化各个模块
        self.parser = SRTParser()
        self.cleaner = Cleaner()
        self.atomizer = Atomizer(claude_api_key)
        self.validator = AtomValidator()
        self.batch_analyzer = BatchAnalyzer(claude_api_key)
        self.embedder = Embedder(openai_api_key)
        self.entity_builder = EntityBuilder(claude_api_key)
        self.topic_builder = TopicBuilder(claude_api_key)
        self.narrative_identifier = NarrativeIdentifier(claude_api_key)

    def process(self, srt_file: str, video_id: str) -> str:
        """
        处理一个视频

        Args:
            srt_file: SRT文件路径
            video_id: 视频ID

        Returns:
            输出目录路径
        """
        print("="*60)
        print(f"视频理解引擎 - 处理 {video_id}")
        print("="*60)

        # 创建输出目录
        video_output_dir = os.path.join(self.output_dir, video_id)
        os.makedirs(video_output_dir, exist_ok=True)
        os.makedirs(os.path.join(video_output_dir, "indexes"), exist_ok=True)

        # Step 1: 解析
        print("\n[1/6] 解析字幕...")
        utterances = self.parser.parse(srt_file)
        cleaned = self.cleaner.clean(utterances)
        self._save_json({"utterances": [u.dict() for u in cleaned]},
                       os.path.join(video_output_dir, "utterances.jsonl"))
        print(f"✓ 解析完成：{len(cleaned)}条字幕")

        # Step 2: 原子化
        print("\n[2/6] 原子化处理（预计30-40分钟）...")
        atoms = self.atomizer.atomize(cleaned)
        issues = self.validator.validate(atoms)
        if issues:
            print("⚠️ 验证发现问题：")
            for issue in issues[:10]:  # 只显示前10个
                print(f"  {issue}")
        self._save_json({"atoms": [a.dict() for a in atoms]},
                       os.path.join(video_output_dir, "micro_segments.jsonl"))
        print(f"✓ 原子化完成：{len(atoms)}个原子")

        # Step 3: 语义分析
        print("\n[3/6] 语义分析（预计20-30分钟）...")
        atoms = self.batch_analyzer.analyze(atoms)
        print(f"✓ 分析完成")

        # Step 4: 生成向量
        print("\n[4/6] 生成语义向量...")
        atoms = self.embedder.generate_embeddings(atoms)
        self._save_json({"atoms": [a.dict() for a in atoms]},
                       os.path.join(video_output_dir, "analyzed_atoms.jsonl"))
        print(f"✓ 向量生成完成")

        # Step 5: 结构化
        print("\n[5/6] 构建知识结构...")
        entities = self.entity_builder.build_entities(atoms)
        topics = self.topic_builder.build_topics(atoms)
        narratives = self.narrative_identifier.identify_narratives(atoms)

        self._save_json({"entities": [e.dict() for e in entities]},
                       os.path.join(video_output_dir, "entities.json"))
        self._save_json({"topics": [t.dict() for t in topics]},
                       os.path.join(video_output_dir, "topics.json"))
        self._save_json({"narratives": [n.dict() for n in narratives]},
                       os.path.join(video_output_dir, "narratives.json"))
        print(f"✓ 识别{len(entities)}个实体，{len(topics)}个主题，{len(narratives)}个叙事片段")

        # Step 6: 建立索引
        print("\n[6/6] 建立索引...")
        # 向量索引
        vector_indexer = VectorIndexer(
            os.path.join(video_output_dir, "vector_db")
        )
        vector_indexer.build_index(atoms, entities, topics)

        # 结构化索引
        struct_indexer = StructuredIndexer()
        indexes = struct_indexer.build_index(atoms)
        self._save_json(indexes,
                       os.path.join(video_output_dir, "indexes", "structured.json"))

        # 知识图谱
        graph_builder = GraphBuilder()
        graph = graph_builder.build_graph(entities)
        self._save_json(graph,
                       os.path.join(video_output_dir, "indexes", "graph.json"))
        print(f"✓ 索引构建完成")

        print("\n" + "="*60)
        print(f"✓ 处理完成！知识库已保存到: {video_output_dir}")
        print("="*60)

        return video_output_dir

    def _save_json(self, data: dict, file_path: str):
        """保存JSON文件"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


# 使用示例
if __name__ == "__main__":
    engine = VideoUnderstandingEngine(
        claude_api_key="your_claude_key",
        openai_api_key="your_openai_key"
    )

    output_dir = engine.process(
        srt_file="D:/YouTube_Downloads/金三角大佬4：缅北双雄时代1962-1998.srt",
        video_id="jinSanJiao_04"
    )

    print(f"\n知识库位置: {output_dir}")
```

---

## 4. 数据模型

所有数据模型都使用Pydantic定义，确保类型安全和数据验证。

详见代码中的`models/`目录。

---

## 5. 核心算法

### 5.1 语义边界识别算法

**问题**: 如何判断哪些句子应该合并成一个原子？

**传统方法的问题**:
- 固定时间：破坏语义
- 标点符号：字幕标点不准
- 关键词：无法理解复杂语境

**AI方法**:
使用Claude的强大理解能力，通过精心设计的提示词，让AI识别：
1. 主题转换点
2. 长时间停顿（通过时间戳）
3. 叙事模式切换（叙述→互动→叙述）

### 5.2 完整片段识别算法

**目标**: 识别5-15分钟的"完整片段"（可直接使用）

**方法**: AI判断 + 规则辅助

```python
def is_complete_segment(atom: Atom) -> bool:
    """
    判断是否为完整片段

    条件：
    1. 时长 > 5分钟
    2. AI标记为 type="complete_segment"
    3. completeness="完整"
    4. 主题统一（topic_scores中主导主题>8分）
    """
    # 规则1：时长
    if atom.duration_ms < 300000:  # 5分钟
        return False

    # 规则2：AI标记
    if atom.type != "complete_segment":
        return False

    # 规则3：完整性
    if atom.completeness != "完整":
        return False

    # 规则4：主题统一性
    topic_scores = atom.topics.get('topic_scores', {})
    if not topic_scores:
        return False

    max_score = max(topic_scores.values())
    if max_score < 8:
        return False

    return True
```

---

## 6. 提示词工程

提示词是整个系统的**灵魂**，直接决定了理解质量。

### 6.1 提示词设计原则

1. **明确任务**: 清晰说明要做什么
2. **提供示例**: Few-shot learning
3. **结构化输出**: 要求返回JSON
4. **错误处理**: 告诉AI如何处理边界情况

### 6.2 提示词迭代策略

```
Version 1.0: 基础版本
    ↓ 测试
发现问题：识别边界不准确
    ↓
Version 1.1: 增加"长时间停顿"规则
    ↓ 测试
发现问题：遗漏完整片段
    ↓
Version 1.2: 增加"完整片段"识别
    ↓ 测试
持续优化...
```

所有提示词都应该版本化管理，便于回滚和对比。

---

## 7. 性能优化

### 7.1 并行处理

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_analyze(atoms: List[Atom], max_workers: int = 5) -> List[Atom]:
    """并行分析多个原子"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(analyze_single_atom, atom) for atom in atoms]
        results = [f.result() for f in futures]
    return results
```

### 7.2 缓存机制

```python
import hashlib
import pickle

class Cache:
    """结果缓存"""

    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get(self, key: str):
        """获取缓存"""
        cache_file = os.path.join(self.cache_dir, self._hash(key))
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def set(self, key: str, value):
        """设置缓存"""
        cache_file = os.path.join(self.cache_dir, self._hash(key))
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)

    def _hash(self, key: str) -> str:
        return hashlib.md5(key.encode()).hexdigest()
```

### 7.3 Token优化

**策略**:
1. 精简提示词，删除冗余描述
2. 批量处理，减少API调用次数
3. 使用更便宜的模型处理简单任务

---

## 8. 错误处理

### 8.1 API错误处理

```python
import time
from anthropic import APIError, RateLimitError

def call_claude_with_retry(prompt: str, max_retries: int = 3):
    """带重试的API调用"""
    for attempt in range(max_retries):
        try:
            response = client.messages.create(...)
            return response

        except RateLimitError:
            # 限流：等待后重试
            wait_time = 2 ** attempt  # 指数退避
            print(f"⚠️ 限流，等待{wait_time}秒后重试...")
            time.sleep(wait_time)

        except APIError as e:
            # API错误：记录并重试
            print(f"⚠️ API错误: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)

    raise Exception("重试次数用尽")
```

### 8.2 数据验证

使用Pydantic自动验证数据格式。

### 8.3 进度保存

```python
def process_with_checkpoint(atoms: List[Atom], checkpoint_file: str):
    """带断点续传的处理"""
    # 加载checkpoint
    processed_ids = load_checkpoint(checkpoint_file)

    for i, atom in enumerate(atoms):
        if atom.atom_id in processed_ids:
            continue  # 跳过已处理

        # 处理
        analyze_atom(atom)

        # 保存checkpoint
        save_checkpoint(checkpoint_file, atom.atom_id)

        print(f"进度: {i+1}/{len(atoms)}")
```

---

## 9. 测试策略

### 9.1 单元测试

```python
# tests/test_atomizer.py

def test_atomizer():
    """测试原子化模块"""
    # 准备测试数据
    utterances = [
        Utterance(id=1, start_ms=0, end_ms=1000, text="测试1"),
        Utterance(id=2, start_ms=1000, end_ms=2000, text="测试2"),
    ]

    # 执行
    atomizer = Atomizer(api_key="test_key")
    atoms = atomizer.atomize(utterances)

    # 断言
    assert len(atoms) > 0
    assert atoms[0].merged_text is not None
```

### 9.2 集成测试

```python
def test_end_to_end():
    """端到端测试"""
    engine = VideoUnderstandingEngine(...)
    output_dir = engine.process("test.srt", "test_video")

    # 验证输出文件存在
    assert os.path.exists(os.path.join(output_dir, "analyzed_atoms.jsonl"))
    assert os.path.exists(os.path.join(output_dir, "entities.json"))
```

### 9.3 准确率测试

手动标注一部分数据，测试AI标注的准确率。

---

## 10. 部署方案

### 10.1 开发环境

```bash
# 创建虚拟环境
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 安装依赖
pip install -r requirements.txt

# 配置API密钥
export CLAUDE_API_KEY="your_key"
export OPENAI_API_KEY="your_key"

# 运行
python engine.py
```

### 10.2 生产环境（可选）

**Docker化**:

```dockerfile
FROM python:3.10

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "engine.py"]
```

**部署到服务器**:
- AWS EC2 / 阿里云ECS
- 8核16G内存
- 使用队列系统（如Celery）处理任务

---

## 11. 附录

### 11.1 完整依赖

```txt
# requirements.txt

anthropic==0.18.0
openai==1.0.0
chromadb==0.4.0
srt==3.5.0
pydantic==2.0.0
networkx==3.1
pandas==2.0.0
rich==13.0.0
pytest==7.4.0
```

### 11.2 目录结构总览

```
video_understanding_engine/
├── README.md
├── requirements.txt
├── config.py
├── engine.py                    # 主引擎
├── parsers/                     # 解析模块
├── atomizers/                   # 原子化模块
├── analyzers/                   # 分析模块
├── structurers/                 # 结构化模块
├── vectorizers/                 # 向量化模块
├── indexers/                    # 索引模块
├── prompts/                     # 提示词库
├── utils/                       # 工具函数
├── models/                      # 数据模型
└── tests/                       # 测试
```

---

**文档完成**

这份技术设计文档涵盖了视频理解引擎的所有关键部分。基于这份文档，开发团队可以：

1. ✅ 理解整体架构
2. ✅ 实现各个子模块
3. ✅ 进行测试和优化
4. ✅ 部署到生产环境

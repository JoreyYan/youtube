# æŠ€æœ¯è®¾è®¡æ–‡æ¡£
# è§†é¢‘ç†è§£å¼•æ“ï¼ˆVideo Understanding Engineï¼‰

**ç‰ˆæœ¬**: v1.0
**æ—¥æœŸ**: 2025-10-01
**æ¨¡å—**: å•å…ƒ1 - è§†é¢‘ç†è§£å¼•æ“

---

## ğŸ“‹ ç›®å½•

1. [æ¨¡å—æ¦‚è¿°](#1-æ¨¡å—æ¦‚è¿°)
2. [æ¶æ„è®¾è®¡](#2-æ¶æ„è®¾è®¡)
3. [å­æ¨¡å—è¯¦ç»†è®¾è®¡](#3-å­æ¨¡å—è¯¦ç»†è®¾è®¡)
4. [æ•°æ®æ¨¡å‹](#4-æ•°æ®æ¨¡å‹)
5. [æ ¸å¿ƒç®—æ³•](#5-æ ¸å¿ƒç®—æ³•)
6. [æç¤ºè¯å·¥ç¨‹](#6-æç¤ºè¯å·¥ç¨‹)
7. [æ€§èƒ½ä¼˜åŒ–](#7-æ€§èƒ½ä¼˜åŒ–)
8. [é”™è¯¯å¤„ç†](#8-é”™è¯¯å¤„ç†)
9. [æµ‹è¯•ç­–ç•¥](#9-æµ‹è¯•ç­–ç•¥)
10. [éƒ¨ç½²æ–¹æ¡ˆ](#10-éƒ¨ç½²æ–¹æ¡ˆ)

---

## 1. æ¨¡å—æ¦‚è¿°

### 1.1 åŠŸèƒ½å®šä¹‰

è§†é¢‘ç†è§£å¼•æ“æ˜¯æ•´ä¸ªç³»ç»Ÿçš„**æ ¸å¿ƒåŸºç¡€**ï¼Œè´Ÿè´£å°†åŸå§‹å­—å¹•æ–‡ä»¶è½¬åŒ–ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†åº“ã€‚

**è¾“å…¥**: å­—å¹•æ–‡ä»¶ï¼ˆ.srtï¼‰
**è¾“å‡º**: å®Œæ•´çš„è§†é¢‘çŸ¥è¯†åº“
**æ ¸å¿ƒèƒ½åŠ›**: æ·±åº¦è¯­ä¹‰ç†è§£ + å¤šç»´åº¦æ ‡æ³¨ + çŸ¥è¯†å›¾è°±æ„å»º

### 1.2 æ ¸å¿ƒä»·å€¼

- âœ… **ä¸€æ¬¡å¤„ç†ï¼Œæ°¸ä¹…ç†è§£**: å¤„ç†åçš„çŸ¥è¯†åº“å¯ä»¥åå¤ä½¿ç”¨ï¼Œæ— éœ€é‡æ–°åˆ†æ
- âœ… **å¤šç»´åº¦ç†è§£**: ä¸åªæ˜¯æ–‡æœ¬ï¼Œè¿˜æœ‰ä¸»é¢˜ã€æƒ…æ„Ÿã€ä»·å€¼ã€å…³ç³»
- âœ… **AIå‹å¥½**: ç”Ÿæˆçš„æ•°æ®æ ¼å¼ä¸“é—¨ä¸ºAIæ£€ç´¢å’Œç†è§£ä¼˜åŒ–

### 1.3 æŠ€æœ¯æŒ‘æˆ˜

| æŒ‘æˆ˜ | éš¾åº¦ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| è¯­ä¹‰è¾¹ç•Œè¯†åˆ« | â­â­â­â­ | AIé©±åŠ¨çš„æ™ºèƒ½åˆå¹¶ |
| ä¸»é¢˜è¯†åˆ«å‡†ç¡®ç‡ | â­â­â­â­ | æç¤ºè¯å·¥ç¨‹ + å¤šç»´éªŒè¯ |
| å¤„ç†é€Ÿåº¦ | â­â­â­ | æ‰¹é‡å¤„ç† + å¹¶è¡ŒåŒ– |
| æˆæœ¬æ§åˆ¶ | â­â­â­ | Tokenä¼˜åŒ– + ç¼“å­˜ |
| å®Œæ•´ç‰‡æ®µè¯†åˆ« | â­â­â­â­â­ | AIåˆ¤æ–­ + è§„åˆ™è¾…åŠ© |

---

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è§†é¢‘ç†è§£å¼•æ“                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚               â”‚               â”‚
          â–¼               â–¼               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  è§£æå±‚  â”‚     â”‚  åˆ†æå±‚  â”‚     â”‚  ç´¢å¼•å±‚  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Parser  â”‚     â”‚Analyzer â”‚     â”‚ Indexer â”‚
    â”‚ Module  â”‚     â”‚ Module  â”‚     â”‚ Module  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµ

```
åŸå§‹å­—å¹• (.srt)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. è§£ææ¨¡å—      â”‚ â†’ utterances.jsonl
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. åŸå­åŒ–æ¨¡å—    â”‚ â†’ micro_segments.jsonl
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æ ‡æ³¨æ¨¡å—      â”‚ â†’ analyzed_atoms.jsonl
â”‚   - ä¸»é¢˜æ ‡æ³¨     â”‚
â”‚   - æƒ…æ„Ÿæ ‡æ³¨     â”‚
â”‚   - ä»·å€¼æ ‡æ³¨     â”‚
â”‚   - å®ä½“æå–     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å‘é‡åŒ–æ¨¡å—    â”‚ â†’ embeddings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. ç»“æ„åŒ–æ¨¡å—    â”‚ â†’ entities.json
â”‚   - å®ä½“å¡ç‰‡     â”‚   topics.json
â”‚   - ä¸»é¢˜ç½‘ç»œ     â”‚   narratives.json
â”‚   - å™äº‹è¯†åˆ«     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ç´¢å¼•æ¨¡å—      â”‚ â†’ vector_db/
â”‚   - å‘é‡ç´¢å¼•     â”‚   indexes/
â”‚   - ç»“æ„åŒ–ç´¢å¼•   â”‚
â”‚   - çŸ¥è¯†å›¾è°±     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 ç›®å½•ç»“æ„

```
video_understanding_engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py                    # é…ç½®æ–‡ä»¶
â”œâ”€â”€ parsers/                     # è§£ææ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ srt_parser.py           # SRTè§£æ
â”‚   â””â”€â”€ cleaner.py              # æ¸…æ´—
â”œâ”€â”€ atomizers/                   # åŸå­åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ atomizer.py             # æ ¸å¿ƒåŸå­åŒ–é€»è¾‘
â”‚   â””â”€â”€ validator.py            # åŸå­éªŒè¯
â”œâ”€â”€ analyzers/                   # åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ topic_analyzer.py       # ä¸»é¢˜åˆ†æ
â”‚   â”œâ”€â”€ emotion_analyzer.py     # æƒ…æ„Ÿåˆ†æ
â”‚   â”œâ”€â”€ value_analyzer.py       # ä»·å€¼è¯„ä¼°
â”‚   â””â”€â”€ entity_extractor.py     # å®ä½“æå–
â”œâ”€â”€ structurers/                 # ç»“æ„åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ entity_builder.py       # æ„å»ºå®ä½“å¡ç‰‡
â”‚   â”œâ”€â”€ topic_builder.py        # æ„å»ºä¸»é¢˜ç½‘ç»œ
â”‚   â””â”€â”€ narrative_identifier.py # è¯†åˆ«å™äº‹ç‰‡æ®µ
â”œâ”€â”€ vectorizers/                 # å‘é‡åŒ–æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ embedder.py             # ç”Ÿæˆembedding
â”œâ”€â”€ indexers/                    # ç´¢å¼•æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_index.py         # å‘é‡ç´¢å¼•
â”‚   â”œâ”€â”€ structured_index.py     # ç»“æ„åŒ–ç´¢å¼•
â”‚   â””â”€â”€ graph_builder.py        # çŸ¥è¯†å›¾è°±
â”œâ”€â”€ prompts/                     # æç¤ºè¯åº“
â”‚   â”œâ”€â”€ atomize.txt
â”‚   â”œâ”€â”€ tag_topics.txt
â”‚   â”œâ”€â”€ tag_emotion.txt
â”‚   â”œâ”€â”€ tag_value.txt
â”‚   â”œâ”€â”€ build_entity.txt
â”‚   â””â”€â”€ build_topic.txt
â”œâ”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api_client.py           # APIè°ƒç”¨å°è£…
â”‚   â”œâ”€â”€ time_utils.py           # æ—¶é—´å¤„ç†
â”‚   â””â”€â”€ file_utils.py           # æ–‡ä»¶æ“ä½œ
â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utterance.py
â”‚   â”œâ”€â”€ atom.py
â”‚   â”œâ”€â”€ entity.py
â”‚   â””â”€â”€ topic.py
â””â”€â”€ engine.py                    # ä¸»å¼•æ“ç±»
```

---

## 3. å­æ¨¡å—è¯¦ç»†è®¾è®¡

### 3.1 è§£ææ¨¡å—ï¼ˆParsersï¼‰

#### åŠŸèƒ½
å°†SRTå­—å¹•æ–‡ä»¶è§£ææˆç»“æ„åŒ–æ•°æ®ï¼Œå¹¶è¿›è¡Œæ¸…æ´—ã€‚

#### æ ¸å¿ƒä»£ç 

```python
# parsers/srt_parser.py

import srt
from datetime import timedelta
from typing import List, Dict
from models.utterance import Utterance

class SRTParser:
    """SRTå­—å¹•è§£æå™¨"""

    def __init__(self):
        self.parsed_count = 0

    def parse(self, file_path: str) -> List[Utterance]:
        """
        è§£æSRTæ–‡ä»¶

        Args:
            file_path: SRTæ–‡ä»¶è·¯å¾„

        Returns:
            Utteranceåˆ—è¡¨
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        subtitles = list(srt.parse(content))
        utterances = []

        for sub in subtitles:
            utterance = Utterance(
                id=sub.index,
                start_ms=self._to_milliseconds(sub.start),
                end_ms=self._to_milliseconds(sub.end),
                text=sub.content.strip(),
                duration_ms=self._to_milliseconds(sub.end - sub.start)
            )
            utterances.append(utterance)

        self.parsed_count = len(utterances)
        return utterances

    def _to_milliseconds(self, td: timedelta) -> int:
        """å°†timedeltaè½¬ä¸ºæ¯«ç§’"""
        return int(td.total_seconds() * 1000)


# parsers/cleaner.py

class Cleaner:
    """å­—å¹•æ¸…æ´—å™¨"""

    # æ— æ„ä¹‰çš„å¡«å……è¯
    FILLER_WORDS = ['å‘ƒ', 'uh', 'um', 'eh', 'å•Š', 'å—¯', '...']

    # æœ€å°æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    MIN_DURATION_MS = 500

    def clean(self, utterances: List[Utterance]) -> List[Utterance]:
        """
        æ¸…æ´—å­—å¹•

        è¿‡æ»¤è§„åˆ™ï¼š
        1. å»é™¤çº¯å¡«å……è¯
        2. å»é™¤è¿‡çŸ­ç‰‡æ®µï¼ˆ<0.5ç§’ï¼‰
        3. æ ‡å‡†åŒ–æ–‡æœ¬
        """
        cleaned = []

        for utt in utterances:
            # è§„åˆ™1ï¼šè¿‡æ»¤å¡«å……è¯
            if utt.text in self.FILLER_WORDS:
                continue

            # è§„åˆ™2ï¼šè¿‡æ»¤è¿‡çŸ­ç‰‡æ®µ
            if utt.duration_ms < self.MIN_DURATION_MS:
                continue

            # è§„åˆ™3ï¼šæ ‡å‡†åŒ–æ–‡æœ¬
            utt.text = self._normalize_text(utt.text)

            cleaned.append(utt)

        return cleaned

    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        # å»é™¤æ¢è¡Œç¬¦
        text = text.replace('\n', ' ')
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        # å»é™¤é¦–å°¾ç©ºæ ¼
        text = text.strip()
        return text
```

#### æ•°æ®æ¨¡å‹

```python
# models/utterance.py

from pydantic import BaseModel

class Utterance(BaseModel):
    """å•å¥å­—å¹•"""
    id: int                    # åºå·
    start_ms: int              # å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    end_ms: int                # ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    text: str                  # æ–‡æœ¬å†…å®¹
    duration_ms: int           # æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

    def to_time_str(self, ms: int) -> str:
        """æ¯«ç§’è½¬æ—¶é—´å­—ç¬¦ä¸² HH:MM:SS.mmm"""
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        milliseconds = ms % 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}.{milliseconds:03d}"

    @property
    def start_time(self) -> str:
        return self.to_time_str(self.start_ms)

    @property
    def end_time(self) -> str:
        return self.to_time_str(self.end_ms)
```

#### è¾“å‡º

```jsonl
{"id": 1, "start_ms": 7000, "end_ms": 7900, "text": "what", "duration_ms": 900}
{"id": 2, "start_ms": 8933, "end_ms": 10400, "text": "å¼€å§‹äº†æ²¡æœ‰å•Š", "duration_ms": 1467}
```

---

### 3.2 åŸå­åŒ–æ¨¡å—ï¼ˆAtomizersï¼‰â­ æ ¸å¿ƒ

#### åŠŸèƒ½
å°†ç¢ç‰‡åŒ–çš„å­—å¹•åˆå¹¶æˆ"è¯­ä¹‰å®Œæ•´çš„ä¿¡æ¯å•å…ƒ"ï¼ˆåŸå­/å¾®ç‰‡æ®µï¼‰ã€‚

**æ ¸å¿ƒæŒ‘æˆ˜**: å¦‚ä½•åˆ¤æ–­å“ªäº›å¥å­åº”è¯¥åˆå¹¶ï¼Ÿ

#### è®¾è®¡æ€è·¯

**æ–¹æ³•**: AIé©±åŠ¨çš„æ™ºèƒ½åˆå¹¶

```
ä¼ ç»Ÿæ–¹æ³•ï¼ˆä¸å¯è¡Œï¼‰ï¼š
âŒ æŒ‰å›ºå®šæ—¶é—´åˆ‡åˆ†ï¼ˆæ¯”å¦‚æ¯30ç§’ï¼‰â†’ ä¼šç ´åè¯­ä¹‰
âŒ æŒ‰å¥å·åˆ‡åˆ† â†’ å­—å¹•æ ‡ç‚¹ä¸å‡†ç¡®
âŒ æŒ‰ä¸»é¢˜è¯åˆ‡æ¢ â†’ æ— æ³•è¯†åˆ«å¤æ‚è¯­å¢ƒ

AIæ–¹æ³•ï¼ˆæ¨èï¼‰ï¼š
âœ… è®©Claudeç†è§£å­—å¹•å†…å®¹
âœ… è¯†åˆ«è¯­ä¹‰è¾¹ç•Œï¼ˆä¸»é¢˜è½¬æ¢ã€åœé¡¿ã€äº’åŠ¨ï¼‰
âœ… åˆå¹¶æˆå®Œæ•´è¡¨è¾¾
âœ… ç‰¹åˆ«è¯†åˆ«"å®Œæ•´ç‰‡æ®µ"ï¼ˆ5-15åˆ†é’Ÿçš„è¿ç»­å†…å®¹ï¼‰
```

#### æ ¸å¿ƒä»£ç 

```python
# atomizers/atomizer.py

import anthropic
import json
from typing import List
from models.utterance import Utterance
from models.atom import Atom

class Atomizer:
    """åŸå­åŒ–å¤„ç†å™¨"""

    def __init__(self, api_key: str, batch_size: int = 50):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.batch_size = batch_size

        # åŠ è½½æç¤ºè¯
        with open('prompts/atomize.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def atomize(self, utterances: List[Utterance]) -> List[Atom]:
        """
        åŸå­åŒ–å¤„ç†

        ç­–ç•¥ï¼š
        1. æ¯æ¬¡å¤„ç†50æ¡å­—å¹•ï¼ˆçº¦2-3åˆ†é’Ÿå†…å®¹ï¼‰
        2. è®©AIè¯†åˆ«è¯­ä¹‰è¾¹ç•Œ
        3. åˆå¹¶æˆå®Œæ•´ç‰‡æ®µ
        """
        atoms = []
        total_batches = (len(utterances) + self.batch_size - 1) // self.batch_size

        for i in range(0, len(utterances), self.batch_size):
            batch = utterances[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1

            print(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}...")

            # è°ƒç”¨AI
            batch_atoms = self._process_batch(batch)
            atoms.extend(batch_atoms)

        return atoms

    def _process_batch(self, batch: List[Utterance]) -> List[Atom]:
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡"""
        # æ„å»ºè¾“å…¥æ–‡æœ¬
        input_text = "\n".join([
            f"[{utt.start_time}] {utt.text}"
            for utt in batch
        ])

        # è°ƒç”¨Claude
        prompt = self.prompt_template.format(input_text=input_text)

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        # è§£æè¿”å›çš„JSON
        result_text = response.content[0].text

        # æå–JSONï¼ˆæœ‰æ—¶Claudeä¼šåœ¨JSONå‰ååŠ è¯´æ˜æ–‡å­—ï¼‰
        json_str = self._extract_json(result_text)
        atoms_data = json.loads(json_str)

        # è½¬æ¢ä¸ºAtomå¯¹è±¡
        atoms = [Atom(**data) for data in atoms_data]

        return atoms

    def _extract_json(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        # å¯»æ‰¾ç¬¬ä¸€ä¸ª [ å’Œæœ€åä¸€ä¸ª ]
        start = text.find('[')
        end = text.rfind(']') + 1

        if start == -1 or end == 0:
            raise ValueError("æ— æ³•ä»è¿”å›ç»“æœä¸­æå–JSON")

        return text[start:end]


# atomizers/validator.py

class AtomValidator:
    """åŸå­éªŒè¯å™¨"""

    def validate(self, atoms: List[Atom]) -> List[str]:
        """
        éªŒè¯åŸå­è´¨é‡

        æ£€æŸ¥ï¼š
        1. æ—¶é—´è¿ç»­æ€§
        2. æ–‡æœ¬é•¿åº¦åˆç†æ€§
        3. æ—¶é—´é—´éš”
        """
        issues = []

        for i, atom in enumerate(atoms):
            # æ£€æŸ¥1ï¼šæ—¶é—´è¿ç»­æ€§
            if i > 0:
                prev_end = atoms[i-1].end_ms
                curr_start = atom.start_ms
                gap_ms = curr_start - prev_end

                if gap_ms > 30000:  # è¶…è¿‡30ç§’é—´éš”
                    issues.append(
                        f"âš ï¸ {atom.atom_id}: ä¸å‰ä¸€ä¸ªåŸå­é—´éš” {gap_ms/1000:.1f}ç§’"
                    )

            # æ£€æŸ¥2ï¼šæ–‡æœ¬é•¿åº¦
            text_len = len(atom.merged_text)
            if text_len < 10:
                issues.append(f"âš ï¸ {atom.atom_id}: æ–‡æœ¬è¿‡çŸ­ï¼ˆ{text_len}å­—ç¬¦ï¼‰")
            elif text_len > 500:
                issues.append(f"âš ï¸ {atom.atom_id}: æ–‡æœ¬è¿‡é•¿ï¼ˆ{text_len}å­—ç¬¦ï¼‰")

            # æ£€æŸ¥3ï¼šæŒç»­æ—¶é—´
            duration_sec = atom.duration_ms / 1000
            if duration_sec > 600:  # è¶…è¿‡10åˆ†é’Ÿ
                if atom.type != "complete_segment":
                    issues.append(
                        f"âš ï¸ {atom.atom_id}: æ—¶é•¿{duration_sec/60:.1f}åˆ†é’Ÿï¼Œ"
                        f"ä½†æœªæ ‡è®°ä¸ºå®Œæ•´ç‰‡æ®µ"
                    )

        return issues
```

#### æ•°æ®æ¨¡å‹

```python
# models/atom.py

from pydantic import BaseModel
from typing import List, Optional

class Atom(BaseModel):
    """ä¿¡æ¯åŸå­/å¾®ç‰‡æ®µ"""
    atom_id: str                      # åŸå­IDï¼Œå¦‚ "A001"
    start_ms: int                     # å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    end_ms: int                       # ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    duration_ms: int                  # æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    merged_text: str                  # åˆå¹¶åçš„æ–‡æœ¬
    type: str                         # ç±»å‹ï¼šfragment/complete_segment
    completeness: str                 # å®Œæ•´æ€§ï¼šå®Œæ•´/éœ€è¦ä¸Šä¸‹æ–‡
    source_utterance_ids: List[int]   # æ¥æºå­—å¹•IDåˆ—è¡¨

    # å¯é€‰å­—æ®µï¼ˆåˆå§‹åŒ–æ—¶ä¸ºç©ºï¼‰
    topics: Optional[dict] = None     # ä¸»é¢˜æ ‡æ³¨
    emotion: Optional[dict] = None    # æƒ…æ„Ÿæ ‡æ³¨
    value: Optional[dict] = None      # ä»·å€¼æ ‡æ³¨
    embedding: Optional[List[float]] = None  # å‘é‡

    @property
    def start_time(self) -> str:
        """æ ¼å¼åŒ–å¼€å§‹æ—¶é—´"""
        return self._ms_to_time(self.start_ms)

    @property
    def end_time(self) -> str:
        """æ ¼å¼åŒ–ç»“æŸæ—¶é—´"""
        return self._ms_to_time(self.end_ms)

    def _ms_to_time(self, ms: int) -> str:
        hours = ms // 3600000
        minutes = (ms % 3600000) // 60000
        seconds = (ms % 60000) // 1000
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
```

#### æç¤ºè¯

```
# prompts/atomize.txt

ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚æˆ‘ä¼šç»™ä½ ä¸€æ®µç›´æ’­çš„å­—å¹•ç‰‡æ®µï¼Œä½ çš„ä»»åŠ¡æ˜¯æŠŠå®ƒä»¬åˆå¹¶æˆ"è¯­ä¹‰å®Œæ•´çš„ä¿¡æ¯å•å…ƒ"ã€‚

ã€æ ¸å¿ƒè§„åˆ™ã€‘
1. åŸå­å¯ä»¥æ˜¯ä»»æ„é•¿åº¦ï¼š
   - çŸ­ï¼š10-30ç§’ï¼ˆä¸€å¥è¯ã€ä¸€æ¬¡äº’åŠ¨ï¼‰
   - ä¸­ï¼š1-5åˆ†é’Ÿï¼ˆä¸€ä¸ªè§‚ç‚¹ã€ä¸€ä¸ªå°æ•…äº‹ï¼‰
   - é•¿ï¼š5-15åˆ†é’Ÿï¼ˆä¸€ä¸ªå®Œæ•´æ•…äº‹ã€å®Œæ•´è®ºè¿°ï¼‰

2. åˆ¤æ–­è¾¹ç•Œçš„æ ‡å‡†ï¼š
   - ä¸»é¢˜è½¬æ¢ â†’ æ–°åŸå­
   - é•¿æ—¶é—´åœé¡¿ï¼ˆ>5ç§’ï¼‰â†’ æ–°åŸå­
   - ä»å™äº‹è½¬åˆ°äº’åŠ¨ â†’ æ–°åŸå­
   - ä»ä¸€ä¸ªäº‹ä»¶è½¬åˆ°å¦ä¸€ä¸ªäº‹ä»¶ â†’ æ–°åŸå­

3. ç‰¹åˆ«è¯†åˆ«"å®Œæ•´ç‰‡æ®µ"ï¼ˆcomplete_segmentï¼‰ï¼š
   å¦‚æœå‘ç°5åˆ†é’Ÿä»¥ä¸Šçš„å†…å®¹æ»¡è¶³ï¼š
   âœ… ä¸»é¢˜ç»Ÿä¸€ï¼Œæ²¡æœ‰è·‘é¢˜
   âœ… é€»è¾‘å®Œæ•´ï¼Œæœ‰å¤´æœ‰å°¾
   âœ… å¯ä»¥ç‹¬ç«‹ç†è§£
   â†’ æ ‡è®°ä¸º type: "complete_segment"

4. ç±»å‹åˆ†ç±»ï¼š
   - å™è¿°å†å²ï¼šè®²è¿°è¿‡å»çš„äº‹ä»¶
   - å›åº”å¼¹å¹•ï¼šä¸è§‚ä¼—äº’åŠ¨
   - å‘è¡¨è§‚ç‚¹ï¼šä¸ªäººè¯„ä»·å’Œçœ‹æ³•
   - è¯»æ¥ä¿¡ï¼šè¯»è§‚ä¼—æ¥ä¿¡
   - é—²èŠï¼šæ— å…³å†…å®¹ã€è¿‡åœº

ã€è¾“å…¥æ ¼å¼ã€‘
[æ—¶é—´] æ–‡æœ¬å†…å®¹

ã€è¾“å‡ºæ ¼å¼ã€‘
è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- atom_id: åŸå­IDï¼ˆA001, A002...ï¼‰
- start_ms: å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- end_ms: ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- duration_ms: æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
- merged_text: åˆå¹¶åçš„å®Œæ•´æ–‡æœ¬
- type: "fragment"ï¼ˆç¢ç‰‡ï¼‰æˆ– "complete_segment"ï¼ˆå®Œæ•´ç‰‡æ®µï¼‰
- completeness: "å®Œæ•´" æˆ– "éœ€è¦ä¸Šä¸‹æ–‡"
- source_utterance_ids: æ¥æºå­—å¹•IDåˆ—è¡¨

ã€ç¤ºä¾‹ã€‘

è¾“å…¥ï¼š
[00:08:20] 1962å¹´
[00:08:25] å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’
[00:08:30] è¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº
[00:08:38] hello æµ·ç»µå®å®
[00:08:40] ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„

è¾“å‡ºï¼š
[
  {
    "atom_id": "A001",
    "start_ms": 500000,
    "end_ms": 510000,
    "duration_ms": 10000,
    "merged_text": "1962å¹´å›½æ°‘å…šæ®‹å†›æ’¤åˆ°é‡‘ä¸‰è§’ï¼Œè¿™æ˜¯æ•´ä¸ªé‡‘ä¸‰è§’é—®é¢˜çš„èµ·æº",
    "type": "fragment",
    "completeness": "å®Œæ•´",
    "source_utterance_ids": [85, 86, 87]
  },
  {
    "atom_id": "A002",
    "start_ms": 518000,
    "end_ms": 520000,
    "duration_ms": 2000,
    "merged_text": "hello æµ·ç»µå®å®",
    "type": "fragment",
    "completeness": "å®Œæ•´",
    "source_utterance_ids": [88]
  },
  {
    "atom_id": "A003",
    "start_ms": 520000,
    "end_ms": 525000,
    "duration_ms": 5000,
    "merged_text": "ç„¶åå‘¢å¤æ²™å°±æ˜¯åœ¨è¿™ä¸ªèƒŒæ™¯ä¸‹å´›èµ·çš„",
    "type": "fragment",
    "completeness": "éœ€è¦ä¸Šä¸‹æ–‡",
    "source_utterance_ids": [89]
  }
]

ã€é‡è¦ã€‘
- åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è¯´æ˜æ–‡å­—
- ç¡®ä¿JSONæ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥è¢«è§£æ
- æ—¶é—´å¿…é¡»è¿ç»­ï¼Œä¸èƒ½æœ‰é‡å æˆ–é—æ¼
```

---

### 3.3 åˆ†ææ¨¡å—ï¼ˆAnalyzersï¼‰

#### åŠŸèƒ½
ç»™æ¯ä¸ªåŸå­æ‰“ä¸Šå¤šç»´åº¦æ ‡ç­¾ã€‚

#### å­æ¨¡å—

##### 3.3.1 ä¸»é¢˜åˆ†æå™¨

```python
# analyzers/topic_analyzer.py

class TopicAnalyzer:
    """ä¸»é¢˜åˆ†æå™¨"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        with open('prompts/tag_topics.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        åˆ†æä¸»é¢˜

        ä¸ºæ¯ä¸ªåŸå­æ ‡æ³¨ï¼š
        - primary_topic: ä¸»è¦ä¸»é¢˜
        - secondary_topics: æ¬¡è¦ä¸»é¢˜åˆ—è¡¨
        - topic_scores: ä¸»é¢˜ç›¸å…³åº¦åˆ†æ•°
        - entities: æå–çš„å®ä½“ï¼ˆäººç‰©ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶ã€æ¦‚å¿µï¼‰
        """
        for i, atom in enumerate(atoms):
            print(f"åˆ†æä¸»é¢˜: {i+1}/{len(atoms)}")

            prompt = self.prompt_template.format(
                atom_text=atom.merged_text
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            atom.topics = result

        return atoms
```

æç¤ºè¯è§å‰é¢çš„"å®Œæ•´å®æ–½æµç¨‹"æ–‡æ¡£ã€‚

##### 3.3.2 æƒ…æ„Ÿåˆ†æå™¨

```python
# analyzers/emotion_analyzer.py

class EmotionAnalyzer:
    """æƒ…æ„Ÿåˆ†æå™¨"""

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        åˆ†ææƒ…æ„Ÿå’Œèƒ½é‡

        æ ‡æ³¨ï¼š
        - emotion_type: æƒ…æ„Ÿç±»å‹
        - energy_level: èƒ½é‡å€¼ï¼ˆ1-10ï¼‰
        - energy_trend: è¶‹åŠ¿ï¼ˆé€’å¢/é€’å‡/å¹³ç¨³ï¼‰
        """
        # ç±»ä¼¼TopicAnalyzerçš„å®ç°
        pass
```

##### 3.3.3 ä»·å€¼è¯„ä¼°å™¨

```python
# analyzers/value_analyzer.py

class ValueAnalyzer:
    """ä»·å€¼è¯„ä¼°å™¨"""

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """
        è¯„ä¼°å†…å®¹ä»·å€¼

        æ ‡æ³¨ï¼š
        - information_density: ä¿¡æ¯å¯†åº¦ï¼ˆ1-10ï¼‰
        - editability: å¯å‰ªè¾‘æ€§ï¼ˆå¿…å‰ª/å¯å‰ª/å¯åˆ /å¿…åˆ ï¼‰
        - independence: ç‹¬ç«‹æ€§
        - special_value: ç‰¹æ®Šä»·å€¼ï¼ˆé‡‘å¥/é«˜æ½®ç‚¹/äº‰è®®ç‚¹/æ— ï¼‰
        """
        pass
```

#### ä¼˜åŒ–ï¼šæ‰¹é‡åˆ†æ

ä¸ºäº†æé«˜æ•ˆç‡ï¼Œå¯ä»¥æŠŠä¸‰ä¸ªç»´åº¦çš„åˆ†æåˆå¹¶åˆ°ä¸€æ¬¡APIè°ƒç”¨ï¼š

```python
# analyzers/batch_analyzer.py

class BatchAnalyzer:
    """æ‰¹é‡åˆ†æå™¨ï¼šä¸€æ¬¡è°ƒç”¨å®Œæˆä¸‰ä¸ªç»´åº¦çš„åˆ†æ"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        # åˆå¹¶çš„æç¤ºè¯
        with open('prompts/analyze_all.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def analyze(self, atoms: List[Atom]) -> List[Atom]:
        """ä¸€æ¬¡æ€§åˆ†æä¸»é¢˜ã€æƒ…æ„Ÿã€ä»·å€¼"""
        for i, atom in enumerate(atoms):
            print(f"åˆ†æ: {i+1}/{len(atoms)}")

            prompt = self.prompt_template.format(
                atom_text=atom.merged_text
            )

            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=3000,
                messages=[{"role": "user", "content": prompt}]
            )

            # è¿”å›ç»“æœåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†
            result = json.loads(response.content[0].text)

            atom.topics = result['topics']
            atom.emotion = result['emotion']
            atom.value = result['value']

        return atoms
```

**æç¤ºè¯**ï¼š

```
# prompts/analyze_all.txt

åˆ†æè¿™æ®µæ–‡æœ¬çš„å¤šä¸ªç»´åº¦ã€‚

ã€æ–‡æœ¬ã€‘
{atom_text}

ã€ä»»åŠ¡ã€‘
è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦åˆ†æï¼š

1. ä¸»é¢˜ç»´åº¦
   - ä¸»è¦ä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
   - æ¬¡è¦ä¸»é¢˜æœ‰å“ªäº›ï¼Ÿ
   - æå–å®ä½“ï¼šäººç‰©ã€åœ°ç‚¹ã€æ—¶é—´ã€äº‹ä»¶ã€æ¦‚å¿µ

2. æƒ…æ„Ÿç»´åº¦
   - æƒ…æ„Ÿç±»å‹ï¼ˆå®¢è§‚å™è¿°/æ¿€åŠ¨/å¹½é»˜/åŒæƒ…/æ‰¹åˆ¤ï¼‰
   - èƒ½é‡å€¼ï¼ˆ1-10ï¼‰
   - è¶‹åŠ¿ï¼ˆé€’å¢/é€’å‡/å¹³ç¨³ï¼‰

3. ä»·å€¼ç»´åº¦
   - ä¿¡æ¯å¯†åº¦ï¼ˆ1-10ï¼‰
   - å¯å‰ªè¾‘æ€§ï¼ˆå¿…å‰ª/å¯å‰ª/å¯åˆ /å¿…åˆ ï¼‰
   - ç‹¬ç«‹æ€§ï¼ˆç‹¬ç«‹/éœ€é“ºå«/éœ€è¡¥å……/ä¾èµ–ä¸Šä¸‹æ–‡ï¼‰
   - ç‰¹æ®Šä»·å€¼ï¼ˆé‡‘å¥/é«˜æ½®ç‚¹/äº‰è®®ç‚¹/æ— ï¼‰

ã€è¾“å‡ºJSONã€‘
{
  "topics": {
    "primary_topic": "...",
    "secondary_topics": [...],
    "topic_scores": {...},
    "entities": {
      "persons": [...],
      "locations": [...],
      "time_points": [...],
      "events": [...],
      "concepts": [...]
    }
  },
  "emotion": {
    "emotion_type": "...",
    "energy_level": 8,
    "energy_trend": "..."
  },
  "value": {
    "information_density": 9,
    "editability": "å¿…å‰ª",
    "independence": "éœ€é“ºå«",
    "special_value": "é«˜æ½®ç‚¹"
  }
}
```

---

### 3.4 å‘é‡åŒ–æ¨¡å—ï¼ˆVectorizersï¼‰

#### åŠŸèƒ½
ä¸ºæ¯ä¸ªåŸå­ç”Ÿæˆè¯­ä¹‰å‘é‡ï¼ˆembeddingï¼‰ï¼Œç”¨äºåç»­çš„è¯­ä¹‰æœç´¢ã€‚

```python
# vectorizers/embedder.py

from openai import OpenAI
from typing import List
from models.atom import Atom

class Embedder:
    """å‘é‡ç”Ÿæˆå™¨"""

    def __init__(self, api_key: str, model: str = "text-embedding-3-large"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.batch_size = 2048  # OpenAIæ”¯æŒçš„æœ€å¤§æ‰¹é‡

    def generate_embeddings(self, atoms: List[Atom]) -> List[Atom]:
        """
        æ‰¹é‡ç”Ÿæˆembedding

        ç­–ç•¥ï¼š
        1. OpenAIæ”¯æŒæ‰¹é‡ç”Ÿæˆï¼Œæ¯æ¬¡æœ€å¤š2048æ¡
        2. ç”¨merged_textä½œä¸ºè¾“å…¥
        3. è¿”å›3072ç»´å‘é‡ï¼ˆtext-embedding-3-largeï¼‰
        """
        texts = [atom.merged_text for atom in atoms]

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            batch_atoms = atoms[i:i + self.batch_size]

            print(f"ç”Ÿæˆå‘é‡: {i+1}-{min(i+self.batch_size, len(texts))}/{len(texts)}")

            # è°ƒç”¨OpenAI
            response = self.client.embeddings.create(
                model=self.model,
                input=batch_texts
            )

            # èµ‹å€¼
            for j, embedding_data in enumerate(response.data):
                batch_atoms[j].embedding = embedding_data.embedding

        return atoms
```

---

### 3.5 ç»“æ„åŒ–æ¨¡å—ï¼ˆStructurersï¼‰

#### åŠŸèƒ½
åŸºäºæ ‡æ³¨åçš„åŸå­ï¼Œæ„å»ºé«˜å±‚æ¬¡çš„ç»“æ„ï¼š
1. å®ä½“å¡ç‰‡
2. ä¸»é¢˜ç½‘ç»œ
3. å™äº‹ç‰‡æ®µ

##### 3.5.1 å®ä½“æ„å»ºå™¨

```python
# structurers/entity_builder.py

from typing import List, Dict
from models.atom import Atom
from models.entity import Entity

class EntityBuilder:
    """å®ä½“å¡ç‰‡æ„å»ºå™¨"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

        with open('prompts/build_entity.txt', 'r', encoding='utf-8') as f:
            self.prompt_template = f.read()

    def build_entities(self, atoms: List[Atom]) -> List[Entity]:
        """
        æ„å»ºå®ä½“å¡ç‰‡

        æµç¨‹ï¼š
        1. æ”¶é›†æ‰€æœ‰æåˆ°çš„å®ä½“
        2. æŒ‰å®ä½“èšåˆç›¸å…³åŸå­
        3. è°ƒç”¨AIç”Ÿæˆå®ä½“å¡ç‰‡
        """
        # Step 1: æ”¶é›†å®ä½“
        entity_mentions = self._collect_entities(atoms)

        # Step 2: ä¸ºæ¯ä¸ªå®ä½“æ„å»ºå¡ç‰‡
        entities = []
        for entity_name, atom_ids in entity_mentions.items():
            if len(atom_ids) < 3:  # è‡³å°‘å‡ºç°3æ¬¡
                continue

            print(f"æ„å»ºå®ä½“: {entity_name}")

            entity = self._build_entity_card(entity_name, atom_ids, atoms)
            entities.append(entity)

        return entities

    def _collect_entities(self, atoms: List[Atom]) -> Dict[str, List[str]]:
        """æ”¶é›†å®ä½“æåŠ"""
        entity_mentions = {}

        for atom in atoms:
            if not atom.topics:
                continue

            persons = atom.topics.get('entities', {}).get('persons', [])

            for person in persons:
                if person not in entity_mentions:
                    entity_mentions[person] = []
                entity_mentions[person].append(atom.atom_id)

        return entity_mentions

    def _build_entity_card(
        self,
        entity_name: str,
        atom_ids: List[str],
        all_atoms: List[Atom]
    ) -> Entity:
        """æ„å»ºå•ä¸ªå®ä½“å¡ç‰‡"""
        # è·å–ç›¸å…³åŸå­
        related_atoms = [a for a in all_atoms if a.atom_id in atom_ids]

        # æ„å»ºè¾“å…¥æ–‡æœ¬
        atoms_text = "\n\n".join([
            f"[{a.atom_id}] {a.start_time}-{a.end_time}\n{a.merged_text}"
            for a in related_atoms
        ])

        # è°ƒç”¨AI
        prompt = self.prompt_template.format(
            entity_name=entity_name,
            related_atoms_text=atoms_text
        )

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )

        entity_data = json.loads(response.content[0].text)

        # ç”Ÿæˆentityçš„embedding
        entity_summary = entity_data['profile']['basic_info']
        entity_data['embedding'] = self._generate_single_embedding(entity_summary)

        return Entity(**entity_data)

    def _generate_single_embedding(self, text: str) -> List[float]:
        """ä¸ºå•ä¸ªæ–‡æœ¬ç”Ÿæˆembedding"""
        client = OpenAI(api_key=self.openai_key)
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=[text]
        )
        return response.data[0].embedding
```

##### 3.5.2 ä¸»é¢˜ç½‘ç»œæ„å»ºå™¨

```python
# structurers/topic_builder.py

class TopicBuilder:
    """ä¸»é¢˜ç½‘ç»œæ„å»ºå™¨"""

    def build_topics(self, atoms: List[Atom]) -> List[Topic]:
        """
        æ„å»ºä¸»é¢˜ç½‘ç»œ

        æµç¨‹ï¼š
        1. ç»Ÿè®¡ä¸»é¢˜å‡ºç°é¢‘ç‡
        2. ä¸ºä¸»è¦ä¸»é¢˜ï¼ˆå‡ºç°10æ¬¡ä»¥ä¸Šï¼‰æ„å»ºè¯¦ç»†å¡ç‰‡
        3. è¯†åˆ«å­ä¸»é¢˜
        4. è®¾è®¡å™äº‹æ¨¡æ¿
        """
        # ç»Ÿè®¡
        topic_stats = self._collect_topic_stats(atoms)

        # æ„å»º
        topics = []
        for topic_name, count in topic_stats.items():
            if count < 10:
                continue

            print(f"æ„å»ºä¸»é¢˜: {topic_name} ({count}æ¬¡)")

            topic = self._build_topic_card(topic_name, atoms)
            topics.append(topic)

        return topics
```

##### 3.5.3 å™äº‹è¯†åˆ«å™¨

```python
# structurers/narrative_identifier.py

class NarrativeIdentifier:
    """å™äº‹ç‰‡æ®µè¯†åˆ«å™¨"""

    def identify_narratives(self, atoms: List[Atom]) -> List[Narrative]:
        """
        è¯†åˆ«å®Œæ•´çš„å™äº‹ç‰‡æ®µ

        å¯»æ‰¾ï¼š
        - æœ‰å¼€å¤´ã€å‘å±•ã€ç»“å±€çš„å®Œæ•´æ•…äº‹
        - æŒç»­5åˆ†é’Ÿä»¥ä¸Š
        - é€»è¾‘è¿è´¯
        """
        # å…ˆæ‰¾æ‰€æœ‰æ ‡è®°ä¸º"complete_segment"çš„åŸå­
        complete_atoms = [a for a in atoms if a.type == "complete_segment"]

        # å†ç”¨AIè¯†åˆ«å…¶ä»–å¯èƒ½çš„å™äº‹ç‰‡æ®µ
        narratives = self._ai_identify_narratives(atoms)

        return narratives
```

---

### 3.6 ç´¢å¼•æ¨¡å—ï¼ˆIndexersï¼‰

#### åŠŸèƒ½
å»ºç«‹å¤šç»´åº¦ç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿæ£€ç´¢ã€‚

##### 3.6.1 å‘é‡ç´¢å¼•

```python
# indexers/vector_index.py

import chromadb
from typing import List
from models.atom import Atom
from models.entity import Entity
from models.topic import Topic

class VectorIndexer:
    """å‘é‡ç´¢å¼•æ„å»ºå™¨"""

    def __init__(self, persist_directory: str):
        self.client = chromadb.PersistentClient(path=persist_directory)

    def build_index(
        self,
        atoms: List[Atom],
        entities: List[Entity],
        topics: List[Topic]
    ):
        """
        æ„å»ºå‘é‡ç´¢å¼•

        æŠŠåŸå­ã€å®ä½“ã€ä¸»é¢˜çš„embeddingéƒ½å­˜å…¥å‘é‡åº“
        """
        # åˆ›å»ºcollection
        collection = self.client.create_collection(
            name="video_knowledge",
            metadata={"description": "è§†é¢‘çŸ¥è¯†åº“"}
        )

        # æ·»åŠ åŸå­
        print("ç´¢å¼•åŸå­...")
        for atom in atoms:
            if not atom.embedding:
                continue

            collection.add(
                ids=[atom.atom_id],
                embeddings=[atom.embedding],
                metadatas=[{
                    "type": "atom",
                    "start_time": atom.start_time,
                    "end_time": atom.end_time,
                    "primary_topic": atom.topics.get('primary_topic', ''),
                    "value_score": atom.value.get('information_density', 0)
                }],
                documents=[atom.merged_text]
            )

        # æ·»åŠ å®ä½“
        print("ç´¢å¼•å®ä½“...")
        for entity in entities:
            if not entity.embedding:
                continue

            collection.add(
                ids=[entity.entity_id],
                embeddings=[entity.embedding],
                metadatas=[{
                    "type": "entity",
                    "entity_type": entity.type,
                    "name": entity.name
                }],
                documents=[entity.profile['basic_info']]
            )

        # æ·»åŠ ä¸»é¢˜
        print("ç´¢å¼•ä¸»é¢˜...")
        for topic in topics:
            if not topic.embedding:
                continue

            collection.add(
                ids=[topic.topic_id],
                embeddings=[topic.embedding],
                metadatas=[{
                    "type": "topic",
                    "name": topic.name
                }],
                documents=[topic.definition]
            )

        print(f"âœ“ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±{collection.count()}æ¡")
```

##### 3.6.2 ç»“æ„åŒ–ç´¢å¼•

```python
# indexers/structured_index.py

class StructuredIndexer:
    """ç»“æ„åŒ–ç´¢å¼•æ„å»ºå™¨"""

    def build_index(self, atoms: List[Atom]) -> dict:
        """
        æ„å»ºå¤šç»´åº¦ç»“æ„åŒ–ç´¢å¼•

        ç´¢å¼•ç»´åº¦ï¼š
        - by_time: æŒ‰æ—¶é—´ï¼ˆ10åˆ†é’Ÿä¸ºå•ä½ï¼‰
        - by_person: æŒ‰äººç‰©
        - by_topic: æŒ‰ä¸»é¢˜
        - by_value: æŒ‰ä»·å€¼ç­‰çº§
        - by_emotion: æŒ‰æƒ…æ„Ÿ
        """
        indexes = {
            "by_time": {},
            "by_person": {},
            "by_topic": {},
            "by_value": {},
            "by_emotion": {}
        }

        for atom in atoms:
            # æ—¶é—´ç´¢å¼•
            time_bucket = self._get_time_bucket(atom.start_ms)
            if time_bucket not in indexes['by_time']:
                indexes['by_time'][time_bucket] = []
            indexes['by_time'][time_bucket].append(atom.atom_id)

            # äººç‰©ç´¢å¼•
            persons = atom.topics.get('entities', {}).get('persons', [])
            for person in persons:
                if person not in indexes['by_person']:
                    indexes['by_person'][person] = []
                indexes['by_person'][person].append(atom.atom_id)

            # ä¸»é¢˜ç´¢å¼•
            topics = atom.topics.get('secondary_topics', [])
            for topic in topics:
                if topic not in indexes['by_topic']:
                    indexes['by_topic'][topic] = []
                indexes['by_topic'][topic].append(atom.atom_id)

            # ä»·å€¼ç´¢å¼•
            value = atom.value.get('information_density', 0)
            value_tier = f"{(value-1)//2*2+1}-{(value-1)//2*2+2}åˆ†"
            if value_tier not in indexes['by_value']:
                indexes['by_value'][value_tier] = []
            indexes['by_value'][value_tier].append(atom.atom_id)

            # æƒ…æ„Ÿç´¢å¼•
            emotion = atom.emotion.get('emotion_type', 'æœªçŸ¥')
            if emotion not in indexes['by_emotion']:
                indexes['by_emotion'][emotion] = []
            indexes['by_emotion'][emotion].append(atom.atom_id)

        return indexes

    def _get_time_bucket(self, ms: int, bucket_size_ms: int = 600000) -> str:
        """è·å–æ—¶é—´æ¡¶ï¼ˆé»˜è®¤10åˆ†é’Ÿï¼‰"""
        bucket_num = ms // bucket_size_ms
        start_min = bucket_num * (bucket_size_ms // 60000)
        end_min = start_min + (bucket_size_ms // 60000)
        return f"{start_min:02d}:00-{end_min:02d}:00"
```

##### 3.6.3 çŸ¥è¯†å›¾è°±æ„å»ºå™¨

```python
# indexers/graph_builder.py

import networkx as nx

class GraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨"""

    def build_graph(self, entities: List[Entity]) -> dict:
        """
        æ„å»ºçŸ¥è¯†å›¾è°±

        èŠ‚ç‚¹ï¼šå®ä½“
        è¾¹ï¼šå…³ç³»
        """
        G = nx.DiGraph()

        # æ·»åŠ èŠ‚ç‚¹
        for entity in entities:
            G.add_node(
                entity.entity_id,
                type=entity.type,
                name=entity.name,
                data=entity.dict()
            )

        # æ·»åŠ è¾¹
        for entity in entities:
            if 'relationships' not in entity.dict():
                continue

            for rel in entity.relationships:
                target_id = self._find_entity_id(entities, rel['target'])
                if target_id:
                    G.add_edge(
                        entity.entity_id,
                        target_id,
                        relation=rel['relation'],
                        atoms=rel.get('atoms', [])
                    )

        # è½¬æ¢ä¸ºJSONæ ¼å¼
        graph_data = nx.node_link_data(G)
        return graph_data

    def _find_entity_id(self, entities: List[Entity], name: str) -> str:
        """æ ¹æ®åç§°æŸ¥æ‰¾å®ä½“ID"""
        for entity in entities:
            if entity.name == name:
                return entity.entity_id
        return None
```

---

### 3.7 ä¸»å¼•æ“ç±»

```python
# engine.py

from typing import Optional
import os
import json

class VideoUnderstandingEngine:
    """è§†é¢‘ç†è§£å¼•æ“ä¸»ç±»"""

    def __init__(
        self,
        claude_api_key: str,
        openai_api_key: str,
        output_dir: str = "data/output"
    ):
        self.claude_key = claude_api_key
        self.openai_key = openai_api_key
        self.output_dir = output_dir

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.parser = SRTParser()
        self.cleaner = Cleaner()
        self.atomizer = Atomizer(claude_api_key)
        self.validator = AtomValidator()
        self.batch_analyzer = BatchAnalyzer(claude_api_key)
        self.embedder = Embedder(openai_api_key)
        self.entity_builder = EntityBuilder(claude_api_key)
        self.topic_builder = TopicBuilder(claude_api_key)
        self.narrative_identifier = NarrativeIdentifier(claude_api_key)

    def process(self, srt_file: str, video_id: str) -> str:
        """
        å¤„ç†ä¸€ä¸ªè§†é¢‘

        Args:
            srt_file: SRTæ–‡ä»¶è·¯å¾„
            video_id: è§†é¢‘ID

        Returns:
            è¾“å‡ºç›®å½•è·¯å¾„
        """
        print("="*60)
        print(f"è§†é¢‘ç†è§£å¼•æ“ - å¤„ç† {video_id}")
        print("="*60)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        video_output_dir = os.path.join(self.output_dir, video_id)
        os.makedirs(video_output_dir, exist_ok=True)
        os.makedirs(os.path.join(video_output_dir, "indexes"), exist_ok=True)

        # Step 1: è§£æ
        print("\n[1/6] è§£æå­—å¹•...")
        utterances = self.parser.parse(srt_file)
        cleaned = self.cleaner.clean(utterances)
        self._save_json({"utterances": [u.dict() for u in cleaned]},
                       os.path.join(video_output_dir, "utterances.jsonl"))
        print(f"âœ“ è§£æå®Œæˆï¼š{len(cleaned)}æ¡å­—å¹•")

        # Step 2: åŸå­åŒ–
        print("\n[2/6] åŸå­åŒ–å¤„ç†ï¼ˆé¢„è®¡30-40åˆ†é’Ÿï¼‰...")
        atoms = self.atomizer.atomize(cleaned)
        issues = self.validator.validate(atoms)
        if issues:
            print("âš ï¸ éªŒè¯å‘ç°é—®é¢˜ï¼š")
            for issue in issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  {issue}")
        self._save_json({"atoms": [a.dict() for a in atoms]},
                       os.path.join(video_output_dir, "micro_segments.jsonl"))
        print(f"âœ“ åŸå­åŒ–å®Œæˆï¼š{len(atoms)}ä¸ªåŸå­")

        # Step 3: è¯­ä¹‰åˆ†æ
        print("\n[3/6] è¯­ä¹‰åˆ†æï¼ˆé¢„è®¡20-30åˆ†é’Ÿï¼‰...")
        atoms = self.batch_analyzer.analyze(atoms)
        print(f"âœ“ åˆ†æå®Œæˆ")

        # Step 4: ç”Ÿæˆå‘é‡
        print("\n[4/6] ç”Ÿæˆè¯­ä¹‰å‘é‡...")
        atoms = self.embedder.generate_embeddings(atoms)
        self._save_json({"atoms": [a.dict() for a in atoms]},
                       os.path.join(video_output_dir, "analyzed_atoms.jsonl"))
        print(f"âœ“ å‘é‡ç”Ÿæˆå®Œæˆ")

        # Step 5: ç»“æ„åŒ–
        print("\n[5/6] æ„å»ºçŸ¥è¯†ç»“æ„...")
        entities = self.entity_builder.build_entities(atoms)
        topics = self.topic_builder.build_topics(atoms)
        narratives = self.narrative_identifier.identify_narratives(atoms)

        self._save_json({"entities": [e.dict() for e in entities]},
                       os.path.join(video_output_dir, "entities.json"))
        self._save_json({"topics": [t.dict() for t in topics]},
                       os.path.join(video_output_dir, "topics.json"))
        self._save_json({"narratives": [n.dict() for n in narratives]},
                       os.path.join(video_output_dir, "narratives.json"))
        print(f"âœ“ è¯†åˆ«{len(entities)}ä¸ªå®ä½“ï¼Œ{len(topics)}ä¸ªä¸»é¢˜ï¼Œ{len(narratives)}ä¸ªå™äº‹ç‰‡æ®µ")

        # Step 6: å»ºç«‹ç´¢å¼•
        print("\n[6/6] å»ºç«‹ç´¢å¼•...")
        # å‘é‡ç´¢å¼•
        vector_indexer = VectorIndexer(
            os.path.join(video_output_dir, "vector_db")
        )
        vector_indexer.build_index(atoms, entities, topics)

        # ç»“æ„åŒ–ç´¢å¼•
        struct_indexer = StructuredIndexer()
        indexes = struct_indexer.build_index(atoms)
        self._save_json(indexes,
                       os.path.join(video_output_dir, "indexes", "structured.json"))

        # çŸ¥è¯†å›¾è°±
        graph_builder = GraphBuilder()
        graph = graph_builder.build_graph(entities)
        self._save_json(graph,
                       os.path.join(video_output_dir, "indexes", "graph.json"))
        print(f"âœ“ ç´¢å¼•æ„å»ºå®Œæˆ")

        print("\n" + "="*60)
        print(f"âœ“ å¤„ç†å®Œæˆï¼çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {video_output_dir}")
        print("="*60)

        return video_output_dir

    def _save_json(self, data: dict, file_path: str):
        """ä¿å­˜JSONæ–‡ä»¶"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    engine = VideoUnderstandingEngine(
        claude_api_key="your_claude_key",
        openai_api_key="your_openai_key"
    )

    output_dir = engine.process(
        srt_file="D:/YouTube_Downloads/é‡‘ä¸‰è§’å¤§ä½¬4ï¼šç¼…åŒ—åŒé›„æ—¶ä»£1962-1998.srt",
        video_id="jinSanJiao_04"
    )

    print(f"\nçŸ¥è¯†åº“ä½ç½®: {output_dir}")
```

---

## 4. æ•°æ®æ¨¡å‹

æ‰€æœ‰æ•°æ®æ¨¡å‹éƒ½ä½¿ç”¨Pydanticå®šä¹‰ï¼Œç¡®ä¿ç±»å‹å®‰å…¨å’Œæ•°æ®éªŒè¯ã€‚

è¯¦è§ä»£ç ä¸­çš„`models/`ç›®å½•ã€‚

---

## 5. æ ¸å¿ƒç®—æ³•

### 5.1 è¯­ä¹‰è¾¹ç•Œè¯†åˆ«ç®—æ³•

**é—®é¢˜**: å¦‚ä½•åˆ¤æ–­å“ªäº›å¥å­åº”è¯¥åˆå¹¶æˆä¸€ä¸ªåŸå­ï¼Ÿ

**ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜**:
- å›ºå®šæ—¶é—´ï¼šç ´åè¯­ä¹‰
- æ ‡ç‚¹ç¬¦å·ï¼šå­—å¹•æ ‡ç‚¹ä¸å‡†
- å…³é”®è¯ï¼šæ— æ³•ç†è§£å¤æ‚è¯­å¢ƒ

**AIæ–¹æ³•**:
ä½¿ç”¨Claudeçš„å¼ºå¤§ç†è§£èƒ½åŠ›ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºè¯ï¼Œè®©AIè¯†åˆ«ï¼š
1. ä¸»é¢˜è½¬æ¢ç‚¹
2. é•¿æ—¶é—´åœé¡¿ï¼ˆé€šè¿‡æ—¶é—´æˆ³ï¼‰
3. å™äº‹æ¨¡å¼åˆ‡æ¢ï¼ˆå™è¿°â†’äº’åŠ¨â†’å™è¿°ï¼‰

### 5.2 å®Œæ•´ç‰‡æ®µè¯†åˆ«ç®—æ³•

**ç›®æ ‡**: è¯†åˆ«5-15åˆ†é’Ÿçš„"å®Œæ•´ç‰‡æ®µ"ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰

**æ–¹æ³•**: AIåˆ¤æ–­ + è§„åˆ™è¾…åŠ©

```python
def is_complete_segment(atom: Atom) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºå®Œæ•´ç‰‡æ®µ

    æ¡ä»¶ï¼š
    1. æ—¶é•¿ > 5åˆ†é’Ÿ
    2. AIæ ‡è®°ä¸º type="complete_segment"
    3. completeness="å®Œæ•´"
    4. ä¸»é¢˜ç»Ÿä¸€ï¼ˆtopic_scoresä¸­ä¸»å¯¼ä¸»é¢˜>8åˆ†ï¼‰
    """
    # è§„åˆ™1ï¼šæ—¶é•¿
    if atom.duration_ms < 300000:  # 5åˆ†é’Ÿ
        return False

    # è§„åˆ™2ï¼šAIæ ‡è®°
    if atom.type != "complete_segment":
        return False

    # è§„åˆ™3ï¼šå®Œæ•´æ€§
    if atom.completeness != "å®Œæ•´":
        return False

    # è§„åˆ™4ï¼šä¸»é¢˜ç»Ÿä¸€æ€§
    topic_scores = atom.topics.get('topic_scores', {})
    if not topic_scores:
        return False

    max_score = max(topic_scores.values())
    if max_score < 8:
        return False

    return True
```

---

## 6. æç¤ºè¯å·¥ç¨‹

æç¤ºè¯æ˜¯æ•´ä¸ªç³»ç»Ÿçš„**çµé­‚**ï¼Œç›´æ¥å†³å®šäº†ç†è§£è´¨é‡ã€‚

### 6.1 æç¤ºè¯è®¾è®¡åŸåˆ™

1. **æ˜ç¡®ä»»åŠ¡**: æ¸…æ™°è¯´æ˜è¦åšä»€ä¹ˆ
2. **æä¾›ç¤ºä¾‹**: Few-shot learning
3. **ç»“æ„åŒ–è¾“å‡º**: è¦æ±‚è¿”å›JSON
4. **é”™è¯¯å¤„ç†**: å‘Šè¯‰AIå¦‚ä½•å¤„ç†è¾¹ç•Œæƒ…å†µ

### 6.2 æç¤ºè¯è¿­ä»£ç­–ç•¥

```
Version 1.0: åŸºç¡€ç‰ˆæœ¬
    â†“ æµ‹è¯•
å‘ç°é—®é¢˜ï¼šè¯†åˆ«è¾¹ç•Œä¸å‡†ç¡®
    â†“
Version 1.1: å¢åŠ "é•¿æ—¶é—´åœé¡¿"è§„åˆ™
    â†“ æµ‹è¯•
å‘ç°é—®é¢˜ï¼šé—æ¼å®Œæ•´ç‰‡æ®µ
    â†“
Version 1.2: å¢åŠ "å®Œæ•´ç‰‡æ®µ"è¯†åˆ«
    â†“ æµ‹è¯•
æŒç»­ä¼˜åŒ–...
```

æ‰€æœ‰æç¤ºè¯éƒ½åº”è¯¥ç‰ˆæœ¬åŒ–ç®¡ç†ï¼Œä¾¿äºå›æ»šå’Œå¯¹æ¯”ã€‚

---

## 7. æ€§èƒ½ä¼˜åŒ–

### 7.1 å¹¶è¡Œå¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_analyze(atoms: List[Atom], max_workers: int = 5) -> List[Atom]:
    """å¹¶è¡Œåˆ†æå¤šä¸ªåŸå­"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(analyze_single_atom, atom) for atom in atoms]
        results = [f.result() for f in futures]
    return results
```

### 7.2 ç¼“å­˜æœºåˆ¶

```python
import hashlib
import pickle

class Cache:
    """ç»“æœç¼“å­˜"""

    def __init__(self, cache_dir: str = ".cache"):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, self._hash(key))
        if os.path.exists(cache_file):
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        return None

    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜"""
        cache_file = os.path.join(self.cache_dir, self._hash(key))
        with open(cache_file, 'wb') as f:
            pickle.dump(value, f)

    def _hash(self, key: str) -> str:
        return hashlib.md5(key.encode()).hexdigest()
```

### 7.3 Tokenä¼˜åŒ–

**ç­–ç•¥**:
1. ç²¾ç®€æç¤ºè¯ï¼Œåˆ é™¤å†—ä½™æè¿°
2. æ‰¹é‡å¤„ç†ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
3. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹å¤„ç†ç®€å•ä»»åŠ¡

---

## 8. é”™è¯¯å¤„ç†

### 8.1 APIé”™è¯¯å¤„ç†

```python
import time
from anthropic import APIError, RateLimitError

def call_claude_with_retry(prompt: str, max_retries: int = 3):
    """å¸¦é‡è¯•çš„APIè°ƒç”¨"""
    for attempt in range(max_retries):
        try:
            response = client.messages.create(...)
            return response

        except RateLimitError:
            # é™æµï¼šç­‰å¾…åé‡è¯•
            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
            print(f"âš ï¸ é™æµï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
            time.sleep(wait_time)

        except APIError as e:
            # APIé”™è¯¯ï¼šè®°å½•å¹¶é‡è¯•
            print(f"âš ï¸ APIé”™è¯¯: {e}")
            if attempt == max_retries - 1:
                raise
            time.sleep(1)

    raise Exception("é‡è¯•æ¬¡æ•°ç”¨å°½")
```

### 8.2 æ•°æ®éªŒè¯

ä½¿ç”¨Pydanticè‡ªåŠ¨éªŒè¯æ•°æ®æ ¼å¼ã€‚

### 8.3 è¿›åº¦ä¿å­˜

```python
def process_with_checkpoint(atoms: List[Atom], checkpoint_file: str):
    """å¸¦æ–­ç‚¹ç»­ä¼ çš„å¤„ç†"""
    # åŠ è½½checkpoint
    processed_ids = load_checkpoint(checkpoint_file)

    for i, atom in enumerate(atoms):
        if atom.atom_id in processed_ids:
            continue  # è·³è¿‡å·²å¤„ç†

        # å¤„ç†
        analyze_atom(atom)

        # ä¿å­˜checkpoint
        save_checkpoint(checkpoint_file, atom.atom_id)

        print(f"è¿›åº¦: {i+1}/{len(atoms)}")
```

---

## 9. æµ‹è¯•ç­–ç•¥

### 9.1 å•å…ƒæµ‹è¯•

```python
# tests/test_atomizer.py

def test_atomizer():
    """æµ‹è¯•åŸå­åŒ–æ¨¡å—"""
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    utterances = [
        Utterance(id=1, start_ms=0, end_ms=1000, text="æµ‹è¯•1"),
        Utterance(id=2, start_ms=1000, end_ms=2000, text="æµ‹è¯•2"),
    ]

    # æ‰§è¡Œ
    atomizer = Atomizer(api_key="test_key")
    atoms = atomizer.atomize(utterances)

    # æ–­è¨€
    assert len(atoms) > 0
    assert atoms[0].merged_text is not None
```

### 9.2 é›†æˆæµ‹è¯•

```python
def test_end_to_end():
    """ç«¯åˆ°ç«¯æµ‹è¯•"""
    engine = VideoUnderstandingEngine(...)
    output_dir = engine.process("test.srt", "test_video")

    # éªŒè¯è¾“å‡ºæ–‡ä»¶å­˜åœ¨
    assert os.path.exists(os.path.join(output_dir, "analyzed_atoms.jsonl"))
    assert os.path.exists(os.path.join(output_dir, "entities.json"))
```

### 9.3 å‡†ç¡®ç‡æµ‹è¯•

æ‰‹åŠ¨æ ‡æ³¨ä¸€éƒ¨åˆ†æ•°æ®ï¼Œæµ‹è¯•AIæ ‡æ³¨çš„å‡†ç¡®ç‡ã€‚

---

## 10. éƒ¨ç½²æ–¹æ¡ˆ

### 10.1 å¼€å‘ç¯å¢ƒ

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®APIå¯†é’¥
export CLAUDE_API_KEY="your_key"
export OPENAI_API_KEY="your_key"

# è¿è¡Œ
python engine.py
```

### 10.2 ç”Ÿäº§ç¯å¢ƒï¼ˆå¯é€‰ï¼‰

**DockeråŒ–**:

```dockerfile
FROM python:3.10

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "engine.py"]
```

**éƒ¨ç½²åˆ°æœåŠ¡å™¨**:
- AWS EC2 / é˜¿é‡Œäº‘ECS
- 8æ ¸16Gå†…å­˜
- ä½¿ç”¨é˜Ÿåˆ—ç³»ç»Ÿï¼ˆå¦‚Celeryï¼‰å¤„ç†ä»»åŠ¡

---

## 11. é™„å½•

### 11.1 å®Œæ•´ä¾èµ–

```txt
# requirements.txt

anthropic==0.18.0
openai==1.0.0
chromadb==0.4.0
srt==3.5.0
pydantic==2.0.0
networkx==3.1
pandas==2.0.0
rich==13.0.0
pytest==7.4.0
```

### 11.2 ç›®å½•ç»“æ„æ€»è§ˆ

```
video_understanding_engine/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py
â”œâ”€â”€ engine.py                    # ä¸»å¼•æ“
â”œâ”€â”€ parsers/                     # è§£ææ¨¡å—
â”œâ”€â”€ atomizers/                   # åŸå­åŒ–æ¨¡å—
â”œâ”€â”€ analyzers/                   # åˆ†ææ¨¡å—
â”œâ”€â”€ structurers/                 # ç»“æ„åŒ–æ¨¡å—
â”œâ”€â”€ vectorizers/                 # å‘é‡åŒ–æ¨¡å—
â”œâ”€â”€ indexers/                    # ç´¢å¼•æ¨¡å—
â”œâ”€â”€ prompts/                     # æç¤ºè¯åº“
â”œâ”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹
â””â”€â”€ tests/                       # æµ‹è¯•
```

---

**æ–‡æ¡£å®Œæˆ**

è¿™ä»½æŠ€æœ¯è®¾è®¡æ–‡æ¡£æ¶µç›–äº†è§†é¢‘ç†è§£å¼•æ“çš„æ‰€æœ‰å…³é”®éƒ¨åˆ†ã€‚åŸºäºè¿™ä»½æ–‡æ¡£ï¼Œå¼€å‘å›¢é˜Ÿå¯ä»¥ï¼š

1. âœ… ç†è§£æ•´ä½“æ¶æ„
2. âœ… å®ç°å„ä¸ªå­æ¨¡å—
3. âœ… è¿›è¡Œæµ‹è¯•å’Œä¼˜åŒ–
4. âœ… éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

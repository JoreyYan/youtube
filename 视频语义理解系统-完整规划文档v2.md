# 视频语义理解与智能二创系统 - 完整规划文档 v2.0

**文档版本**: v2.0
**创建日期**: 2025-10-01
**最后更新**: 2025-10-01
**状态**: 开发中

---

## 📋 目录

1. [产品概述](#1-产品概述)
2. [核心架构设计](#2-核心架构设计)
3. [数据模型定义](#3-数据模型定义)
4. [处理流程详解](#4-处理流程详解)
5. [技术实现方案](#5-技术实现方案)
6. [成本预算](#6-成本预算)
7. [开发路线图](#7-开发路线图)
8. [附录](#8-附录)

---

## 1. 产品概述

### 1.1 产品定位

**核心定义**: 基于AI的视频内容深度理解系统，能够将长视频（2小时+）自动分解为可检索、可重组的语义单元，支持跨视频的主题检索和智能二创。

**目标用户**:
- 视频二创作者（up主、自媒体）
- 内容运营团队
- 知识整理者

**核心价值**:
1. **深度理解**: 不只是字幕转文本，而是理解视频的主题、结构、逻辑
2. **智能检索**: 语义搜索，跨视频查找相关内容
3. **自动重组**: AI根据新主题自动组织素材，生成剪辑方案

### 1.2 典型使用场景

#### 场景1: 单视频深度理解
```
输入: 2小时金融历史讲解视频 + SRT字幕
处理: 自动分析，生成知识库
输出:
  - 30-50个叙事片段（每个3-15分钟）
  - 342个语义原子（每个10秒-3分钟）
  - 主题索引、人物索引、时间线
  - 可搜索、可重组
时间: 1-1.5小时
成本: $10-14
```

#### 场景2: 跨视频主题合集
```
需求: 从5个视频中提取所有"美元霸权"相关片段，制作20分钟专题
操作:
  1. 用户输入: "美元霸权的历史演变"
  2. AI搜索: 跨5个视频检索相关片段
  3. AI组织: 按时间线或逻辑重组
  4. AI生成: 剪辑方案（时间轴 + 转场文案）
输出: 完整的剪辑方案 + EDL文件
```

#### 场景3: 持续积累知识库
```
工作流:
  - 每周处理5-10个新视频
  - 自动加入统一知识库
  - 动态更新主题分类树
  - 动态更新实体图谱
  - 随时按主题检索任意内容
价值: 视频资产持续增值，构建个人/团队知识图谱
```

### 1.3 与竞品对比

| 维度 | 传统剪辑工具 | AI字幕工具 | **本系统** |
|------|------------|-----------|-----------|
| 内容理解 | ❌ 无 | ⚠️ 关键词级别 | ✅ 深度语义理解 |
| 结构识别 | ❌ 无 | ❌ 无 | ✅ 叙事片段自动识别 |
| 跨视频检索 | ❌ 不支持 | ❌ 不支持 | ✅ 统一知识库 |
| 智能重组 | ⚠️ 手动剪辑 | ⚠️ 简单拼接 | ✅ AI理解后重组 |
| 可扩展性 | ⚠️ 单视频 | ⚠️ 单视频 | ✅ 支持无限视频规模 |

---

## 2. 核心架构设计

### 2.1 数据层次架构（四层模型）

```
┌─────────────────────────────────────────────────────────┐
│ Level 3: 全局索引层                                       │
│ - 动态主题分类树                                          │
│ - 实体知识图谱                                            │
│ - 跨视频时间线                                            │
│ - 全局向量库                                              │
└─────────────────────────────────────────────────────────┘
                              ↑
                    (自动聚合，多视频后构建)
                              ↑
┌─────────────────────────────────────────────────────────┐
│ Level 2: 叙事片段层 (Narrative Segments) ⭐核心层        │
│ - 30-50个完整叙事单元                                     │
│ - 每个3-15分钟                                            │
│ - 完整语义标注 + 结构分析                                 │
│ - 可直接使用的素材单元                                    │
└─────────────────────────────────────────────────────────┘
                              ↑
                    (智能识别和聚合)
                              ↑
┌─────────────────────────────────────────────────────────┐
│ Level 1: 原子层 (Atoms)                                  │
│ - 342个语义原子                                           │
│ - 每个10秒-3分钟                                          │
│ - 轻量标注（关键词 + 向量）                               │
│ - 最小信息单元                                            │
└─────────────────────────────────────────────────────────┘
                              ↑
                    (AI智能合并)
                              ↑
┌─────────────────────────────────────────────────────────┐
│ Level 0: 字幕层 (Utterances)                             │
│ - 3580条原始字幕                                          │
│ - SRT解析后的结构化数据                                   │
└─────────────────────────────────────────────────────────┘
```

### 2.2 核心设计原则

#### 原则1: 分层标注，成本优化
- **Level 1 (原子)**: 轻量标注 - 所有342个原子
- **Level 2 (片段)**: 深度标注 - 只标注30-50个叙事片段
- **Level 3 (全局)**: 聚合索引 - 基于已标注内容自动生成

**理由**:
- 不浪费成本在闲聊、过场等低价值原子上
- 重点深度分析完整叙事片段
- 成本从$15-20降至$10-14

#### 原则2: 动态标签，无限扩展
- ❌ **不使用固定标签**: 避免100个视频后标签体系崩溃
- ✅ **AI自由提取**: 每个视频自由标注，保持原始性
- ✅ **动态聚类**: 多视频后自动构建分类树
- ✅ **语义向量为核心**: 即使标签不同，语义相似也能检索到

**理由**:
- 内容类型会不断演化（金融→科技→历史→...）
- 同一概念不同视频用词不同（"美元霸权" vs "美元主导地位"）
- 系统需要从1个视频扩展到10000个视频

#### 原则3: 语义向量 + 结构化索引 双驱动
- **语义向量**: 捕获深层语义，支持模糊检索
- **结构化索引**: 精确查询，支持复杂筛选
- **双重保障**: 互补优势，确保检索准确性

#### 原则4: 人机协作，AI辅助而非替代
- AI负责理解和组织
- 人负责创意和决策
- AI生成方案供人选择和调整

---

## 3. 数据模型定义

### 3.1 Level 0: Utterance (字幕)

**用途**: 底层数据，解析后不直接使用

```json
{
  "id": 1,
  "start_ms": 8933,
  "end_ms": 10400,
  "text": "开始了没有啊",
  "duration_ms": 1467,
  "start_time": "00:00:08,933",
  "end_time": "00:00:10,400"
}
```

**字段说明**:
- `id`: 字幕序号
- `start_ms/end_ms`: 毫秒级时间戳
- `text`: 字幕文本
- `duration_ms`: 持续时间
- `start_time/end_time`: SRT格式时间字符串

---

### 3.2 Level 1: Atom (原子)

**用途**: 最小语义单元，轻量标注

```json
{
  "atom_id": "A045",
  "start_ms": 145230,
  "end_ms": 162450,
  "duration_ms": 17220,
  "start_time": "00:02:25,230",
  "end_time": "00:02:42,450",
  "duration_seconds": 17.22,

  // ===== 基础信息 =====
  "merged_text": "举个例子来说，在1971年尼克松宣布美元脱钩黄金后...",
  "type": "陈述",  // 陈述 | 问题 | 回答 | 举例 | 总结 | 引用 | 过渡 | 互动
  "completeness": "完整",  // 完整 | 基本完整 | 不完整
  "source_utterance_ids": [234, 235, 236, 237, 238, 239],

  // ===== 轻量标注（所有原子都有）=====
  "quick_tags": ["美元", "1971年", "尼克松", "黄金"],  // 3-5个关键词
  "embedding": [0.023, 0.451, -0.234, ...]  // 3072维向量
}
```

**字段说明**:
- **基础信息**: 时间、文本、类型、完整性（已有）
- **quick_tags**: 快速提取的关键词，用于基础检索
- **embedding**: 语义向量，用于语义搜索（核心）

**生成方式**:
- `quick_tags`: AI快速提取或TF-IDF
- `embedding`: OpenAI text-embedding-3-large

**成本**: ~$2 (342个原子)

---

### 3.3 Level 2: Narrative Segment (叙事片段) ⭐核心

**用途**: 完整叙事单元，深度标注

```json
{
  "segment_id": "SEG_003",
  "title": "布雷顿森林体系的崩溃（1971）",
  "start_ms": 450000,
  "end_ms": 1350000,
  "duration_ms": 900000,  // 15分钟
  "start_time": "00:07:30",
  "end_time": "00:22:30",

  // ===== 组成原子 =====
  "atoms": ["A045", "A046", "A047", "A048", "A049", "A050",
            "A051", "A052", "A053", "A054"],  // 10个原子
  "atom_count": 10,

  // ===== 完整文本 =====
  "full_text": "1944年布雷顿森林会议...[完整15分钟的文本]",

  // ===== 叙事结构分析 =====
  "narrative_structure": {
    "type": "历史叙事",  // 历史叙事 | 理论解释 | 数据分析 | 案例说明 | 观点论述
    "structure": "背景铺垫 → 危机爆发 → 决策过程 → 结果影响",

    "acts": [
      {
        "act": "背景铺垫",
        "atoms": ["A045", "A046"],
        "duration": "3:30",
        "summary": "二战后美国建立布雷顿森林体系，美元与黄金挂钩"
      },
      {
        "act": "危机爆发",
        "atoms": ["A047", "A048", "A049"],
        "duration": "5:00",
        "summary": "1960年代美国黄金储备流失，体系面临崩溃"
      },
      {
        "act": "决策过程",
        "atoms": ["A050", "A051"],
        "duration": "3:30",
        "summary": "尼克松团队秘密讨论，最终决定脱钩"
      },
      {
        "act": "结果影响",
        "atoms": ["A052", "A053", "A054"],
        "duration": "3:00",
        "summary": "1971年8月15日宣布，全球震惊，美元信用危机"
      }
    ]
  },

  // ===== 主题标注（自由标签）=====
  "topics": {
    "free_tags": [
      "布雷顿森林体系",
      "1971年尼克松冲击",
      "美元黄金脱钩",
      "国际货币体系变革",
      "货币主权",
      "金本位制度",
      "美元信用货币化"
    ],
    "tag_count": 7
  },

  // ===== 实体提取（自由提取）=====
  "entities": {
    "persons": ["尼克松", "戴高乐", "保罗·沃尔克"],
    "countries": ["美国", "法国", "德国"],
    "organizations": ["美联储", "IMF", "世界银行"],
    "locations": ["华盛顿", "巴黎"],
    "time_points": ["1944年", "1960年代", "1971年8月15日"],
    "events": [
      "布雷顿森林协议签署",
      "法国要求兑换黄金",
      "尼克松关闭黄金窗口"
    ],
    "concepts": [
      "金本位",
      "美元霸权",
      "国际货币体系",
      "信用货币",
      "货币主权"
    ]
  },

  // ===== 内容分面 =====
  "content_facet": {
    "type": "历史叙述",  // 历史叙述 | 观点评论 | 数据分析 | 案例说明 | 互动回应
    "aspect": "历史事件全景",  // 成因 | 影响 | 应对 | 对比 | 历史事件全景
    "stance": "批判性分析",  // 中立客观 | 批判性 | 支持性 | 讽刺性
    "perspective": "从美国霸权角度"
  },

  // ===== AI深度分析 =====
  "ai_analysis": {
    "core_argument": "布雷顿森林体系的崩溃标志着美国从'黄金美元'转向'信用美元'，是美元霸权的重要转折点",

    "key_insights": [
      "尼克松的决定是被迫的，而非主动的战略调整",
      "此事件为后续石油美元体系埋下伏笔",
      "展现了美国如何通过制度创新维持霸权"
    ],

    "logical_flow": "因果链条：二战后建立体系 → 美国过度发行美元 → 黄金储备不足 → 被迫脱钩 → 寻找新锚点（石油）",

    "related_themes": ["美元霸权", "石油美元", "金融霸权"],

    "crossref_segments": ["SEG_007"]  // 相关片段（如"石油美元的建立"）
  },

  // ===== 完整性评估 =====
  "completeness": {
    "is_complete": true,
    "can_standalone": true,  // 可以单独成片
    "missing_context": [],
    "recommend_additions": ["可补充后续石油美元的建立过程"]
  },

  // ===== 语义向量 =====
  "embedding": [0.123, 0.456, ...]  // 基于完整15分钟文本的向量
}
```

**字段说明**:
- **叙事结构**: 多幕结构，每幕有明确功能
- **自由标签**: AI自由提取，不受限制
- **实体提取**: 6类实体，自由提取
- **内容分面**: 理解"这段在讲什么角度"
- **AI分析**: 深度理解，提取核心论点和逻辑
- **完整性**: 评估是否可独立使用

**生成方式**: Claude API深度分析

**成本**: 30-50个片段 × $0.15 = $5-8

---

### 3.4 Level 3: Global Indexes (全局索引)

**用途**: 跨视频检索和理解

#### 3.4.1 动态主题分类树

```json
{
  "taxonomy_version": "v2024-10-01",
  "total_videos": 100,
  "total_segments": 3000,
  "last_updated": "2024-10-01T12:00:00Z",

  "themes": {
    "美元霸权体系": {
      "theme_id": "theme_001",
      "canonical_name": "美元霸权体系",

      // 标签变体（语义聚类结果）
      "variants": [
        "美元霸权",
        "美元主导地位",
        "美国金融霸权",
        "美元国际货币地位",
        "dollar hegemony"
      ],

      "segment_count": 145,
      "total_duration_minutes": 780,
      "segments": ["SEG_003", "SEG_045", "SEG_089", ...],

      // 子主题
      "sub_themes": {
        "布雷顿森林体系": {
          "theme_id": "theme_001_001",
          "variants": ["布雷顿森林", "战后货币体系", "Bretton Woods"],
          "segment_count": 34,
          "time_span": ["1944年", "1971年"],
          "segments": ["SEG_003", ...]
        },
        "石油美元": {
          "theme_id": "theme_001_002",
          "variants": ["石油美元", "石油-美元体系", "petrodollar"],
          "segment_count": 28,
          "time_span": ["1973年", "现在"],
          "segments": ["SEG_007", ...]
        }
      },

      // AI生成的主题摘要
      "ai_summary": "美元霸权是指美国通过美元的国际货币地位，在全球金融体系中占据主导地位。主要经历三个阶段：金本位时代（1944-1971）、石油美元时代（1973-2000s）、信用货币时代（2000s至今）。",

      "embedding": [0.234, 0.567, ...]
    },

    "中美关系": {
      "theme_id": "theme_002",
      "canonical_name": "中美关系",
      "variants": ["中美关系", "中美博弈", "中美竞争", "sino-us relations"],
      "segment_count": 89,
      "segments": [...],
      "sub_themes": {
        "贸易战": {...},
        "科技竞争": {...},
        "台海问题": {...}
      }
    }
  }
}
```

#### 3.4.2 实体知识图谱

```json
{
  "entity_graph_version": "v2024-10-01",
  "total_entities": 450,

  "entities": {
    "尼克松": {
      "entity_id": "person_nixon",
      "entity_type": "person",
      "canonical_name": "尼克松",
      "variants": ["尼克松", "理查德·尼克松", "Nixon", "Richard Nixon"],

      "mention_count": 45,
      "segments": ["SEG_003", "SEG_007", "SEG_045", ...],

      "profile": {
        "basic_info": "美国第37任总统（1969-1974）",
        "key_roles": ["美国总统", "布雷顿森林体系终结者"],
        "key_events": [
          {
            "event": "关闭黄金窗口",
            "date": "1971-08-15",
            "segments": ["SEG_003"]
          },
          {
            "event": "访华",
            "date": "1972-02",
            "segments": ["SEG_045"]
          }
        ],
        "related_themes": ["美元霸权", "中美关系", "冷战"],
        "ai_summary": "尼克松在视频中主要被讨论的是其1971年关闭黄金窗口的决定，这被认为是美元霸权转型的关键节点..."
      },

      "relationships": [
        {
          "target_entity": "person_kissinger",
          "relation": "政治伙伴",
          "description": "基辛格是尼克松的国家安全顾问和国务卿",
          "segments": ["SEG_003", "SEG_045"]
        }
      ],

      "embedding": [0.345, 0.678, ...]
    },

    "美联储": {
      "entity_id": "org_federal_reserve",
      "entity_type": "organization",
      "canonical_name": "美联储",
      "variants": ["美联储", "美国联邦储备系统", "Federal Reserve", "Fed"],
      "mention_count": 78,
      "segments": [...],
      "profile": {...}
    }
  },

  "relationships": [
    {
      "source": "person_nixon",
      "target": "org_federal_reserve",
      "relation": "政策制定者",
      "strength": 0.85,
      "segments": ["SEG_003", ...]
    }
  ]
}
```

#### 3.4.3 全局时间线

```json
{
  "timeline_version": "v2024-10-01",
  "time_span": ["1944年", "2024年"],

  "timeline": [
    {
      "year": "1944",
      "events": [
        {
          "event_name": "布雷顿森林协议签署",
          "date": "1944-07",
          "segments": ["SEG_002", "SEG_003"],
          "related_entities": ["person_fdr", "org_imf"],
          "related_themes": ["美元霸权", "国际货币体系"],
          "total_duration_minutes": 45
        }
      ]
    },
    {
      "year": "1971",
      "events": [
        {
          "event_name": "尼克松关闭黄金窗口",
          "date": "1971-08-15",
          "segments": ["SEG_003", "SEG_004"],
          "related_entities": ["person_nixon"],
          "related_themes": ["美元霸权", "布雷顿森林体系"],
          "significance": "美元从金本位转向信用货币的标志性事件",
          "total_duration_minutes": 65
        }
      ]
    }
  ]
}
```

---

## 4. 处理流程详解

### 4.1 单视频处理流程

```
输入: video.srt (2小时视频字幕)
    ↓
【Phase 1: 解析和原子化】(30-40分钟)
    ├─ Step 1.1: 解析SRT → 3580条字幕
    ├─ Step 1.2: AI智能合并 → 342个原子
    └─ Step 1.3: 轻量标注 → 关键词 + embedding
    成本: $2
    输出: atoms.json
    ↓
【Phase 2: 识别叙事片段】(10-15分钟)
    ├─ Step 2.1: 规则初筛 → 60-80个候选片段
    ├─ Step 2.2: AI精炼 → 30-50个叙事片段
    └─ Step 2.3: 生成片段元数据
    成本: $2-3
    输出: segments_meta.json
    ↓
【Phase 3: 深度标注片段】(20-30分钟)
    ├─ Step 3.1: 叙事结构分析
    ├─ Step 3.2: 主题标注（自由标签）
    ├─ Step 3.3: 实体提取（6类）
    ├─ Step 3.4: 内容分面标注
    ├─ Step 3.5: AI深度分析
    └─ Step 3.6: 生成embedding
    成本: $5-8
    输出: segments_full.json
    ↓
【Phase 4: 建立索引】(5-10分钟)
    ├─ Step 4.1: 向量索引（Chromadb）
    ├─ Step 4.2: 结构化索引（by_time/by_entity/by_theme）
    └─ Step 4.3: 生成检索接口
    成本: $0
    输出: vector_db/ + indexes.json
    ↓
完成: 知识库就绪
总时间: 1-1.5小时
总成本: $10-14
```

### 4.2 多视频处理流程

```
前提: 已处理10-100个视频
    ↓
【Phase 5: 全局索引构建】(一次性)
    ├─ Step 5.1: 收集所有片段标签
    ├─ Step 5.2: 向量聚类 → 识别同义标签
    ├─ Step 5.3: 构建动态分类树
    ├─ Step 5.4: 构建实体图谱
    ├─ Step 5.5: 构建全局时间线
    └─ Step 5.6: 建立统一向量库
    成本: $5-10（首次），$1-2（增量更新）
    输出:
      - taxonomy.json（分类树）
      - entity_graph.json（实体图谱）
      - timeline.json（时间线）
      - unified_vector_db/（统一向量库）
    ↓
完成: 跨视频检索系统就绪
```

### 4.3 AI二创工作流

```
用户需求: "我想做20分钟视频，讲美元霸权的历史演变"
    ↓
【Step 1: 智能检索】
    ├─ 语义搜索: 在统一向量库中搜索
    ├─ 标签匹配: 在分类树中查找"美元霸权"及其变体
    └─ 合并结果: 去重排序
    返回: 20个相关叙事片段（共3小时素材）
    ↓
【Step 2: AI分析和推荐】
    ├─ 分析片段内容和结构
    ├─ 识别核心论点和逻辑
    └─ 提出多个方案
    返回:
      - 方案A: 按时间线组织（布雷顿森林 → 石油美元 → 现状）
      - 方案B: 按三幕剧组织（起源 → 发展 → 挑战）
      - 方案C: 按对比组织（美元 vs 其他货币）
    ↓
【Step 3: 用户选择和调整】
    用户: "选方案A，但太长了，压缩到20分钟"
    ↓
【Step 4: AI生成剪辑方案】
    ├─ 精选片段: 从3小时压缩到20分钟
    ├─ 设计转场: 生成连接文案
    ├─ 生成时间轴: EDL格式
    └─ 生成脚本: ffmpeg命令
    成本: $0.5-1
    输出: edit_plan.json
    ↓
【Step 5: 执行剪辑】
    ├─ 方式1: 手动在PR/Final Cut中按方案剪辑
    └─ 方式2: 自动化（ffmpeg执行）
    ↓
完成: 20分钟成片
```

---

## 5. 技术实现方案

### 5.1 技术栈

| 层级 | 技术选型 | 说明 |
|------|---------|------|
| **AI理解** | Claude 3.5 Sonnet | 语义分析、结构识别 |
| **AI理解** | Claude 4.0 Sonnet | 深度分析、方案生成 |
| **语义向量** | OpenAI text-embedding-3-large | 3072维，准确度高 |
| **向量数据库** | Chromadb | 开源、简单、性能好 |
| **编程语言** | Python 3.10+ | AI生态完善 |
| **数据存储** | JSON文件 | 结构化，易读易改 |
| **前端** | Next.js 15 + TypeScript | 已实现基础可视化 |

### 5.2 核心模块设计

#### 模块1: 原子化引擎 (Atomizer)

**文件**: `atomizers/atomizer.py`

**功能**: 将字幕合并成语义原子

**关键代码**:
```python
class Atomizer:
    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)
        self.prompt = load_prompt('prompts/atomize_v2.txt')

    def atomize(self, utterances: List[Utterance]) -> List[Atom]:
        """
        原子化处理
        策略: 每次处理50条字幕（约2-3分钟）
        """
        atoms = []
        batch_size = 50

        for i in range(0, len(utterances), batch_size):
            batch = utterances[i:i+batch_size]
            batch_atoms = self._process_batch(batch)
            atoms.extend(batch_atoms)

        return atoms

    def _process_batch(self, batch: List[Utterance]) -> List[Atom]:
        # 构建输入
        input_text = "\n".join([
            f"[{utt.start_time}] {utt.text}"
            for utt in batch
        ])

        # 调用Claude
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            temperature=0,
            messages=[{
                "role": "user",
                "content": self.prompt.format(input_text=input_text)
            }]
        )

        # 解析JSON
        atoms_data = json.loads(response.content[0].text)
        return [Atom(**data) for data in atoms_data]
```

**Prompt设计**: 见附录A.1

---

#### 模块2: 快速标注器 (QuickTagger)

**文件**: `taggers/quick_tagger.py`

**功能**: 为所有原子生成关键词标签

**关键代码**:
```python
class QuickTagger:
    def tag_atoms(self, atoms: List[Atom]) -> List[Atom]:
        """
        批量生成关键词标签
        策略: 简单prompt，快速提取
        """
        for atom in atoms:
            atom.quick_tags = self._extract_keywords(atom.merged_text)

        return atoms

    def _extract_keywords(self, text: str) -> List[str]:
        prompt = f"""
从这段文本中提取3-5个关键词：
{text}

只输出JSON: {{"keywords": [...]}}
"""
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=100,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )

        result = json.loads(response.content[0].text)
        return result['keywords']
```

---

#### 模块3: 片段识别器 (SegmentIdentifier)

**文件**: `segmenters/segment_identifier.py`

**功能**: 识别叙事片段

**关键代码**:
```python
class SegmentIdentifier:
    def identify_segments(self, atoms: List[Atom]) -> List[SegmentMeta]:
        """
        两步识别：规则初筛 + AI精炼
        """
        # Step 1: 规则初筛
        candidates = self._rule_based_identify(atoms)

        # Step 2: AI精炼
        segments = self._ai_refine(candidates, atoms)

        return segments

    def _rule_based_identify(self, atoms: List[Atom]) -> List[List[str]]:
        """基于规则识别候选片段"""
        segments = []
        current_segment = []

        for i, atom in enumerate(atoms):
            current_segment.append(atom.atom_id)

            should_break = False

            # 条件1: 间隔>30秒
            if i < len(atoms) - 1:
                gap = atoms[i+1].start_ms - atom.end_ms
                if gap > 30000:
                    should_break = True

            # 条件2: 类型突变
            if i < len(atoms) - 1:
                if atom.type == '陈述' and atoms[i+1].type == '回应弹幕':
                    should_break = True

            # 条件3: 时长>15分钟
            if len(current_segment) > 0:
                start = next(a for a in atoms if a.atom_id == current_segment[0])
                duration = atom.end_ms - start.start_ms
                if duration > 900000:
                    should_break = True

            if should_break and len(current_segment) >= 3:
                segments.append(current_segment)
                current_segment = []

        return segments

    def _ai_refine(self, candidates: List[List[str]],
                   atoms: List[Atom]) -> List[SegmentMeta]:
        """AI精炼和验证"""
        # 准备原子摘要
        atoms_summary = [
            {
                "atom_id": a.atom_id,
                "time": f"{a.start_time}-{a.end_time}",
                "text_preview": a.merged_text[:100],
                "type": a.type,
                "tags": a.quick_tags
            }
            for a in atoms
        ]

        prompt = f"""
基于以下原子列表和候选片段，识别真正的"叙事片段"。

【原子列表】
{json.dumps(atoms_summary, ensure_ascii=False)}

【候选片段】
{json.dumps(candidates, ensure_ascii=False)}

【任务】
验证和精炼候选片段，只保留真正的叙事片段。

【标准】
1. 主题连贯
2. 时间连续
3. 逻辑完整
4. 时长3-15分钟

【输出JSON】
[
  {{
    "segment_id": "SEG_001",
    "title": "片段标题",
    "atoms": ["A001", "A002", ...],
    "reason": "为什么是一个片段"
  }}
]
"""

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=8000,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )

        segments_meta = json.loads(response.content[0].text)
        return [SegmentMeta(**s) for s in segments_meta]
```

---

#### 模块4: 深度分析器 (DeepAnalyzer)

**文件**: `analyzers/deep_analyzer.py`

**功能**: 深度标注叙事片段

**关键代码**:
```python
class DeepAnalyzer:
    def analyze_segment(self, segment_meta: SegmentMeta,
                       atoms: List[Atom]) -> NarrativeSegment:
        """
        深度分析一个叙事片段
        """
        # 获取片段的完整文本
        segment_atoms = [a for a in atoms if a.atom_id in segment_meta.atoms]
        full_text = " ".join([a.merged_text for a in segment_atoms])

        # 调用综合分析prompt
        prompt = self._build_comprehensive_prompt(full_text)

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=8000,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )

        analysis = json.loads(response.content[0].text)

        # 生成embedding
        embedding = self.embedder.generate(full_text)

        # 构建完整片段对象
        segment = NarrativeSegment(
            segment_id=segment_meta.segment_id,
            title=segment_meta.title,
            atoms=segment_meta.atoms,
            full_text=full_text,
            narrative_structure=analysis['narrative_structure'],
            topics=analysis['topics'],
            entities=analysis['entities'],
            content_facet=analysis['content_facet'],
            ai_analysis=analysis['ai_analysis'],
            embedding=embedding
        )

        return segment

    def _build_comprehensive_prompt(self, text: str) -> str:
        """构建综合分析prompt"""
        return f"""
深度分析这段金融/历史/政治类视频内容（15分钟）。

【文本】
{text}

【分析任务】

1. 叙事结构分析
   - 类型: 历史叙事 | 理论解释 | 数据分析 | 案例说明 | 观点论述
   - 结构: 描述叙事的展开方式（如：背景→危机→决策→结果）
   - 分幕: 把内容分成2-4幕，每幕有明确作用

2. 主题标注（自由提取）
   - 提取5-10个描述性标签
   - 标签要准确、具体
   - 优先用原文关键词
   - 不限制标签范围

3. 实体提取
   - persons: 人物
   - countries: 国家/地区
   - organizations: 组织机构
   - locations: 地点
   - time_points: 时间点
   - events: 重大事件
   - concepts: 关键概念

4. 内容分面
   - type: 历史叙述 | 观点评论 | 数据分析 | 案例说明 | 互动回应
   - aspect: 成因 | 影响 | 应对 | 对比 | 历史事件全景
   - stance: 中立客观 | 批判性 | 支持性 | 讽刺性
   - perspective: 从什么角度看问题

5. AI深度分析
   - core_argument: 这段的核心论点
   - key_insights: 3-5个关键洞察
   - logical_flow: 逻辑展开方式
   - related_themes: 相关主题

【输出JSON】
{{
  "narrative_structure": {{...}},
  "topics": {{...}},
  "entities": {{...}},
  "content_facet": {{...}},
  "ai_analysis": {{...}}
}}

【重要】
- 只输出JSON
- 标签和实体不限制范围，自由提取
- 分析要深入，不要泛泛而谈
"""
```

**Prompt设计**: 见附录A.2

---

#### 模块5: 向量索引器 (VectorIndexer)

**文件**: `indexers/vector_indexer.py`

**功能**: 建立向量索引

**关键代码**:
```python
import chromadb

class VectorIndexer:
    def __init__(self, persist_dir: str):
        self.client = chromadb.PersistentClient(path=persist_dir)

    def build_index(self, atoms: List[Atom],
                   segments: List[NarrativeSegment]):
        """
        构建向量索引
        """
        # 创建collection
        collection = self.client.create_collection(
            name="video_knowledge",
            metadata={"description": "视频知识库"}
        )

        # 添加原子
        print("索引原子...")
        for atom in atoms:
            collection.add(
                ids=[atom.atom_id],
                embeddings=[atom.embedding],
                metadatas=[{
                    "type": "atom",
                    "start_time": atom.start_time,
                    "duration": atom.duration_seconds,
                    "tags": ",".join(atom.quick_tags)
                }],
                documents=[atom.merged_text]
            )

        # 添加片段
        print("索引叙事片段...")
        for segment in segments:
            collection.add(
                ids=[segment.segment_id],
                embeddings=[segment.embedding],
                metadatas=[{
                    "type": "narrative_segment",
                    "title": segment.title,
                    "duration": (segment.end_ms - segment.start_ms) / 1000,
                    "tags": ",".join(segment.topics.free_tags),
                    "entities": ",".join(
                        segment.entities.persons +
                        segment.entities.countries +
                        segment.entities.organizations
                    )
                }],
                documents=[segment.full_text]
            )

        print(f"✓ 索引完成，共{collection.count()}条")
```

---

#### 模块6: 全局索引构建器 (GlobalIndexBuilder)

**文件**: `indexers/global_index_builder.py`

**功能**: 构建跨视频全局索引

**关键代码**:
```python
class GlobalIndexBuilder:
    def build_taxonomy(self, all_segments: List[NarrativeSegment]) -> Taxonomy:
        """
        构建动态主题分类树
        """
        # Step 1: 收集所有标签
        all_tags = []
        for segment in all_segments:
            for tag in segment.topics.free_tags:
                all_tags.append({
                    'tag': tag,
                    'segment_id': segment.segment_id,
                    'embedding': segment.embedding
                })

        # Step 2: 向量聚类
        clusters = self._cluster_tags_by_embedding(all_tags)

        # Step 3: 为每个cluster生成canonical name
        taxonomy = {}
        for cluster in clusters:
            canonical_name = self._ai_generate_canonical_name(cluster['tags'])

            taxonomy[canonical_name] = {
                'variants': cluster['tags'],
                'segment_count': len(cluster['segments']),
                'segments': cluster['segments']
            }

        return taxonomy

    def _cluster_tags_by_embedding(self, tags: List[dict]) -> List[dict]:
        """基于embedding聚类标签"""
        from sklearn.cluster import DBSCAN
        import numpy as np

        # 提取embedding矩阵
        embeddings = np.array([t['embedding'] for t in tags])

        # DBSCAN聚类
        clustering = DBSCAN(eps=0.15, min_samples=3, metric='cosine')
        labels = clustering.fit_predict(embeddings)

        # 组织聚类结果
        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = {
                    'tags': set(),
                    'segments': set()
                }
            clusters[label]['tags'].add(tags[i]['tag'])
            clusters[label]['segments'].add(tags[i]['segment_id'])

        return [
            {
                'tags': list(c['tags']),
                'segments': list(c['segments'])
            }
            for c in clusters.values()
        ]

    def _ai_generate_canonical_name(self, tags: List[str]) -> str:
        """让AI为聚类生成统一名称"""
        prompt = f"""
以下是语义相似的标签变体：
{json.dumps(tags, ensure_ascii=False)}

请选择或生成一个最佳的统一名称（canonical name）。

要求：
- 准确概括所有变体的含义
- 简洁明了
- 优先选择最常见的说法

只输出名称，不要解释。
"""

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=50,
            temperature=0,
            messages=[{"role": "user", "content": prompt}]
        )

        return response.content[0].text.strip()
```

---

### 5.3 主引擎类

**文件**: `engine.py`

**功能**: 协调所有模块

**关键代码**:
```python
class VideoUnderstandingEngine:
    def __init__(self, claude_key: str, openai_key: str, output_dir: str):
        self.claude_key = claude_key
        self.openai_key = openai_key
        self.output_dir = output_dir

        # 初始化模块
        self.parser = SRTParser()
        self.atomizer = Atomizer(claude_key)
        self.quick_tagger = QuickTagger(claude_key)
        self.embedder = Embedder(openai_key)
        self.segment_identifier = SegmentIdentifier(claude_key)
        self.deep_analyzer = DeepAnalyzer(claude_key, openai_key)
        self.vector_indexer = VectorIndexer(output_dir)

    def process_video(self, srt_file: str, video_id: str) -> str:
        """
        处理单个视频
        """
        print("="*60)
        print(f"处理视频: {video_id}")
        print("="*60)

        video_dir = os.path.join(self.output_dir, video_id)
        os.makedirs(video_dir, exist_ok=True)

        # Phase 1: 原子化
        print("\n[Phase 1/4] 原子化处理...")
        utterances = self.parser.parse(srt_file)
        atoms = self.atomizer.atomize(utterances)
        atoms = self.quick_tagger.tag_atoms(atoms)
        atoms = self.embedder.embed_atoms(atoms)
        save_json(atoms, f"{video_dir}/atoms.json")
        print(f"✓ 生成{len(atoms)}个原子")

        # Phase 2: 识别片段
        print("\n[Phase 2/4] 识别叙事片段...")
        segment_metas = self.segment_identifier.identify_segments(atoms)
        save_json(segment_metas, f"{video_dir}/segments_meta.json")
        print(f"✓ 识别{len(segment_metas)}个叙事片段")

        # Phase 3: 深度标注
        print("\n[Phase 3/4] 深度标注片段...")
        segments = []
        for meta in segment_metas:
            segment = self.deep_analyzer.analyze_segment(meta, atoms)
            segments.append(segment)
        save_json(segments, f"{video_dir}/segments.json")
        print(f"✓ 深度标注完成")

        # Phase 4: 建立索引
        print("\n[Phase 4/4] 建立索引...")
        self.vector_indexer.build_index(atoms, segments)
        print(f"✓ 索引完成")

        print("\n" + "="*60)
        print(f"✓ 处理完成！输出目录: {video_dir}")
        print("="*60)

        return video_dir
```

---

## 6. 成本预算

### 6.1 单视频处理成本

**假设**: 2小时视频，约3500条字幕

| 阶段 | 操作 | API调用 | 成本估算 |
|------|------|--------|----------|
| Phase 1 | 原子化 | Claude (342次小batch) | $2 |
| Phase 1 | 快速标注 | Claude (342次快速) | $1 |
| Phase 1 | 生成embedding | OpenAI (342个原子) | $1 |
| Phase 2 | 识别片段 | Claude (1次大context) | $2 |
| Phase 3 | 深度标注 | Claude (30-50次中等) | $6 |
| Phase 3 | 片段embedding | OpenAI (30-50个片段) | $0.5 |
| **总计** | - | - | **$12.5** |

**对比**:
- 之前方案（标注所有342个原子）: $15-20
- 当前方案（只标注30-50个片段）: $12.5
- **节省**: ~30%

### 6.2 多视频增量成本

**假设**: 已有100个视频

| 操作 | 频率 | 成本 |
|------|------|------|
| 处理新视频 | 每个 | $12.5 |
| 全局索引重建 | 每10个新视频 | $2 |
| **月度成本（20个新视频）** | - | **$254** |

### 6.3 AI二创成本

| 操作 | 成本 |
|------|------|
| 检索和匹配 | $0 (基于向量库) |
| 方案生成 | $0.5-1 |
| 方案调整（每次） | $0.3-0.5 |
| **单次二创总成本** | **$0.8-1.5** |

---

## 7. 开发路线图

### 7.1 Phase 1: 核心引擎开发（Week 1-2）

**目标**: 实现单视频处理完整流程

**任务清单**:
- [x] ✅ 基础原子化（已完成）
- [ ] ⏳ 完善Prompt（原子化 + 快速标注）
- [ ] ⏳ 实现片段识别器
- [ ] ⏳ 实现深度分析器
- [ ] ⏳ 实现向量索引器
- [ ] ⏳ 集成主引擎
- [ ] ⏳ 重新处理测试视频
- [ ] ⏳ 验证结果质量

**交付物**:
- `engine.py` (主引擎)
- `prompts/` (所有Prompt)
- `data/output/video_001/` (完整处理结果)
  - atoms.json
  - segments.json
  - vector_db/

**验收标准**:
- 能成功处理2小时视频
- 生成30-50个叙事片段
- 每个片段有完整标注
- 向量检索可用

---

### 7.2 Phase 2: 检索系统开发（Week 3）

**目标**: 实现智能检索功能

**任务清单**:
- [ ] ⏳ 实现语义搜索
- [ ] ⏳ 实现结构化查询
- [ ] ⏳ 实现混合检索
- [ ] ⏳ 构建查询API
- [ ] ⏳ CLI测试工具

**交付物**:
- `searchers/semantic_searcher.py`
- `searchers/structured_searcher.py`
- `searchers/hybrid_searcher.py`
- `cli/search.py` (测试工具)

**验收标准**:
- 语义搜索准确率 >80%
- 混合检索覆盖率 >95%
- 响应时间 <1秒

---

### 7.3 Phase 3: 多视频支持（Week 4）

**目标**: 支持多视频管理和全局索引

**任务清单**:
- [ ] ⏳ 实现全局索引构建器
- [ ] ⏳ 动态分类树算法
- [ ] ⏳ 实体图谱构建
- [ ] ⏳ 全局时间线生成
- [ ] ⏳ 统一向量库
- [ ] ⏳ 跨视频检索

**交付物**:
- `indexers/global_index_builder.py`
- `data/global/` (全局索引)
  - taxonomy.json
  - entity_graph.json
  - timeline.json

**验收标准**:
- 支持100个视频
- 全局检索准确
- 动态分类树合理

---

### 7.4 Phase 4: AI二创功能（Week 5-6）

**目标**: 实现AI辅助剪辑方案生成

**任务清单**:
- [ ] ⏳ 实现方案生成器
- [ ] ⏳ 多轮对话系统
- [ ] ⏳ EDL格式生成
- [ ] ⏳ CLI交互界面
- [ ] ⏳ 方案模板库

**交付物**:
- `generators/plan_generator.py`
- `generators/edl_generator.py`
- `cli/creative.py`

**验收标准**:
- 能生成可执行方案
- 支持多轮调整
- EDL格式正确

---

### 7.5 Phase 5: 前端集成（Week 7-8）

**目标**: Web界面

**任务清单**:
- [ ] ⏳ 后端API开发（FastAPI）
- [ ] ⏳ 前端检索界面
- [ ] ⏳ 片段预览播放器
- [ ] ⏳ AI对话界面
- [ ] ⏳ 方案生成界面

**交付物**:
- `api/` (后端API)
- `atom-viewer/` (前端扩展)

**验收标准**:
- 完整的用户工作流
- 响应流畅
- 可部署

---

## 8. 附录

### 附录A: Prompt模板

#### A.1 原子化Prompt

**文件**: `prompts/atomize_v2.txt`

```
你是视频内容分析专家。你的任务是将视频字幕合并成"语义完整的信息单元"（原子）。

【核心规则】

1. 什么是"原子"？
   - 表达一个完整意思的最小单元
   - 可以是一句话、一个观点、一次互动
   - 时长通常10秒-3分钟

2. 合并边界判断：
   - 主题转换 → 新原子
   - 长时间停顿（>5秒）→ 新原子
   - 从陈述转到互动 → 新原子
   - 从一个事件转到另一个事件 → 新原子

3. 类型分类：
   - 陈述: 讲述事实、阐述观点、介绍概念
   - 问题: 提出问题、设问
   - 回答: 回答问题
   - 举例: 举例说明
   - 总结: 总结归纳
   - 引用: 引用他人观点
   - 过渡: 话题转换、承上启下
   - 互动: 与观众互动、回应弹幕

4. 完整性评级：
   - 完整: 可独立理解，不需上下文
   - 基本完整: 大致完整，缺少少量上下文
   - 不完整: 依赖上下文

【输入格式】
[时间] 文本

【输出格式】
返回JSON数组：
[
  {
    "atom_id": "A001",
    "start_ms": 起始毫秒,
    "end_ms": 结束毫秒,
    "duration_ms": 持续毫秒,
    "merged_text": "合并后的文本",
    "type": "陈述",
    "completeness": "完整",
    "source_utterance_ids": [1, 2, 3]
  }
]

【重要】
- 只输出JSON，不要解释
- 确保时间连续，不重叠
- atom_id格式: A001, A002, ...
- merged_text是原文拼接，保持原样

【示例输入】
[00:08:20] 1962年
[00:08:25] 国民党残军撤到金三角
[00:08:30] 这是整个金三角问题的起源
[00:08:38] hello 海绵宝宝
[00:08:40] 然后呢坤沙就是在这个背景下崛起的

【示例输出】
[
  {
    "atom_id": "A001",
    "start_ms": 500000,
    "end_ms": 510000,
    "duration_ms": 10000,
    "merged_text": "1962年国民党残军撤到金三角，这是整个金三角问题的起源",
    "type": "陈述",
    "completeness": "完整",
    "source_utterance_ids": [85, 86, 87]
  },
  {
    "atom_id": "A002",
    "start_ms": 518000,
    "end_ms": 520000,
    "duration_ms": 2000,
    "merged_text": "hello 海绵宝宝",
    "type": "互动",
    "completeness": "完整",
    "source_utterance_ids": [88]
  }
]

现在处理以下字幕：
{input_text}
```

---

#### A.2 深度分析Prompt

**文件**: `prompts/analyze_comprehensive.txt`

```
深度分析这段金融/历史/政治类视频内容（3-15分钟叙事片段）。

【文本】
{full_text}

【分析任务】

1. 叙事结构分析

   type（选择一个）:
   - 历史叙事: 讲述历史事件的发展过程
   - 理论解释: 解释经济/政治理论
   - 数据分析: 基于数据的分析论证
   - 案例说明: 通过具体案例阐述观点
   - 观点论述: 表达个人观点和判断

   structure:
   - 用简短语言描述叙事展开方式
   - 例如: "背景铺垫 → 危机爆发 → 决策过程 → 结果影响"

   acts（分幕）:
   - 把内容分成2-4幕
   - 每幕包含: act名称, atoms列表, duration, summary
   - 每幕有明确作用（背景/发展/高潮/结局等）

2. 主题标注（完全自由提取）

   free_tags:
   - 提取5-10个描述性标签
   - 标签要准确、具体
   - 优先用原文中的关键词和术语
   - 不要泛泛而谈（"历史"太宽 → "1971年货币史"更好）
   - **不限制标签范围，AI自由判断**

   示例:
   - 好: ["布雷顿森林体系", "1971年尼克松冲击", "美元黄金脱钩"]
   - 差: ["历史", "经济", "金融"]

3. 实体提取（完全自由提取）

   提取6类实体，每类列举所有提到的：

   persons: 人物名称
   - 政治家、经济学家、企业家等
   - 例: ["尼克松", "戴高乐", "保罗·沃尔克"]

   countries: 国家/地区
   - 例: ["美国", "法国", "德国", "沙特"]

   organizations: 组织机构
   - 政府部门、国际组织、企业
   - 例: ["美联储", "IMF", "世界银行"]

   locations: 地点
   - 城市、地区等
   - 例: ["华盛顿", "巴黎"]

   time_points: 时间点
   - 精确到年份或具体日期
   - 例: ["1944年", "1971年8月15日", "1960年代"]

   events: 重大事件
   - 历史事件的名称
   - 例: ["布雷顿森林协议签署", "尼克松关闭黄金窗口"]

   concepts: 关键概念
   - 经济/金融/政治概念
   - 例: ["金本位", "美元霸权", "货币主权"]

4. 内容分面

   type: 历史叙述 | 观点评论 | 数据分析 | 案例说明 | 互动回应

   aspect（这段在讲什么角度）:
   - 成因分析: 解释为什么发生
   - 影响分析: 讲述造成的影响
   - 应对方法: 讨论如何应对
   - 历史对比: 对比不同时期
   - 理论解释: 用理论解释现象
   - 现状描述: 描述当前状况
   - 历史事件全景: 完整讲述事件始末

   stance（博主立场，如果明显）:
   - 中立客观: 客观陈述事实
   - 批判性: 批评、质疑
   - 支持性: 赞同、支持
   - 讽刺性: 讽刺、嘲讽

   perspective（可选）:
   - 从什么角度看问题
   - 例: "从美国霸权角度", "从中国视角", "从受害国角度"

5. AI深度分析

   core_argument:
   - 用一句话概括这段的核心论点
   - 要准确、简洁

   key_insights:
   - 提取3-5个关键洞察
   - 不是简单复述，而是提炼观点

   logical_flow:
   - 描述论述的逻辑展开方式
   - 例: "因果链条：A导致B，B导致C，C导致D"

   related_themes:
   - 相关主题列表
   - 用于跨片段关联

【输出JSON格式】

{
  "narrative_structure": {
    "type": "...",
    "structure": "...",
    "acts": [
      {
        "act": "...",
        "atoms": ["A045", "A046"],
        "duration": "3:30",
        "summary": "..."
      }
    ]
  },

  "topics": {
    "free_tags": [...]
  },

  "entities": {
    "persons": [...],
    "countries": [...],
    "organizations": [...],
    "locations": [...],
    "time_points": [...],
    "events": [...],
    "concepts": [...]
  },

  "content_facet": {
    "type": "...",
    "aspect": "...",
    "stance": "...",
    "perspective": "..."
  },

  "ai_analysis": {
    "core_argument": "...",
    "key_insights": [...],
    "logical_flow": "...",
    "related_themes": [...]
  }
}

【重要提醒】

1. **标签和实体完全自由提取**
   - 不要被示例限制
   - AI自由判断，根据内容提取
   - 用原文的关键词

2. **分析要深入**
   - 不要泛泛而谈
   - 提取实质性洞察
   - 理解逻辑关系

3. **只输出JSON**
   - 不要有markdown代码块标记
   - 不要有解释文字
   - 确保JSON格式正确

现在开始分析。
```

---

### 附录B: 文件结构

```
video_understanding_engine/
├── README.md
├── requirements.txt
├── config.py
├── engine.py                    # 主引擎
│
├── parsers/                     # 解析模块
│   ├── __init__.py
│   ├── srt_parser.py
│   └── cleaner.py
│
├── atomizers/                   # 原子化模块
│   ├── __init__.py
│   ├── atomizer.py
│   └── validator.py
│
├── taggers/                     # 标注模块
│   ├── __init__.py
│   └── quick_tagger.py
│
├── segmenters/                  # 片段识别
│   ├── __init__.py
│   └── segment_identifier.py
│
├── analyzers/                   # 深度分析
│   ├── __init__.py
│   └── deep_analyzer.py
│
├── embedders/                   # 向量生成
│   ├── __init__.py
│   └── embedder.py
│
├── indexers/                    # 索引构建
│   ├── __init__.py
│   ├── vector_indexer.py
│   ├── structured_indexer.py
│   └── global_index_builder.py
│
├── searchers/                   # 检索模块
│   ├── __init__.py
│   ├── semantic_searcher.py
│   ├── structured_searcher.py
│   └── hybrid_searcher.py
│
├── generators/                  # 方案生成
│   ├── __init__.py
│   ├── plan_generator.py
│   └── edl_generator.py
│
├── models/                      # 数据模型
│   ├── __init__.py
│   ├── utterance.py
│   ├── atom.py
│   ├── segment.py
│   ├── entity.py
│   └── taxonomy.py
│
├── prompts/                     # Prompt模板
│   ├── atomize_v2.txt
│   ├── analyze_comprehensive.txt
│   ├── identify_segments.txt
│   └── generate_plan.txt
│
├── utils/                       # 工具函数
│   ├── __init__.py
│   ├── api_client.py
│   ├── time_utils.py
│   └── file_utils.py
│
├── cli/                         # 命令行工具
│   ├── process.py               # 处理视频
│   ├── search.py                # 测试检索
│   └── creative.py              # AI二创
│
└── tests/                       # 测试
    └── ...
```

---

### 附录C: API密钥配置

**文件**: `config.py`

```python
import os
from dotenv import load_dotenv

load_dotenv()

CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# 模型配置
CLAUDE_MODEL = "claude-3-5-sonnet-20241022"
OPENAI_EMBEDDING_MODEL = "text-embedding-3-large"

# 处理配置
ATOM_BATCH_SIZE = 50  # 每次处理50条字幕
MAX_SEGMENT_DURATION_MS = 900000  # 片段最大15分钟

# 成本控制
MAX_RETRY = 3
TIMEOUT = 120

# 输出路径
OUTPUT_DIR = "data/output"
VECTOR_DB_DIR = "data/vector_db"
GLOBAL_INDEX_DIR = "data/global"
```

**环境变量**: `.env`

```
CLAUDE_API_KEY=your_claude_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
```

---

### 附录D: 依赖包

**文件**: `requirements.txt`

```
anthropic==0.18.0
openai==1.0.0
chromadb==0.4.22
srt==3.5.0
pydantic==2.5.0
python-dotenv==1.0.0
rich==13.7.0
scikit-learn==1.3.0
numpy==1.24.0
pandas==2.1.0
```

---

## 文档结束

**下一步行动**:
1. Review这份文档，确认设计方案
2. 开始Phase 1开发
3. 实现核心引擎
4. 测试验证

**预期时间线**:
- Week 1-2: 核心引擎
- Week 3: 检索系统
- Week 4: 多视频支持
- Week 5-6: AI二创
- Week 7-8: 前端集成

**总计**: 8周完成完整系统
